% Encoding: UTF-8

@Article{Yang2016291,
  author          = {Yang, X. and Mao, K.},
  title           = {Learning multi-prototype word embedding from single-prototype word embedding with integrated knowledge},
  journal         = {Expert Systems with Applications},
  year            = {2016},
  volume          = {56},
  pages           = {291-299},
  note            = {cited By},
  abstract        = {Distributional semantic models (DSM) or word embeddings are widely used in prediction of semantic similarity and relatedness. However, context aware similarity and relatedness prediction is still a challenging issue because most DSM models or word embeddings use one vector per word without considering polysemy and homonym. In this paper, we propose a supervised fine tuning framework to transform the existing single-prototype word embeddings into multi-prototype word embeddings based on lexical semantic resources. As a post-processing step, the proposed framework is compatible with any sense inventory and any word embedding. To test the proposed learning framework, both intrinsic and extrinsic evaluations are conducted. Experiments results of 3 tasks with 8 datasets show that the multi-prototype word representations learned by the proposed framework outperform single-prototype word representations. Â© 2016 Elsevier Ltd. All rights reserved.},
  affiliation     = {School of Electrical and Electronic Engineering, Nanyang Technological University, Nanyang Avenue, Singapore, Singapore},
  author_keywords = {Distributional semantic model; Fine tuning; Multi-prototype word embedding; Semantic similarity},
  document_type   = {Article},
  doi             = {10.1016/j.eswa.2016.03.013},
  source          = {Scopus},
  url             = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961960526&doi=10.1016%2fj.eswa.2016.03.013&partnerID=40&md5=a9903a94e467da3c952eaa33c41ef8b9},
}

@Comment{jabref-meta: databaseType:bibtex;}
