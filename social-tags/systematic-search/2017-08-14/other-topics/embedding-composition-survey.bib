% Encoding: UTF-8

@InProceedings{conf/acl/IyyerMBD15,
  author    = {Iyyer, Mohit and Manjunatha, Varun and Boyd-Graber, Jordan L. and III, Hal Daum√©},
  title     = {Deep Unordered Composition Rivals Syntactic Methods for Text Classification.},
  booktitle = {ACL (1)},
  year      = {2015},
  pages     = {1681-1691},
  publisher = {The Association for Computer Linguistics},
  added-at  = {2015-08-02T00:00:00.000+0200},
  biburl    = {https://www.bibsonomy.org/bibtex/28b0cab3025fce65c895665998b1091e5/dblp},
  crossref  = {conf/acl/2015-1},
  ee        = {http://aclweb.org/anthology/P/P15/P15-1162.pdf},
  interhash = {29853a04479b960deec99ff5af354a0e},
  intrahash = {8b0cab3025fce65c895665998b1091e5},
  isbn      = {978-1-941643-72-3},
  keywords  = {dblp},
  timestamp = {2015-08-04T11:41:15.000+0200},
  url       = {http://dblp.uni-trier.de/db/conf/acl/acl2015-1.html#IyyerMBD15},
}

@InProceedings{Sheikh2016LearningWI,
  author = {Imran Sheikh and Irina Illina and Dominique Fohr and Georges Linar{\`e}s},
  title  = {Learning Word Importance with the Neural Bag-of-Words Model},
  year   = {2016},
}

@Article{DBLP:journals/corr/DaiOL15,
  author    = {Andrew M. Dai and Christopher Olah and Quoc V. Le},
  title     = {Document Embedding with Paragraph Vectors},
  journal   = {CoRR},
  year      = {2015},
  volume    = {abs/1507.07998},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/DaiOL15},
  timestamp = {Wed, 07 Jun 2017 14:42:05 +0200},
  url       = {http://arxiv.org/abs/1507.07998},
}

@InProceedings{ye_etal_2016,
  author    = {X. Ye and H. Shen and X. Ma and R. Bunescu and C. Liu},
  title     = {From Word Embeddings to Document Similarities for Improved Information Retrieval in Software Engineering},
  booktitle = {2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE)},
  year      = {2016},
  pages     = {404-415},
  month     = {May},
  abstract  = {The application of information retrieval techniques to search tasks in software engineering is made difficult by the lexical gap between search queries, usually expressed in natural language (e.g. English), and retrieved documents, usually expressed in code (e.g. programming languages). This is often the case in bug and feature location, community question answering, or more generally the communication between technical personnel and non-technical stake holders in a software project. In this paper, we propose bridging the lexical gap by projecting natural language statements and code snippets as meaning vectors in a shared representation space. In the proposed architecture, word embeddings are rst trained on API documents, tutorials, and reference documents, and then aggregated in order to estimate semantic similarities between documents. Empirical evaluations show that the learned vector space embeddings lead to improvements in a previously explored bug localization task and a newly de ned task of linking API documents to computer programming questions.},
  doi       = {10.1145/2884781.2884862},
  keywords  = {application program interfaces;document handling;information retrieval;natural language processing;software engineering;system documentation;API documents;bug localization task;code snippets;document similarities;information retrieval techniques;natural language statements;reference documents;software engineering;tutorials;vector space embeddings;word embeddings;Computer bugs;Context;Mathematical model;Natural languages;Semantics;Software;Vocabulary;API documents;bug localization;bug reports;skip-gram model;word embeddings},
}

@Article{yang_etal_2017,
  author   = {L. Yang and X. Chen and Z. Liu and M. Sun},
  title    = {Improving Word Representations with Document Labels},
  journal  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year     = {2017},
  volume   = {25},
  number   = {4},
  pages    = {863-870},
  month    = {April},
  issn     = {2329-9290},
  abstract = {Word representations, aiming to build vectors for each word, have been successfully used in a variety of applications. Most word representations are learned from large amounts of documents ignoring other information. This is rather suboptimal because the side information of the documents, such as document labels, is not used in learning word representations. In this paper, we focus on how to exploit these side information to improve word representations. We propose to incorporate document labels into the learning process of word representations in two frameworks: neural network and matrix factorization. The experimental results on word analogy and word similarity task show that our models can better capture the semantic and syntactic information than the original models. Our models also improve the performance of word representations on text classification task.},
  doi      = {10.1109/TASLP.2017.2658019},
  keywords = {matrix decomposition;neural nets;word processing;document labels;learning process;matrix factorization;neural network;semantic information;side information;syntactic information;text classification task;word analogy;word representations;Context;Mathematical model;Matrix decomposition;Neural networks;Predictive models;Semantics;Training;Matrix factorization;skip-gram;word representations, rank2},
}

@Comment{jabref-meta: databaseType:bibtex;}
