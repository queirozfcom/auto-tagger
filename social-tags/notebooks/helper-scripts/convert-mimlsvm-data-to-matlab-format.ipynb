{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This script converts multi-label data in numpy ndarray format to the format thats used by the MIMLSVM code (can be downloaded here: http://lamda.nju.edu.cn/code_MIMLBoost%20and%20MIMLSVM.ashx)\n",
    "\n",
    "- The first step is optional (you can just use the second step if your data is already in vectorized format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```matlab\n",
    "%       MIML_TO_MLL takes,\n",
    "%           train_bags       - An M1x1 cell, the jth instance of the ith training bag is stored in train_bags{i,1}(j,:)\n",
    "%           train_target     - A QxM1 array, if the ith training bag belongs to the jth class, then train_target(j,i) equals +1, otherwise train_target(j,i) equals -1\n",
    "%           test_bags        - An M2x1 cell, the jth instance of the ith test bag is stored in test_bags{i,1}(j,:)\n",
    "%           test_target      - A QxM2 array, if the ith test bag belongs to the jth class, test_target(j,i) equals +1, otherwise test_target(j,i) equals -1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOtes:\n",
    "\n",
    "- M1 = number of training bags\n",
    "- M2 = number of test bags\n",
    "- Q = number of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import TextTilingTokenizer\n",
    "from scipy.io import savemat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler,MinMaxScaler\n",
    "\n",
    "pd.set_option('display.max_colwidth',1000)\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = os.path.join(os.getcwd(), os.pardir, '../')\n",
    "sys.path.append(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.data.movielens_20m_imdb\n",
    "%aimport src.helpers.labels,src.helpers.neighbours, src.helpers.segments\n",
    "%aimport src.utils.dataframes, src.utils.clusters, src.utils.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.data.movielens_20m_imdb import load_df_or_get_from_cache\n",
    "from src.helpers.labels import truncate_labels\n",
    "\n",
    "from src.utils.dataframes import sample_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODELS_ROOT = os.path.abspath(\"../../models/ranking/movielens-ovr-linear-svc-calibrated/\")\n",
    "INTERIM_DATA_ROOT = os.path.abspath(\"../../data/interim/movielens-ml20m-imdb/\")\n",
    "PATH_TO_PROCESSED_FILE = os.path.abspath('../../data/processed/movielens-20m-imdb-tags-and-synopses-2017-12-20.csv')\n",
    "\n",
    "OUT_PATH = '/home/felipe/Downloads/MIMLBoost&MIMLSVM/sample/testing/'\n",
    "\n",
    "# CONFIGS\n",
    "SEED= 42\n",
    "MAX_NB_WORDS = 200\n",
    "SAMPLING_FACTOR = 0.3\n",
    "MIN_TAG_DF = 10\n",
    "\n",
    "W=20 # Pseudosentence size (in words) - not specified in the paper, taken from TextTiling default values\n",
    "K=10 # Size (in sentences) of the block used in the block comparison method - not specified in the paper, taken from TextTiling default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = load_df_or_get_from_cache(PATH_TO_PROCESSED_FILE,INTERIM_DATA_ROOT)\n",
    "\n",
    "docs_df = sample_rows(docs_df,SAMPLING_FACTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "docs_df['sentences'] = docs_df['synopsis'].map(lambda row: sentence_tokenizer.tokenize(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok = TextTilingTokenizer(w=W, k=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_segments(candidates):\n",
    "    \n",
    "    try:\n",
    "        # we must manually insert \"\\n\\n\" because this is how \n",
    "        # texttilingtokenizer requires candidate boundaries to be \n",
    "        # represented.\n",
    "        segments = tok.tokenize(\"\\n\\n\".join(candidates))\n",
    "    except ValueError:\n",
    "        # this happens when the candidate list is too small for the \n",
    "        # text tiling tokenizer to be able to find segments. so just return\n",
    "        # the original sentences.\n",
    "        segments= candidates\n",
    "        \n",
    "    # now remove the artificially added chars\n",
    "    segments = [segment.replace(\"\\n\\n\",\" \").strip() for segment in segments]\n",
    "    \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_df['segments'] = docs_df['sentences'].map(lambda candidates: extract_segments(candidates))\n",
    "docs_df['num_segments'] = docs_df['segments'].map( lambda sents: len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_segments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2013.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.982116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.242181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>92.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       num_segments\n",
       "count   2013.000000\n",
       "mean       9.982116\n",
       "std       10.242181\n",
       "min        1.000000\n",
       "25%        3.000000\n",
       "50%        7.000000\n",
       "75%       12.000000\n",
       "max       92.000000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df[['num_segments']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = docs_df['synopsis'].values\n",
    "segments = docs_df['segments'].values\n",
    "\n",
    "labels = docs_df[\"tags\"].map(lambda tagstring: tagstring.split(\",\"))\n",
    "labels = truncate_labels(labels,MIN_TAG_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of train documents: 1711\n",
      "total number of validation documents: 302\n",
      "total number of unique tags: 608 \n"
     ]
    }
   ],
   "source": [
    "documents_train, documents_test, segments_train, segments_test, target_train, target_test = train_test_split(\n",
    "    documents, \n",
    "    segments,\n",
    "    labels,\n",
    "    test_size=0.15,\n",
    "    random_state=SEED)\n",
    "\n",
    "print('total number of train documents: {}'.format(documents_train.shape[0]))\n",
    "print('total number of validation documents: {}'.format(documents_test.shape[0]))\n",
    "print(\"total number of unique tags: {} \".format(len(mlb.classes_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=MAX_NB_WORDS).fit(documents_train)\n",
    "label_binarizer = MultiLabelBinarizer().fit(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorize each instance in each bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'September 1960: on a purely punitive basis, eight scouts must climb the solid mass of Br√©vent to 2500 meters of altitude. The so beautiful and so majestic mountain which draws up face them very quickly reveals dangerous. All the techniques of orientation learned at the scouts will do nothing there. The teenagers find themselves delivered to themselves. Lost in the abrupt throats, the eight boys are confronted cold, with the hunger and the fear. A tension starts to reign between these young people of different origins and of which concerns divergent: some worry for Algeria, for a brother left to the combat, or a family threatened of expulsion in metropolis, still the girls worry to like others. The group divides then to find an exit, but one of them disappears in water frozen from a torrent. While young people try to protect itself in a closed down refuge, the others will seek the helps...'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right now, segments_train and segments_test are list of lists of strings.\n",
    "\n",
    "vectorized_segments = list()\n",
    "\n",
    "for i in range(100):\n",
    "    current_segments = segments_train[i]\n",
    "    \n",
    "    current_vectorized_segments = list()\n",
    "    \n",
    "    for segment in current_segments:\n",
    "        \n",
    "        segment_data = np.array([segment])\n",
    "        \n",
    "        vectorized_segment = vectorizer.transform(segment_data).toarray()\n",
    "        \n",
    "        current_vectorized_segments.append(vectorized_segment)\n",
    "        \n",
    "    vectorized_segments.append(current_vectorized_segments)   \n",
    "    \n",
    "np_vectorized_segments = np.array(vectorized_segments)   \n",
    "\n",
    "\n",
    "train_labels = label_binarizer.transform(target_train[:100])\n",
    "\n",
    "# they want +1/-1 indicators\n",
    "train_labels[train_labels == 0] = -1\n",
    "\n",
    "savemat(OUT_PATH+'/train.mat',{'bags':np_vectorized_segments, 'targets':train_labels.T})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right now, segments_train and segments_test are list of lists of strings.\n",
    "\n",
    "vectorized_segments = list()\n",
    "\n",
    "for i in range(100):\n",
    "    current_segments = segments_test[i]\n",
    "    \n",
    "    current_vectorized_segments = list()\n",
    "    \n",
    "    for segment in current_segments:\n",
    "        \n",
    "        segment_data = np.array([segment])\n",
    "        \n",
    "        vectorized_segment = vectorizer.transform(segment_data).toarray()\n",
    "               \n",
    "        current_vectorized_segments.append(vectorized_segment)\n",
    "        \n",
    "    vectorized_segments.append(current_vectorized_segments)   \n",
    "    \n",
    "np_vectorized_segments = np.array(vectorized_segments)   \n",
    "\n",
    "test_labels = label_binarizer.transform(target_test[:100])\n",
    "\n",
    "# they want +1/-1 indicators\n",
    "test_labels[test_labels == 0] = -1\n",
    "\n",
    "savemat(OUT_PATH+'/test.mat',{'bags':np_vectorized_segments, 'targets':test_labels.T})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Global TF Kernel (Python 3)",
   "language": "python",
   "name": "global-tf-python-3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
