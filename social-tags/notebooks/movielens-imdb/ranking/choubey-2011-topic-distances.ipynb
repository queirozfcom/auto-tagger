{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see http://krex.k-state.edu/dspace/bitstream/handle/2097/9785/RahulChoubey2011.pdf\n",
    "\n",
    "This is an implementation of the **second** method of the two described above, namely topic distance.\n",
    "\n",
    "Extract topics from the dataset using LDA then, for each test document, find out what are the most significant topics for it. Then find out which training set documents have similar topic distributions (as measured by KL-divergence) and propagate the tags from the one most similar document.\n",
    "\n",
    "> This is the same thing as doing kNN where k=1 and the distance measure between representations is the KL-divergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import sklearn\n",
    "import sys\n",
    "import string\n",
    "\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity,cosine_distances\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV,ParameterGrid, train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier,NearestNeighbors\n",
    "\n",
    "from tqdm import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_dir = os.path.join(os.getcwd(), os.pardir, '../../')\n",
    "sys.path.append(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%aimport src.data.movielens_20m_imdb\n",
    "%aimport src.helpers.labels,src.helpers.neighbours, src.helpers.topics\n",
    "%aimport src.utils.dataframes, src.utils.clusters, src.utils.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.data.movielens_20m_imdb import load_df_or_get_from_cache\n",
    "from src.helpers.labels import truncate_labels\n",
    "from src.helpers.neighbours import get_predicted_labels_from_neighbours\n",
    "from src.helpers.segments import make_distance_matrix_for_segments,vectorize_segments\n",
    "\n",
    "from src.utils.dataframes import sample_rows\n",
    "from src.utils.metrics import ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODELS_ROOT = os.path.abspath(\"../../../models/ranking/movielens-topics/\")\n",
    "INTERIM_DATA_ROOT = os.path.abspath(\"../../../data/interim/movielens-ml20m-imdb/\")\n",
    "PATH_TO_PROCESSED_FILE = os.path.abspath('../../../data/processed/movielens-20m-imdb-tags-and-synopses-2017-12-13.csv')\n",
    "OUTPUT_FILE = 'output-topic-distances-'+ datetime.now().strftime('%Y-%m-%d-%H-%M-%S')+'.txt'\n",
    "\n",
    "# CONFIGS\n",
    "\n",
    "SEED= 42\n",
    "\n",
    "MAX_NB_WORDS = 5000\n",
    "\n",
    "# for sampling\n",
    "MIN_TAG_DF = 10\n",
    "\n",
    "# CONFIGS\n",
    "MAX_NB_WORDS = 5000\n",
    "NB_NEIGHBOURS = 1\n",
    "DISTANCE_METRIC= lambda a,b: stats.entropy(a,b)\n",
    "STOP_WORDS='english' # using stopwords since most people using LDA do this\n",
    "NB_COMPONENTS = [100,200,400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_df = load_df_or_get_from_cache(PATH_TO_PROCESSED_FILE,INTERIM_DATA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = docs_df['synopsis'].values\n",
    "labels = docs_df[\"tags\"].map(lambda tagstring: tagstring.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "truncated_labels = truncate_labels(labels,MIN_TAG_DF)\n",
    "\n",
    "binary_labels = mlb.fit_transform(truncated_labels)\n",
    "\n",
    "print(\"total number of unique tags: {} \".format(len(mlb.classes_)))\n",
    "\n",
    "indices = np.arange(len(data))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "data = [data[i] for i in indices]\n",
    "targets = binary_labels[indices]\n",
    "num_validation_samples = int(0.15 * len(data))\n",
    "\n",
    "X_train = data[:-num_validation_samples]\n",
    "Y_train = targets[:-num_validation_samples]\n",
    "X_val = data[-num_validation_samples:]\n",
    "Y_val = targets[-num_validation_samples:]\n",
    "\n",
    "print('total number of train documents: {}'.format(len(X_train)))\n",
    "print('total number of validation documents: {}'.format(len(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(max_features=MAX_NB_WORDS, stop_words=STOP_WORDS)\n",
    "# it's ok to fit in the whole data since this is not training a model\n",
    "vect.fit(data)\n",
    "\n",
    "X_train_vect = vect.transform(X_train)\n",
    "X_val_vect = vect.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = dict()\n",
    "\n",
    "for nb_comp in NB_COMPONENTS:\n",
    "\n",
    "    if os.path.isfile(MODELS_ROOT+\"/lda-{}.p\".format(nb_comp)):\n",
    "        lda[nb_comp]=joblib.load(open(MODELS_ROOT+\"/lda-{}.p\".format(nb_comp),\"rb\"))\n",
    "    \n",
    "    else:\n",
    "        lda[nb_comp] = LatentDirichletAllocation(n_components=nb_comp, learning_method='online')\n",
    "        lda[nb_comp].fit(X_train_vect)\n",
    "        joblib.dump(lda[nb_comp],open(MODELS_ROOT+\"/lda-{}.p\".format(nb_comp),\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# next block is not verified. I need to check that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_score(nb_comp):\n",
    "    nbrs = NearestNeighbors(n_neighbors=NB_NEIGHBOURS, metric=DISTANCE_METRIC)\n",
    "    \n",
    "    lda_model = lda[nb_comp]\n",
    "    \n",
    "    # train\n",
    "    X_train = lda_model.transform(X_train_vect)\n",
    "    X_val = lda_model.transform(X_val_vect)\n",
    "    \n",
    "    nbrs.fit(X_train)\n",
    "    \n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "\n",
    "    # these are distances from and indices of the nearest neighbor\n",
    "    # for every point in X_val\n",
    "    distances_matrix_val, indices_matrix_val = nbrs.kneighbors(X_val)\n",
    "\n",
    "    \n",
    "    neighbour_labels_tensor_val = Y_train[indices_matrix_val]    \n",
    "\n",
    "#     distances_matrix.shape, indices_matrix.shape, neighbour_labels_tensor.shape\n",
    "    \n",
    "    \n",
    "    for i in range(distances_matrix_val.shape[0]):\n",
    "\n",
    "        distances = distances_matrix_val[i].ravel()  \n",
    "\n",
    "        neighbour_labels = neighbour_labels_tensor_val[i]\n",
    "\n",
    "        y_pred = get_predicted_labels_from_neighbours(neighbour_labels, distances)\n",
    "        y_preds.append(y_pred)\n",
    "\n",
    "    Y_pred_val = np.array(y_preds)\n",
    "    \n",
    "    # training data\n",
    "    # maybe it'll always be perfect because the query object itself is in the neighbor set\n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "    \n",
    "    distances_matrix_train, indices_matrix_train = nbrs.kneighbors(X_train)\n",
    "    \n",
    "    neighbour_labels_tensor_train = Y_train[indices_matrix_train]\n",
    "    \n",
    "    for i in range(distances_matrix_train.shape[0]):\n",
    "\n",
    "        distances = distances_matrix_train[i].ravel()  \n",
    "\n",
    "        neighbour_labels = neighbour_labels_tensor_train[i]\n",
    "\n",
    "        y_pred = get_predicted_labels_from_neighbours(neighbour_labels, distances)\n",
    "        y_preds.append(y_pred)\n",
    "\n",
    "    Y_pred_train = np.array(y_preds)\n",
    "        \n",
    "    with open(OUTPUT_FILE,'a+') as f:\n",
    "\n",
    "        f.write('RESULTS FOR NB_COMP={}\\n'.format(nb_comp))\n",
    "\n",
    "        ks = [1,2,3,4,5,6,7,8,9,10]       \n",
    "\n",
    "        f.write('NORMALIZED MICRO-F1:\\n')    \n",
    "        for k in ks:\n",
    "            f.write(\"train micro-F1 @{}: {}\\n\".format(k,ranking.micro_f1_at_k(Y_train,Y_pred_train,k=k,normalize=True)))\n",
    "            f.write(\"validation micro-F1 @{}: {}\\n\".format(k,ranking.micro_f1_at_k(Y_val,Y_pred_val,k=k,normalize=True)))\n",
    "\n",
    "        f.write(\"\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_score(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Global TF Kernel (Python 3)",
   "language": "python",
   "name": "global-tf-python-3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
