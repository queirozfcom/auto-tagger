{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mimlsvm\n",
    "\n",
    "mi = mulit-instance\n",
    "ml = multi-label\n",
    "svm = svm\n",
    "\n",
    "As described in Shen et al 2009: http://ieeexplore.ieee.org/document/5346261/\n",
    "\n",
    "> Should we use SVM-struct instead? https://github.com/pystruct/pystruct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "good params: {'svm_degree': 3, 'svm_kernel': 'poly', 'medoid_normalization': None, 'nb_medoids_ratio': 0.2, 'svm_gamma': 'auto', 'vectorizer_norm': None, 'svm_c': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import sklearn\n",
    "import sys\n",
    "import string\n",
    "\n",
    "from nltk import TextTilingTokenizer\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV,ParameterGrid, train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler,MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from tqdm import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_dir = os.path.join(os.getcwd(), os.pardir, '../../')\n",
    "sys.path.append(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%aimport src.data.movielens_20m_imdb\n",
    "%aimport src.helpers.labels, src.helpers.segments\n",
    "%aimport src.utils.dataframes, src.utils.clusters, src.utils.metrics, src.utils.distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.data.movielens_20m_imdb import load_df_or_get_from_cache\n",
    "from src.helpers.labels import truncate_labels\n",
    "from src.helpers.segments import make_distance_matrix_for_segments,vectorize_segments\n",
    "\n",
    "from src.utils.dataframes import sample_rows\n",
    "from src.utils.metrics import ranking\n",
    "from src.utils.clusters import k_medoids\n",
    "from src.utils.distances import hausdorff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODELS_ROOT = os.path.abspath(\"../../../models/ranking/movielens-mimlsvm/\")\n",
    "INTERIM_DATA_ROOT = os.path.abspath(\"../../../data/interim/movielens-ml20m-imdb/\")\n",
    "PATH_TO_PROCESSED_FILE = os.path.abspath('../../../data/processed/movielens-20m-imdb-tags-and-synopses-2017-12-20.csv')\n",
    "\n",
    "# CONFIGS\n",
    "\n",
    "SEED= 42\n",
    "MAX_NB_WORDS = 300\n",
    "NB_DOCS=0.2\n",
    "MIN_TAG_DF=10\n",
    "\n",
    "# preprocessing\n",
    "\n",
    "# grid search:\n",
    "\n",
    "VECTORIZER_NORM = ['l1','l2',None]\n",
    "W=20 # Pseudosentence size (in words) - not specified in the paper, taken from TextTiling default values\n",
    "K=10 # Size (in sentences) of the block used in the block comparison method - not specified in the paper, taken from TextTiling default values\n",
    "\n",
    "# calculating medoids\n",
    "NORMALIZATION = ['standard','minmax',None] # not specified in the paper\n",
    "SAMPLE_TO_NB_MEDOIDS_RATIO = [0.2,0.3,0.4] # not specified in the paper, but taken from MIMLSVM canonical implementation\n",
    "# classification\n",
    "SVM_KERNEL=['rbf','linear','poly'] # not specified in the paper, but taken from MIMLSVM canonical implementation\n",
    "SVM_GAMMA=['auto'] # not specified in the paper, but taken from MIMLSVM canonical implementation\n",
    "SVM_C= [0.001,0.01,0.1,1]# not specified in the paper, but taken from MIMLSVM canonical implementation\n",
    "SVM_DEGREE=[3,4,5]\n",
    "\n",
    "####################################\n",
    "### FINAL PARAMS AFTER GRID SEARCH\n",
    "###################################\n",
    "MAX_NB_WORDS = 1000\n",
    "\n",
    "# preprocessing\n",
    "VECTORIZER_NORM = [None]\n",
    "W=20 # Pseudosentence size (in words) - not specified in the paper, taken from TextTiling default values\n",
    "K=10 # Size (in sentences) of the block used in the block comparison method - not specified in the paper, taken from TextTiling default values\n",
    "\n",
    "# calculating medoids\n",
    "NORMALIZATION = [None] # not specified in the paper\n",
    "SAMPLE_TO_NB_MEDOIDS_RATIO = [0.2] # not specified in the paper, but taken from MIMLSVM canonical implementation\n",
    "# classification\n",
    "SVM_KERNEL=['poly'] # not specified in the paper, but taken from MIMLSVM canonical implementation\n",
    "SVM_GAMMA=['auto'] # not specified in the paper, but taken from MIMLSVM canonical implementation\n",
    "SVM_C= [1]# not specified in the paper, but taken from MIMLSVM canonical implementation\n",
    "SVM_DEGREE=[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_df = load_df_or_get_from_cache(PATH_TO_PROCESSED_FILE,INTERIM_DATA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove this for production\n",
    "docs_df = sample_rows(docs_df,NB_DOCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_df['sentences'] = docs_df['synopsis'].map(lambda row: sentence_tokenizer.tokenize(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 1.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "docs_df['num_sentences'] = docs_df['sentences'].map( lambda sents: len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>title</th>\n",
       "      <th>synopsis</th>\n",
       "      <th>tags</th>\n",
       "      <th>num_tags</th>\n",
       "      <th>sentences</th>\n",
       "      <th>num_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2069</td>\n",
       "      <td>Trip to Bountiful, The (1985)</td>\n",
       "      <td>Houston, April 1947. \"Mama\" Carrie Watts has l...</td>\n",
       "      <td>oscar-best-actress,reviewed,vhs,betamax</td>\n",
       "      <td>4</td>\n",
       "      <td>[Houston, April 1947., \"Mama\" Carrie Watts has...</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2165</td>\n",
       "      <td>Your Friends and Neighbors (1998)</td>\n",
       "      <td>Set in an unnamed Midwestern American city, tw...</td>\n",
       "      <td>social-satire</td>\n",
       "      <td>1</td>\n",
       "      <td>[Set in an unnamed Midwestern American city, t...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76077</td>\n",
       "      <td>Hot Tub Time Machine (2010)</td>\n",
       "      <td>In 2010, three friends are dissatisfied with t...</td>\n",
       "      <td>direction,1980s,acting,comedy,few-funny-scenes...</td>\n",
       "      <td>16</td>\n",
       "      <td>[In 2010, three friends are dissatisfied with ...</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2814</td>\n",
       "      <td>Bat, The (1959)</td>\n",
       "      <td>Murder-mystery writer Cornelia van Gorder [Agn...</td>\n",
       "      <td>vincent-price,bd-r,dvd-video,storm,writer,seri...</td>\n",
       "      <td>6</td>\n",
       "      <td>[Murder-mystery writer Cornelia van Gorder [Ag...</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93865</td>\n",
       "      <td>Frankenstein (1910)</td>\n",
       "      <td>The film starts with the intro page and a mess...</td>\n",
       "      <td>short,silent-movie,frankenstein's-monster,base...</td>\n",
       "      <td>6</td>\n",
       "      <td>[The film starts with the intro page and a mes...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id                              title  \\\n",
       "0      2069      Trip to Bountiful, The (1985)   \n",
       "1      2165  Your Friends and Neighbors (1998)   \n",
       "2     76077        Hot Tub Time Machine (2010)   \n",
       "3      2814                    Bat, The (1959)   \n",
       "4     93865                Frankenstein (1910)   \n",
       "\n",
       "                                            synopsis  \\\n",
       "0  Houston, April 1947. \"Mama\" Carrie Watts has l...   \n",
       "1  Set in an unnamed Midwestern American city, tw...   \n",
       "2  In 2010, three friends are dissatisfied with t...   \n",
       "3  Murder-mystery writer Cornelia van Gorder [Agn...   \n",
       "4  The film starts with the intro page and a mess...   \n",
       "\n",
       "                                                tags  num_tags  \\\n",
       "0            oscar-best-actress,reviewed,vhs,betamax         4   \n",
       "1                                      social-satire         1   \n",
       "2  direction,1980s,acting,comedy,few-funny-scenes...        16   \n",
       "3  vincent-price,bd-r,dvd-video,storm,writer,seri...         6   \n",
       "4  short,silent-movie,frankenstein's-monster,base...         6   \n",
       "\n",
       "                                           sentences  num_sentences  \n",
       "0  [Houston, April 1947., \"Mama\" Carrie Watts has...             34  \n",
       "1  [Set in an unnamed Midwestern American city, t...             36  \n",
       "2  [In 2010, three friends are dissatisfied with ...             43  \n",
       "3  [Murder-mystery writer Cornelia van Gorder [Ag...            103  \n",
       "4  [The film starts with the intro page and a mes...             13  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Houston, April 1947.',\n",
       " '\"Mama\" Carrie Watts has lived with her son Ludie and his disagreeable wife Jessie Mae in a small apartment for 15 years.',\n",
       " 'One night they all have trouble sleeping.',\n",
       " 'Jessie Mae nags at Mama persistently, and Mama begs Ludie to let her return home to Bountiful.',\n",
       " 'The next day, Mama takes her latest pension check, packs a bag, and goes to both the train and bus station; at both places, dispatchers tell her that no trips go to Bountiful anymore.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df.iloc[0]['sentences'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>num_tags</th>\n",
       "      <th>num_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>41089.172000</td>\n",
       "      <td>12.137000</td>\n",
       "      <td>58.517000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>39455.932202</td>\n",
       "      <td>14.887776</td>\n",
       "      <td>67.129607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3946.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>27825.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>37.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>73447.750000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>74.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>130402.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>544.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            movie_id     num_tags  num_sentences\n",
       "count    1000.000000  1000.000000    1000.000000\n",
       "mean    41089.172000    12.137000      58.517000\n",
       "std     39455.932202    14.887776      67.129607\n",
       "min         6.000000     1.000000       1.000000\n",
       "25%      3946.500000     3.000000      13.000000\n",
       "50%     27825.000000     7.000000      37.000000\n",
       "75%     73447.750000    16.000000      74.000000\n",
       "max    130402.000000   154.000000     544.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok = TextTilingTokenizer(w=W, k=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_segments(candidates):\n",
    "    \n",
    "    try:\n",
    "        # we must manually insert \"\\n\\n\" because this is how \n",
    "        # texttilingtokenizer requires candidate boundaries to be \n",
    "        # represented.\n",
    "        segments = tok.tokenize(\"\\n\\n\".join(candidates))\n",
    "    except ValueError:\n",
    "        # this happens when the candidate list is too small for the \n",
    "        # text tiling tokenizer to be able to find segments. so just return\n",
    "        # the original sentences.\n",
    "        segments= candidates\n",
    "        \n",
    "    # now remove the artificially added chars\n",
    "    segments = [segment.replace(\"\\n\\n\",\" \").strip() for segment in segments]\n",
    "    \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 32s, sys: 76 ms, total: 5min 32s\n",
      "Wall time: 5min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "docs_df['segments'] = docs_df['sentences'].map(lambda candidates: extract_segments(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Houston, April 1947. \"Mama\" Carrie Watts has lived with her son Ludie and his disagreeable wife Jessie Mae in a small apartment for 15 years. One night they all have trouble sleeping. Jessie Mae nags at Mama persistently, and Mama begs Ludie to let her return home to Bountiful.',\n",
       " 'The next day, Mama takes her latest pension check, packs a bag, and goes to both the train and bus station; at both places, dispatchers tell her that no trips go to Bountiful anymore. She buys a bus ticket for a nearby town and heads off with great excitement, evading Ludie and Jessie Mae. Along the way, she meets a young woman named Thelma, who is sad that her husband is away in the military.',\n",
       " 'Mama complains about Jessie Mae, wishing she still had her own house. Mama abruptly tells Thelma that she did not love her own husband, but rather was always in love with another man; she cries in explaining that she could not marry him because their fathers disliked each other. When Mama makes it to her stop with Thelma, she is shocked to learn that the friend she wanted to see in Bountiful died just days earlier, and that the town itself no longer has any residents. She also realizes that she left her purse on the bus, but the dispatcher calls ahead to have it brought back. Mama blithely wonders with Thelma why her life is filled with trouble yet relief. Thelma takes off to her destination, and Mama lies down to sleep on a bench.',\n",
       " \"Hours later, the town sheriff comes by to tell the dispatcher he'd gotten a call looking for Mrs. Watts. When the sheriff tells her that Ludie is coming to pick her up in the morning, Mama begs to be allowed to see Bountiful, only 12 miles away. She breaks down in tears, explaining that she just wants to see her old home once before she dies. The sheriff calls a doctor to have her sedated. At dawn, with Mama in a much calmer mood, the sheriff agrees to drive Mama to Bountiful.\",\n",
       " 'She is dismayed as they drive through the abandoned town, with its handful of buildings overgrown with weeds. He drives her out to her old house, also long since empty, and they chat. Mama eventually becomes emotional, knowing that she has outlived her family and her house. The sheriff leaves Mama alone to walk through the house for a while. Ludie drives to the house and finds her sitting on the porch. She says she got her wish. Ludie regrets not bringing her earlier, saying, \"I just thought it\\'d be easier if we never saw the house again,\" and he refuses to go in. As they stand outside, Ludie gets upset recalling Mama\\'s father\\'s funeral at the house when he was 10, pointing out that he\\'s never had children, feeling that he\\'s never done good by his mother or wife. He confesses that he\\'s been repressing the memories of his childhood, because they do not help him. Mama wonders what will still be left after they and the house are gone: the river, the fields, the trees, the smell.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df['segments'][0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "segments = docs_df['segments'].values\n",
    "documents = docs_df['synopsis'].values\n",
    "labels = docs_df[\"tags\"].map(lambda tagstring: tagstring.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove for production\n",
    "labels = truncate_labels(labels,MIN_TAG_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_train_dataset(distance_matrix, medoid_indices):\n",
    "    \"\"\"\n",
    "    Returns a matrix where element Aij contains the distance from sample i to medoid j.\n",
    "\n",
    "    :param distance_matrix: MxM matrix with pairwise distances\n",
    "    :param medoid_indices: array of length N containing the indices of the medoids for each cluster\n",
    "    :return: distances to medoids (MxN matrix)\n",
    "    \"\"\"\n",
    "\n",
    "    return distance_matrix[:,medoid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_test_dataset(source_vectorized_segments, medoid_vectorized_segments):\n",
    "    \"\"\"\n",
    "    Calculates the distances from every source_document (reprsented by its segments) to every medoid\n",
    "    document (also represented by its segments) using the hausdorff distance.\n",
    "    \n",
    "    Returns a matrix where element Aij contains the distance from sample i to medoid j.\n",
    "\n",
    "    :param source_vectorized_segments: array of length M, where each element is a matrix with one row\n",
    "        for every segment in a source document\n",
    "    :param medoid_vectorized_segments: array of length N where each element is a matrix with one row\n",
    "        for every segment in a medoid document\n",
    "    :return: distances to medoids (MxN matrix)\n",
    "    \"\"\"\n",
    "    \n",
    "    num_test_samples = len(source_vectorized_segments)\n",
    "    num_medoids = len(medoid_vectorized_segments)\n",
    "    \n",
    "    test_dataset = np.zeros((num_test_samples,num_medoids))    \n",
    "    \n",
    "    for i,source_segments in enumerate(source_vectorized_segments):\n",
    "        for j,medoid_segments in enumerate(medoid_vectorized_segments):\n",
    "            test_dataset[i][j] = hausdorff(source_segments.toarray(),medoid_segments.toarray())\n",
    "            \n",
    "    return np.array(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of train documents: 850\n",
      "total number of validation documents: 150\n"
     ]
    }
   ],
   "source": [
    "# segments, documents and labelsets are defined outside of the parameterGrid loop\n",
    "# because they're the same for every configuration    \n",
    "segments_train, segments_test, documents_train, documents_test, Y_train, Y_test = train_test_split(segments,\n",
    "                                                                                               documents,\n",
    "                                                                                               labels,\n",
    "                                                                                               test_size=0.15)\n",
    "\n",
    "print('total number of train documents: {}'.format(len(documents_train)))\n",
    "print('total number of validation documents: {}'.format(len(documents_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of unique tags: 292 \n"
     ]
    }
   ],
   "source": [
    "# the binarizer needs to be fit on all labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(labels)\n",
    "\n",
    "Y_train = mlb.transform(Y_train)\n",
    "Y_test = mlb.transform(Y_test)\n",
    "\n",
    "print(\"total number of unique tags: {} \".format(len(mlb.classes_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search_parameters = [\n",
    "    {\n",
    "        'medoid_normalization':  NORMALIZATION,\n",
    "        'svm_kernel': ['poly'],\n",
    "        'svm_c':SVM_C,\n",
    "        'svm_degree' :SVM_DEGREE,\n",
    "        'svm_gamma':SVM_GAMMA,\n",
    "        'vectorizer_norm': VECTORIZER_NORM,\n",
    "        'nb_medoids_ratio': SAMPLE_TO_NB_MEDOIDS_RATIO\n",
    "    },\n",
    "    {\n",
    "        'medoid_normalization':  NORMALIZATION,\n",
    "        'svm_kernel': ['rbf'],\n",
    "        'svm_c':SVM_C,\n",
    "        'svm_degree' :[None],\n",
    "        'svm_gamma':SVM_GAMMA,\n",
    "        'vectorizer_norm': VECTORIZER_NORM,\n",
    "        'nb_medoids_ratio': SAMPLE_TO_NB_MEDOIDS_RATIO\n",
    "    },\n",
    "    {\n",
    "        'medoid_normalization':  NORMALIZATION,\n",
    "        'svm_kernel': ['linear'],\n",
    "        'svm_c':SVM_C,\n",
    "        'svm_gamma':SVM_GAMMA,\n",
    "        'svm_degree' :[None],\n",
    "        'vectorizer_norm': VECTORIZER_NORM,\n",
    "        'nb_medoids_ratio': SAMPLE_TO_NB_MEDOIDS_RATIO\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = [\n",
    "    {\n",
    "        'medoid_normalization':  [None],\n",
    "        'svm_kernel': ['poly'],\n",
    "        'svm_c':[1.0],\n",
    "        'svm_degree' :[3],\n",
    "        'svm_gamma':['auto'],\n",
    "        'vectorizer_norm': [None],\n",
    "        'nb_medoids_ratio': [0.2],\n",
    "        'max_features':[300]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of configurations to test: 1\n"
     ]
    }
   ],
   "source": [
    "print('total number of configurations to test: {}'.format(len(ParameterGrid(parameters))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# algorithm:\n",
    "# split each document into segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, configuration: {'medoid_normalization': None, 'max_features': 300, 'vectorizer_norm': None, 'svm_kernel': 'poly', 'svm_degree': 3, 'nb_medoids_ratio': 0.2, 'svm_c': 1.0, 'svm_gamma': 'auto'}\n",
      "\n",
      "train micro-F1 @1: 0.37320574162679426\n",
      "validation micro-F1 @1: 0.27586206896551724\n",
      "train micro-F1 @2: 0.2599795291709314\n",
      "validation micro-F1 @2: 0.2658959537572254\n",
      "train micro-F1 @3: 0.23468328141225336\n",
      "validation micro-F1 @3: 0.24561403508771928\n",
      "train micro-F1 @4: 0.2208267922553637\n",
      "validation micro-F1 @4: 0.2430453879941435\n",
      "train micro-F1 @5: 0.22250104558762024\n",
      "validation micro-F1 @5: 0.22904368358913813\n",
      "train micro-F1 @6: 0.2095839915745129\n",
      "validation micro-F1 @6: 0.21782178217821782\n",
      "train micro-F1 @7: 0.19942502647904373\n",
      "validation micro-F1 @7: 0.20971867007672634\n",
      "train micro-F1 @8: 0.19508958195089582\n",
      "validation micro-F1 @8: 0.19548872180451127\n",
      "train micro-F1 @9: 0.1966053748231966\n",
      "validation micro-F1 @9: 0.18426361802286484\n",
      "train micro-F1 @10: 0.19341126461211477\n",
      "validation micro-F1 @10: 0.1785063752276867\n"
     ]
    }
   ],
   "source": [
    "for (i,configuration) in enumerate(ParameterGrid(parameters)):\n",
    "    \n",
    "    # TFIDF_VECTORIZER = COUNT_VECTORIZER + TFIDF_TRANSFORMER\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_features=configuration['max_features'], \n",
    "        norm=configuration['vectorizer_norm'])\n",
    "    \n",
    "    # TRAINING SET\n",
    "    tfidf_vectorizer.fit(documents_train)\n",
    "    tfidf_segments_train = vectorize_segments(segments_train, tfidf_vectorizer)\n",
    "        \n",
    "    \n",
    "    # THE FOLLOWING BLOCK TAKES SOME TIME, BUT IT WILL ONLY RUN ONCE\n",
    "    \n",
    "    path_to_cache = INTERIM_DATA_ROOT.rstrip('/') + \"/mimlsvm/distance-matrix-train-sample-{}-{}-{}.p\".format(\n",
    "        NB_DOCS,\n",
    "        configuration['max_features'],\n",
    "        configuration['vectorizer_norm'])\n",
    "    \n",
    "    if os.path.isfile(path_to_cache):\n",
    "        dist_matrix_train = pickle.load(open(path_to_cache,\"rb\"))\n",
    "    else:\n",
    "        print('Fitting distance matrix for norm={}'.format(configuration['vectorizer_norm']))\n",
    "        \n",
    "        dist_matrix_train = make_distance_matrix_for_segments(tfidf_segments_train)\n",
    "        pickle.dump(dist_matrix_train, open(path_to_cache, \"wb\"))\n",
    "    \n",
    "    # nb_medoids depends upon the dataset length\n",
    "    ratio = configuration['nb_medoids_ratio']\n",
    "    nb_medoids = int(len(tfidf_segments_train) * ratio)\n",
    "    \n",
    "    medoids_indices_train = k_medoids(dist_matrix_train,nb_medoids)[0]\n",
    "\n",
    "    X_train = make_train_dataset(dist_matrix_train,medoids_indices_train)\n",
    "    \n",
    "    # TEST SET\n",
    "    \n",
    "    tfidf_segments_test = vectorize_segments(segments_test, tfidf_vectorizer)\n",
    "          \n",
    "    # medoids trained on the training set\n",
    "    fitted_medoids = tfidf_segments_train[medoids_indices_train]\n",
    "    X_test = make_test_dataset(tfidf_segments_test,fitted_medoids)\n",
    "      \n",
    "    clf = OneVsRestClassifier(\n",
    "        SVC(kernel=configuration['svm_kernel'],\n",
    "            gamma=configuration['svm_gamma'],\n",
    "            C=configuration['svm_c'],\n",
    "            degree=configuration['svm_degree'],\n",
    "            probability=True,\n",
    "           ),n_jobs=-1)    \n",
    "\n",
    "   \n",
    "    \n",
    "    if configuration['medoid_normalization'] == 'standard':      \n",
    "        scaler = StandardScaler()\n",
    "        X_train_final = scaler.fit_transform(X_train)\n",
    "        X_test_final = scaler.transform(X_test)\n",
    "    elif configuration['medoid_normalization'] == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_final = scaler.fit_transform(X_train)\n",
    "        X_test_final = scaler.transform(X_test)\n",
    "    else:\n",
    "        X_train_final = X_train\n",
    "        X_test_final = X_test\n",
    "    \n",
    "    # y_train was defined outside the loop    \n",
    "    clf.fit(X_train,Y_train)\n",
    "    \n",
    "    # train score\n",
    "    Y_pred_train = clf.predict_proba(X_train)\n",
    "    \n",
    "    # validation score\n",
    "    Y_pred_test = clf.predict_proba(X_test)  \n",
    "    \n",
    "    print(\"iter: {}, configuration: {}\\n\".format(i,configuration))\n",
    "    \n",
    "    ks = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "    for k in ks:\n",
    "        print(\"train micro-F1 @{}: {}\".format(k,ranking.micro_f1_at_k(Y_train,Y_pred_train,k=k,normalize=True)))\n",
    "        print(\"validation micro-F1 @{}: {}\".format(k,ranking.micro_f1_at_k(Y_test,Y_pred_test,k=k,normalize=True)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
