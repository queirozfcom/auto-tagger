{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mimlsvm\n",
    "\n",
    "mi = mulit-instance\n",
    "ml = multi-label\n",
    "svm = svm\n",
    "\n",
    "As described in Shen et al 2009: http://ieeexplore.ieee.org/document/5346261/\n",
    "\n",
    "> Should we use SVM-struct instead? https://github.com/pystruct/pystruct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import sklearn\n",
    "import sys\n",
    "import string\n",
    "\n",
    "from hausdorff import hausdorff\n",
    "\n",
    "from nltk import TextTilingTokenizer\n",
    "from scipy.spatial.distance  import directed_hausdorff, pdist\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV,ParameterGrid, train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler,MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from tqdm import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_dir = os.path.join(os.getcwd(), os.pardir, '../../')\n",
    "sys.path.append(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%aimport src.data.movielens_20m_imdb\n",
    "%aimport src.helpers.labels,src.helpers.neighbours, src.helpers.segments\n",
    "%aimport src.utils.dataframes, src.utils.clusters, src.utils.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.data.movielens_20m_imdb import load_df_or_get_from_cache\n",
    "from src.helpers.labels import truncate_labels\n",
    "from src.helpers.neighbours import get_predicted_labels_from_neighbours\n",
    "from src.helpers.segments import make_distance_matrix_for_segments,vectorize_segments\n",
    "\n",
    "from src.utils.dataframes import sample_rows\n",
    "from src.utils.metrics import ranking\n",
    "from src.utils.clusters import k_medoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODELS_ROOT = os.path.abspath(\"../../../models/ranking/movielens-ovr-linear-svc-calibrated/\")\n",
    "INTERIM_DATA_ROOT = os.path.abspath(\"../../../data/interim/movielens-ml20m-imdb/\")\n",
    "PATH_TO_PROCESSED_FILE = os.path.abspath('../../../data/processed/movielens-20m-imdb-tags-and-synopses-2017-12-20.csv')\n",
    "\n",
    "# CONFIGS\n",
    "\n",
    "SEED= 42\n",
    "MAX_NB_WORDS = 1000\n",
    "NB_DOCS=0.3\n",
    "MIN_TAG_DF=10\n",
    "\n",
    "# preprocessing\n",
    "\n",
    "VECTORIZER_NORM = ['l1','l2',None]\n",
    "\n",
    "W=20 # Pseudosentence size (in words) - not specified in the paper, taken from TextTiling default values\n",
    "K=10 # Size (in sentences) of the block used in the block comparison method - not specified in the paper, taken from TextTiling default values\n",
    "\n",
    "# calculating medoids\n",
    "NORMALIZATION = ['standard','minmax',None] # not specified in the paper\n",
    "SAMPLE_TO_NB_MEDOIDS_RATIO = [0.2,0.3,0.4] # not specified in the paper, but taken from MIMLSVM canonical implementation\n",
    "\n",
    "# classification\n",
    "SVM_KERNEL=['rbf','linear','poly'] # not specified in the paper, but taken from MIMLSVM canonical implementation\n",
    "SVM_GAMMA=[0.1,0.2,0.3,0.5,1.0,'auto'] # not specified in the paper, but taken from MIMLSVM canonical implementation\n",
    "SVM_C= [0.001,0.01,0.1,1]# not specified in the paper, but taken from MIMLSVM canonical implementation\n",
    "SVM_DEGREE=[3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_df = load_df_or_get_from_cache(PATH_TO_PROCESSED_FILE,INTERIM_DATA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove this for production\n",
    "docs_df = sample_rows(docs_df,NB_DOCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_df['sentences'] = docs_df['synopsis'].map(lambda row: sentence_tokenizer.tokenize(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "docs_df['num_sentences'] = docs_df['sentences'].map( lambda sents: len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df.iloc[0]['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok = TextTilingTokenizer(w=W, k=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_segments(candidates):\n",
    "    \n",
    "    try:\n",
    "        # we must manually insert \"\\n\\n\" because this is how \n",
    "        # texttilingtokenizer requires candidate boundaries to be \n",
    "        # represented.\n",
    "        segments = tok.tokenize(\"\\n\\n\".join(candidates))\n",
    "    except ValueError:\n",
    "        # this happens when the candidate list is too small for the \n",
    "        # text tiling tokenizer to be able to find segments. so just return\n",
    "        # the original sentences.\n",
    "        segments= candidates\n",
    "        \n",
    "    # now remove the artificially added chars\n",
    "    segments = [segment.replace(\"\\n\\n\",\" \").strip() for segment in segments]\n",
    "    \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "docs_df['segments'] = docs_df['sentences'].map(lambda candidates: extract_segments(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df['segments'][0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "segments = docs_df['segments'].values\n",
    "documents = docs_df['synopsis'].values\n",
    "labels = docs_df[\"tags\"].map(lambda tagstring: tagstring.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove for production\n",
    "labels = truncate_labels(labels,MIN_TAG_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_train_dataset(distance_matrix, medoid_indices):\n",
    "    \"\"\"\n",
    "    Returns a matrix where element Aij contains the distance from sample i to medoid j.\n",
    "\n",
    "    :param distance_matrix: MxM matrix with pairwise distances\n",
    "    :param medoid_indices: array of length N containing the indices of the medoids for each cluster\n",
    "    :return: distances to medoids (MxN matrix)\n",
    "    \"\"\"\n",
    "\n",
    "    return distance_matrix[:,medoid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_test_dataset(source_vectorized_segments, medoid_vectorized_segments):\n",
    "    \"\"\"\n",
    "    Calculates the distances from every source_document (reprsented by its segments) to every medoid\n",
    "    document (also represented by its segments) using the hausdorff distance.\n",
    "    \n",
    "    Returns a matrix where element Aij contains the distance from sample i to medoid j.\n",
    "\n",
    "    :param source_vectorized_segments: array of length M, where each element is a matrix with one row\n",
    "        for every segment in a source document\n",
    "    :param medoid_vectorized_segments: array of length N where each element is a matrix with one row\n",
    "        for every segment in a medoid document\n",
    "    :return: distances to medoids (MxN matrix)\n",
    "    \"\"\"\n",
    "    \n",
    "    num_test_samples = len(source_vectorized_segments)\n",
    "    num_medoids = len(medoid_vectorized_segments)\n",
    "    \n",
    "    test_dataset = np.zeros((num_test_samples,num_medoids))    \n",
    "    \n",
    "    for i,source_segments in enumerate(source_vectorized_segments):\n",
    "        for j,medoid_segments in enumerate(medoid_vectorized_segments):\n",
    "            test_dataset[i][j] = hausdorff(source_segments.toarray(),medoid_segments.toarray())\n",
    "            \n",
    "    return np.array(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segments, documents and labelsets are defined outside of the parameterGrid loop\n",
    "# because they're the same for every configuration    \n",
    "segments_train, segments_test, documents_train, documents_test, Y_train, Y_test = train_test_split(segments,\n",
    "                                                                                               documents,\n",
    "                                                                                               labels,\n",
    "                                                                                               test_size=0.15)\n",
    "\n",
    "print('total number of train documents: {}'.format(len(documents_train)))\n",
    "print('total number of validation documents: {}'.format(len(documents_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "\n",
    "# the binarizer needs to be fit on all labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(labels)\n",
    "\n",
    "Y_train = mlb.transform(Y_train)\n",
    "Y_test = mlb.transform(Y_test)\n",
    "\n",
    "print(\"total number of unique tags: {} \".format(len(mlb.classes_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters = [\n",
    "    {\n",
    "        'medoid_normalization':  NORMALIZATION,\n",
    "        'svm_kernel': SVM_KERNEL,\n",
    "        'svm_gamma': SVM_GAMMA,\n",
    "        'svm_c':SVM_C,\n",
    "        'svm_degree' :SVM_DEGREE,\n",
    "        'vectorizer_norm': VECTORIZER_NORM,\n",
    "        'nb_medoids_ratio': SAMPLE_TO_NB_MEDOIDS_RATIO\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total number of configurations to test: {}'.format(len(ParameterGrid(parameters))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_score = float(\"-inf\")\n",
    "best_configuration = None\n",
    "\n",
    "for (i,configuration) in enumerate(tqdm(ParameterGrid(parameters))):\n",
    "    \n",
    "    # TFIDF_VECTORIZER = COUNT_VECTORIZER + TFIDF_TRANSFORMER\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=MAX_NB_WORDS, norm=configuration['vectorizer_norm'])\n",
    "    # TRAINING SET\n",
    "    tfidf_vectorizer.fit(documents_train)\n",
    "    tfidf_segments_train = vectorize_segments(segments_train, tfidf_vectorizer)\n",
    "        \n",
    "    \n",
    "    # THE FOLLOWING BLOCK TAKES SOME TIME, BUT IT WILL ONLY RUN ONCE\n",
    "    \n",
    "    path_to_cache = INTERIM_DATA_ROOT.rstrip('/') + \"/mimlsvm/distance-matrix-train-sample-{}-{}-{}.p\".format(\n",
    "        NB_DOCS,\n",
    "        MAX_NB_WORDS,\n",
    "        configuration['vectorizer_norm'])\n",
    "    \n",
    "    if os.path.isfile(path_to_cache):\n",
    "        dist_matrix_train = pickle.load(open(path_to_cache,\"rb\"))\n",
    "    else:\n",
    "        print('Fitting distance matrix for norm={}'.format(configuration['vectorizer_norm']))\n",
    "        \n",
    "        dist_matrix_train = make_distance_matrix_for_segments(tfidf_segments_train)\n",
    "        pickle.dump(dist_matrix_train, open(path_to_cache, \"wb\"))\n",
    "    \n",
    "    # nb_medoids depends upon the dataset length\n",
    "    ratio = configuration['nb_medoids_ratio']\n",
    "    nb_medoids = int(len(tfidf_segments_train) * ratio)\n",
    "    \n",
    "    medoids_indices_train = k_medoids(dist_matrix_train,nb_medoids)[0]\n",
    "\n",
    "    X_train = make_train_dataset(dist_matrix_train,medoids_indices_train)\n",
    "    \n",
    "    # TEST SET\n",
    "    \n",
    "    tfidf_segments_test = vectorize_segments(segments_test, tfidf_vectorizer)\n",
    "          \n",
    "    # medoids trained on the training set\n",
    "    fitted_medoids = tfidf_segments_train[medoids_indices_train]\n",
    "    X_test = make_test_dataset(tfidf_segments_test,fitted_medoids)\n",
    "      \n",
    "    clf = OneVsRestClassifier(\n",
    "        SVC(kernel=configuration['svm_kernel'],\n",
    "            gamma=configuration['svm_gamma'],\n",
    "            C=configuration['svm_c'],\n",
    "            degree=configuration['svm_degree'],\n",
    "            probability=True,\n",
    "           ),n_jobs=-1)    \n",
    "    \n",
    "    if configuration['normalization'] == 'standard':      \n",
    "        scaler = StandardScaler()\n",
    "        X_train_final = scaler.fit_transform(X_train)\n",
    "        X_test_final = scaler.transform(X_test)\n",
    "    elif configuration['normalization'] == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_final = scaler.fit_transform(X_train)\n",
    "        X_test_final = scaler.transform(X_test)\n",
    "    else:\n",
    "        X_train_final = X_train\n",
    "        X_test_final = X_test\n",
    "    \n",
    "    # y_train was defined outside the loop    \n",
    "    clf.fit(X_train,Y_train)\n",
    "    \n",
    "    # train score\n",
    "    Y_pred_train = clf.predict_proba(X_train)\n",
    "    \n",
    "    # validation score\n",
    "    Y_pred_test = clf.predict_proba(X_test)  \n",
    "    \n",
    "    print(\"iter: {}, configuration: {}\\n\".format(i,configuration))\n",
    "    \n",
    "    ks = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "    for k in ks:\n",
    "#         print(\"train micro-F1 @{}: {}\".format(k,ranking.micro_f1_at_k(Y_train,Y_pred_train,k=k,normalize=True)))\n",
    "        print(\"validation micro-F1 @{}: {}\".format(k,ranking.micro_f1_at_k(Y_test,Y_pred_test,k=k,normalize=True)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Global TF Kernel (Python 3)",
   "language": "python",
   "name": "global-tf-python-3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
