{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see http://krex.k-state.edu/dspace/bitstream/handle/2097/9785/RahulChoubey2011.pdf\n",
    "\n",
    "This is an implementation of the first method of the two described above.\n",
    "\n",
    "Extract topics from the dataset using LDA then, for each test document, find out what are the most significant topics for it. Then they take the top 5 most likely words for the most likely topic and use those as recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import sklearn\n",
    "import sys\n",
    "import string\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity,cosine_distances\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV,ParameterGrid, train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier,NearestNeighbors\n",
    "\n",
    "from tqdm import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = os.path.join(os.getcwd(), os.pardir, '../../')\n",
    "sys.path.append(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.data.movielens_20m_imdb\n",
    "%aimport src.helpers.labels,src.helpers.neighbours, src.helpers.topics\n",
    "%aimport src.utils.dataframes, src.utils.clusters, src.utils.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.movielens_20m_imdb import load_df_or_get_from_cache\n",
    "from src.helpers.labels import truncate_labels\n",
    "from src.helpers.neighbours import get_predicted_labels_from_neighbours\n",
    "from src.helpers.segments import make_distance_matrix_for_segments,vectorize_segments\n",
    "from src.helpers.topics import get_word_weight_dict\n",
    "\n",
    "from src.utils.dataframes import sample_rows\n",
    "from src.utils.metrics import ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_ROOT = os.path.abspath(\"../../../models/ranking/movielens-topics/\")\n",
    "INTERIM_DATA_ROOT = os.path.abspath(\"../../../data/interim/movielens-ml20m-imdb/\")\n",
    "PATH_TO_PROCESSED_FILE = os.path.abspath('../../../data/processed/movielens-20m-imdb-tags-and-synopses-2017-12-13.csv')\n",
    "OUTPUT_FILE = 'output-'+ datetime.now().strftime('%Y-%m-%d-%H-%M-%S')+'.txt'\n",
    "\n",
    "# CONFIGS\n",
    "\n",
    "SEED= 42\n",
    "\n",
    "MAX_NB_WORDS = 5000\n",
    "\n",
    "# for sampling\n",
    "MIN_TAG_DF = 10\n",
    "\n",
    "STOP_WORDS='english' # using stopwords since most people using LDA do this\n",
    "NB_COMPONENTS = [50,100,200,400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = load_df_or_get_from_cache(PATH_TO_PROCESSED_FILE,INTERIM_DATA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>title</th>\n",
       "      <th>synopsis</th>\n",
       "      <th>tags</th>\n",
       "      <th>num_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>A boy called Andy Davis (voice: John Morris) u...</td>\n",
       "      <td>buy,want-to-see-again,unlikely-friendships,inn...</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>The film begins in 1869 in the town of Brantfo...</td>\n",
       "      <td>childish,robin-williams,time,not-for-kids,adap...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>Heat (1995)</td>\n",
       "      <td>An inbound Blue Line train pulls in to Firesto...</td>\n",
       "      <td>rviolence,soundtrack,suspense,dialogue,bibliot...</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>Sabrina (1995)</td>\n",
       "      <td>Sabrina Fairchild (Julia Ormond), is the Larra...</td>\n",
       "      <td>based-on-a-play,clv,remake,relationships,chick...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Tom and Huck (1995)</td>\n",
       "      <td>The film opens with Injun Joe (Eric Schweig) a...</td>\n",
       "      <td>based-on-a-book,adapted-frombook,seen</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id                title  \\\n",
       "0         1     Toy Story (1995)   \n",
       "1         2       Jumanji (1995)   \n",
       "2         6          Heat (1995)   \n",
       "3         7       Sabrina (1995)   \n",
       "4         8  Tom and Huck (1995)   \n",
       "\n",
       "                                            synopsis  \\\n",
       "0  A boy called Andy Davis (voice: John Morris) u...   \n",
       "1  The film begins in 1869 in the town of Brantfo...   \n",
       "2  An inbound Blue Line train pulls in to Firesto...   \n",
       "3  Sabrina Fairchild (Julia Ormond), is the Larra...   \n",
       "4  The film opens with Injun Joe (Eric Schweig) a...   \n",
       "\n",
       "                                                tags  num_tags  \n",
       "0  buy,want-to-see-again,unlikely-friendships,inn...        59  \n",
       "1  childish,robin-williams,time,not-for-kids,adap...        19  \n",
       "2  rviolence,soundtrack,suspense,dialogue,bibliot...        57  \n",
       "3  based-on-a-play,clv,remake,relationships,chick...        13  \n",
       "4              based-on-a-book,adapted-frombook,seen         3  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>num_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6710.000000</td>\n",
       "      <td>6710.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>41263.124888</td>\n",
       "      <td>12.214605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>39409.134389</td>\n",
       "      <td>14.369509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4106.250000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>31251.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>74531.500000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>131082.000000</td>\n",
       "      <td>189.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            movie_id     num_tags\n",
       "count    6710.000000  6710.000000\n",
       "mean    41263.124888    12.214605\n",
       "std     39409.134389    14.369509\n",
       "min         1.000000     1.000000\n",
       "25%      4106.250000     3.000000\n",
       "50%     31251.000000     7.000000\n",
       "75%     74531.500000    16.000000\n",
       "max    131082.000000   189.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = docs_df['synopsis'].values\n",
    "labels = docs_df[\"tags\"].map(lambda tagstring: tagstring.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of unique tags: 2138 \n",
      "total number of train documents: 5704\n",
      "total number of validation documents: 1006\n"
     ]
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "truncated_labels = truncate_labels(labels,MIN_TAG_DF)\n",
    "\n",
    "binary_labels = mlb.fit_transform(truncated_labels)\n",
    "\n",
    "print(\"total number of unique tags: {} \".format(len(mlb.classes_)))\n",
    "\n",
    "indices = np.arange(len(data))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "data = [data[i] for i in indices]\n",
    "targets = binary_labels[indices]\n",
    "num_validation_samples = int(0.15 * len(data))\n",
    "\n",
    "X_train = data[:-num_validation_samples]\n",
    "Y_train = targets[:-num_validation_samples]\n",
    "X_val = data[-num_validation_samples:]\n",
    "Y_val = targets[-num_validation_samples:]\n",
    "\n",
    "print('total number of train documents: {}'.format(len(X_train)))\n",
    "print('total number of validation documents: {}'.format(len(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(max_features=MAX_NB_WORDS, stop_words=STOP_WORDS)\n",
    "# it's ok to fit in the whole data since this is not training a model\n",
    "vect.fit(data)\n",
    "\n",
    "X_train_vect = vect.transform(X_train)\n",
    "X_val_vect = vect.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = dict()\n",
    "\n",
    "for nb_comp in NB_COMPONENTS:\n",
    "\n",
    "    if os.path.isfile(MODELS_ROOT+\"/lda-{}.p\".format(nb_comp)):\n",
    "        lda[nb_comp]=joblib.load(open(MODELS_ROOT+\"/lda-{}.p\".format(nb_comp),\"rb\"))\n",
    "    \n",
    "    else:\n",
    "        lda[nb_comp] = LatentDirichletAllocation(n_components=nb_comp, learning_method='online')\n",
    "        lda[nb_comp].fit(X_train_vect)\n",
    "        joblib.dump(lda[nb_comp],open(MODELS_ROOT+\"/lda-{}.p\".format(nb_comp),\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_score(nb_comp):\n",
    "    \n",
    "    vocabulary = vect.get_feature_names()\n",
    "    tag_vocabulary = mlb.classes_\n",
    "    \n",
    "    model = lda[nb_comp]\n",
    "    \n",
    "    X_train = model.transform(X_train_vect)\n",
    "    X_val = model.transform(X_val_vect)\n",
    "    \n",
    "    # training\n",
    "    \n",
    "    Y_pred_train = []\n",
    "\n",
    "    for (i,test_document_topics_distr) in enumerate(X_train):\n",
    "\n",
    "        # 'word' => weight in topic\n",
    "        word_weight_dict = get_word_weight_dict(model,0,vocabulary)\n",
    "\n",
    "        # only keep words that are valid tags\n",
    "        valid_tag_elems = [(k,v) for (k,v) in word_weight_dict.items() if k in tag_vocabulary]\n",
    "\n",
    "        valid_word_weight_dict = dict(valid_tag_elems)\n",
    "        valid_words = valid_word_weight_dict.keys()\n",
    "\n",
    "        # make a dummy y_score out of that (the numbers aren't probabililties\n",
    "        # but we're only interested in the ranking, not absolute numbers)\n",
    "        y_scores_dummy = [valid_word_weight_dict[tag_value] if tag_value in valid_words else 0.0 for (tag_index,tag_value) in enumerate(tag_vocabulary)]\n",
    "\n",
    "        y_scores_dummy = np.array(y_scores_dummy)\n",
    "\n",
    "        Y_pred_train.append(y_scores_dummy)\n",
    "\n",
    "    Y_pred_train = np.array(Y_pred_train)\n",
    "    joblib.dump(Y_pred_train,open(MODELS_ROOT+\"/y-pred-train-topic-words-{}.p\".format(nb_comp),\"wb\"))\n",
    "    \n",
    "    # validation\n",
    "    \n",
    "    Y_pred_val = []\n",
    "\n",
    "    for (i,test_document_topics_distr) in enumerate(X_val):\n",
    "\n",
    "        # 'word' => weight in topic\n",
    "        word_weight_dict = get_word_weight_dict(model,0,vocabulary)\n",
    "\n",
    "        # only keep words that are valid tags\n",
    "        valid_tag_elems = [(k,v) for (k,v) in word_weight_dict.items() if k in tag_vocabulary]\n",
    "\n",
    "        valid_word_weight_dict = dict(valid_tag_elems)\n",
    "        valid_words = valid_word_weight_dict.keys()\n",
    "\n",
    "        # make a dummy y_score out of that (the numbers aren't probabililties\n",
    "        # but we're only interested in the ranking, not absolute numbers)\n",
    "        y_scores_dummy = [valid_word_weight_dict[tag_value] if tag_value in valid_words else 0.0 for (tag_index,tag_value) in enumerate(tag_vocabulary)]\n",
    "\n",
    "        y_scores_dummy = np.array(y_scores_dummy)\n",
    "\n",
    "        Y_pred_val.append(y_scores_dummy)\n",
    "\n",
    "    Y_pred_val = np.array(Y_pred_val)\n",
    "    joblib.dump(Y_pred_val,open(MODELS_ROOT+\"/y-pred-val-topic-words-{}.p\".format(nb_comp),\"wb\"))\n",
    "    \n",
    "    # scoring\n",
    "    \n",
    "    print(\"scoring for nb_comp={}\".format(nb_comp))\n",
    "    \n",
    "    print(X_train.shape,X_val.shape,Y_train.shape,Y_val.shape)\n",
    "    \n",
    "    \n",
    "    with open(OUTPUT_FILE,'a+') as f:\n",
    "        \n",
    "        f.write('RESULTS FOR NB_COMP={} \\n'.format(nb_comp))\n",
    "        \n",
    "        ks = [1,2,3,4,5,6,7,8,9,10]\n",
    "        \n",
    "        f.write('UNNORMALIZED MICRO-F1:')\n",
    "        for k in ks:\n",
    "            f.write(\"train micro-F1 @{}: {}\".format(k,ranking.micro_f1_at_k(Y_train,Y_pred_train,k=k)))\n",
    "            f.write(\"validation micro-F1 @{}: {}\".format(k,ranking.micro_f1_at_k(Y_val,Y_pred_val,k=k)))\n",
    "\n",
    "        f.write('NORMALIZED MICRO-F1:')    \n",
    "        for k in ks:\n",
    "            f.write(\"train micro-F1 @{}: {}\".format(k,ranking.micro_f1_at_k(Y_train,Y_pred_train,k=k,normalize=True)))\n",
    "            f.write(\"validation micro-F1 @{}: {}\".format(k,ranking.micro_f1_at_k(Y_val,Y_pred_val,k=k,normalize=True)))\n",
    "        \n",
    "        f.write(\"\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nb_comp in NB_COMPONENTS:\n",
    "    train_and_score(nb_comp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
