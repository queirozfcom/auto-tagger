{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ovr-svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import sklearn\n",
    "import sys\n",
    "import string\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,average_precision_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV,ParameterGrid, train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler,MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from tqdm import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_dir = os.path.join(os.getcwd(), os.pardir, '../../')\n",
    "sys.path.append(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.data.movielens_20m_imdb\n",
    "%aimport src.helpers.labels,src.helpers.neighbours, src.helpers.segments,src.helpers.embeddings\n",
    "%aimport src.utils.dataframes, src.utils.clusters, src.utils.metrics,src.utils.plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.data.movielens_20m_imdb import load_df_or_get_from_cache\n",
    "from src.helpers.labels import truncate_labels\n",
    "from src.helpers.neighbours import get_predicted_labels_from_neighbours\n",
    "from src.helpers.segments import make_distance_matrix_for_segments,vectorize_segments\n",
    "from src.helpers.embeddings import read_glove_wiki_weighted\n",
    "\n",
    "\n",
    "from src.utils.dataframes import sample_rows\n",
    "from src.utils.metrics import ranking\n",
    "from src.utils.plotting import plot_micro_f1_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODELS_ROOT = os.path.abspath(\"../../../models/ranking/movielens-ovr-linear-svc-calibrated/\")\n",
    "INTERIM_DATA_ROOT = os.path.abspath(\"../../../data/interim/movielens-ml20m-imdb/\")\n",
    "PATH_TO_PROCESSED_FILE = os.path.abspath('../../../data/processed/movielens-20m-imdb-tags-and-synopses-2017-12-20.csv')\n",
    "\n",
    "# CONFIGS\n",
    "SEED= 42\n",
    "\n",
    "MAX_NB_WORDS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_df = load_df_or_get_from_cache(PATH_TO_PROCESSED_FILE,INTERIM_DATA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = docs_df[\"tags\"].map(lambda tagstring: tagstring.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of unique tags: 2138 \n",
      "total number of train documents: 5704\n",
      "total number of validation documents: 1006\n"
     ]
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "binary_labels = mlb.fit_transform(labels)\n",
    "\n",
    "print(\"total number of unique tags: {} \".format(len(mlb.classes_)))\n",
    "\n",
    "data = docs_df['synopsis'].values\n",
    "indices = np.arange(len(data))\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "data = [data[i] for i in indices]\n",
    "targets = binary_labels[indices]\n",
    "num_validation_samples = int(0.15 * len(data))\n",
    "\n",
    "X_train = data[:-num_validation_samples]\n",
    "Y_train = targets[:-num_validation_samples]\n",
    "X_val = data[-num_validation_samples:]\n",
    "Y_val = targets[-num_validation_samples:]\n",
    "\n",
    "print('total number of train documents: {}'.format(len(X_train)))\n",
    "print('total number of validation documents: {}'.format(len(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only fit on the train data because we're using the IDF scores to weigh the embeddings\n",
    "vect = TfidfVectorizer(max_features=MAX_NB_WORDS)\n",
    "vect.fit(X_train)\n",
    "\n",
    "feature_names = vect.get_feature_names()\n",
    "idf = vect.idf_\n",
    "idf_index = dict(zip(vect.get_feature_names(), idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# analyzer = preprocess + tokenize\n",
    "tokenize_func = vect.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    return tokenize_func(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tok = Parallel(n_jobs=2)(delayed(tokenize)(doc) for doc in X_train)\n",
    "X_val_tok = Parallel(n_jobs=2)(delayed(tokenize)(doc) for doc in X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transform into embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall, 1000 out of 1000 embeddings were weighted. Total available embeddings: 400000\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = read_glove_wiki_weighted(\n",
    "    d=100,\n",
    "    weight_index=idf_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bag_of_weighted_embeddings(document_tokens):\n",
    "    out = [embeddings_index[token] for token in document_tokens if token in embeddings_index.keys()]\n",
    "    \n",
    "    return np.mean(np.array(out),axis=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_boe = Parallel(n_jobs=2)(delayed(build_bag_of_weighted_embeddings)(tokenized_doc) for tokenized_doc in X_train_tok)\n",
    "X_train_boe = np.array(X_train_boe)\n",
    "\n",
    "X_val_boe = Parallel(n_jobs=2)(delayed(build_bag_of_weighted_embeddings)(tokenized_doc) for tokenized_doc in X_val_tok)\n",
    "X_val_boe = np.array(X_val_boe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('clf', OneVsRestClassifier(CalibratedClassifierCV(SVC(),cv=2),n_jobs=-1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = [\n",
    "    { \n",
    "        \"clf__estimator__base_estimator__kernel\": ['rbf','poly','linear'],\n",
    "        \"clf__estimator__base_estimator__degree\": [2,3,4,5]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf': OneVsRestClassifier(estimator=CalibratedClassifierCV(base_estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "   max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "   tol=0.001, verbose=False),\n",
       "             cv=2, method='sigmoid'),\n",
       "           n_jobs=-1),\n",
       " 'clf__estimator': CalibratedClassifierCV(base_estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "   max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "   tol=0.001, verbose=False),\n",
       "             cv=2, method='sigmoid'),\n",
       " 'clf__estimator__base_estimator': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "   max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "   tol=0.001, verbose=False),\n",
       " 'clf__estimator__base_estimator__C': 1.0,\n",
       " 'clf__estimator__base_estimator__cache_size': 200,\n",
       " 'clf__estimator__base_estimator__class_weight': None,\n",
       " 'clf__estimator__base_estimator__coef0': 0.0,\n",
       " 'clf__estimator__base_estimator__decision_function_shape': 'ovr',\n",
       " 'clf__estimator__base_estimator__degree': 3,\n",
       " 'clf__estimator__base_estimator__gamma': 'auto',\n",
       " 'clf__estimator__base_estimator__kernel': 'rbf',\n",
       " 'clf__estimator__base_estimator__max_iter': -1,\n",
       " 'clf__estimator__base_estimator__probability': False,\n",
       " 'clf__estimator__base_estimator__random_state': None,\n",
       " 'clf__estimator__base_estimator__shrinking': True,\n",
       " 'clf__estimator__base_estimator__tol': 0.001,\n",
       " 'clf__estimator__base_estimator__verbose': False,\n",
       " 'clf__estimator__cv': 2,\n",
       " 'clf__estimator__method': 'sigmoid',\n",
       " 'clf__n_jobs': -1,\n",
       " 'memory': None,\n",
       " 'steps': [('clf',\n",
       "   OneVsRestClassifier(estimator=CalibratedClassifierCV(base_estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "     decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "     max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "     tol=0.001, verbose=False),\n",
       "               cv=2, method='sigmoid'),\n",
       "             n_jobs=-1))]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__estimator__base_estimator__kernel': 'rbf', 'clf__estimator__base_estimator__degree': 2}\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for g in tqdm(ParameterGrid(parameters)):\n",
    "    print(g)\n",
    "    \n",
    "    pipeline.set_params(**g)\n",
    "    \n",
    "    pipeline.fit(X_train_boe,Y_train)\n",
    "    \n",
    "    Y_pred_train = pipeline.predict_proba(X_train_boe)    \n",
    "    Y_pred_val = pipeline.predict_proba(X_val_boe)\n",
    "    \n",
    "    train_scores = [ranking.micro_f1_at_k(Y_train,Y_pred_train,k=k,normalize=True) for k in ks]\n",
    "    validation_scores = [ranking.micro_f1_at_k(Y_val,Y_pred_val,k=k,normalize=True) for k in ks]\n",
    "    \n",
    "    print(train_scores)\n",
    "    print(validation_scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ks = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "train_scores = [ranking.micro_f1_at_k(Y_train,Y_pred_train,k=k,normalize=True) for k in ks]\n",
    "validation_scores = [ranking.micro_f1_at_k(Y_val,Y_pred_val,k=k,normalize=True) for k in ks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "img = plt.gcf()\n",
    "ax = plt.gca()\n",
    "plot_micro_f1_at_k(validation_scores,ax,train_scores)\n",
    "plt.gcf().set_size_inches(7,5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
