{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mimlsvm\n",
    "\n",
    "mi = mulit-instance\n",
    "ml = multi-label\n",
    "svm = svm\n",
    "\n",
    "As described in Shen et al 2009: http://ieeexplore.ieee.org/document/5346261/\n",
    "\n",
    "> Should we use SVM-struct instead? https://github.com/pystruct/pystruct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import sklearn\n",
    "import sys\n",
    "import string\n",
    "\n",
    "from hausdorff import hausdorff\n",
    "\n",
    "from nltk import TextTilingTokenizer\n",
    "from scipy.spatial.distance  import directed_hausdorff, pdist\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV,ParameterGrid, train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler,MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from tqdm import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_dir = os.path.join(os.getcwd(), os.pardir, '../src')\n",
    "sys.path.append(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%aimport data.movielens_20m_imdb\n",
    "%aimport helpers.labels,helpers.neighbours, helpers.segments\n",
    "%aimport utils.dataframes, utils.clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data.movielens_20m_imdb import load_or_get_from_cache\n",
    "from helpers.labels import truncate_labels\n",
    "from helpers.neighbours import get_predicted_labels_from_neighbours\n",
    "from helpers.segments import make_distance_matrix_for_segments,vectorize_segments\n",
    "\n",
    "from utils.dataframes import sample_rows\n",
    "from utils.clusters import k_medoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INTERIM_DATA_ROOT = os.path.abspath(\"../../data/interim/movielens-ml20m-imdb/\")\n",
    "ML_ROOT = \"/media/felipe/SAMSUNG/movielens/ml-20m/\"\n",
    "IMDB_ROOT = \"/media/felipe/SAMSUNG/imdb/\"\n",
    "\n",
    "PATH_TO_MOVIES = ML_ROOT + \"/movies.csv\"\n",
    "PATH_TO_TAG_ASSIGNMENTS = ML_ROOT + \"/tags.csv\"\n",
    "PATH_TO_MOVIE_PLOTS = IMDB_ROOT+\"/plot.list\"\n",
    "\n",
    "# CONFIGS\n",
    "\n",
    "MAX_NB_WORDS = 300\n",
    "PREPROC=None\n",
    "STOP_WORDS='english'\n",
    "VECTORIZER_NORM = 'l2'\n",
    "\n",
    "# for sampling\n",
    "NB_DOCS = 1000\n",
    "\n",
    "# Pseudosentence size (in words)\n",
    "W=20 # not specified in the paper, taken from TextTiling default values\n",
    "\n",
    "#  Size (in sentences) of the block used in the block comparison method\n",
    "K=10 # not specified in the paper, taken from TextTiling default values\n",
    "\n",
    "MIN_LABEL_DF = 5 # like in the paper\n",
    "\n",
    "SAMPLE_TO_NB_MEDOIDS_RATIO = 0.2 # not specified in the paper, but taken from MIMLSVM canonical implementation\n",
    "SVM_KERNEL='poly' # not specified in the paper, but taken from MIMLSVM canonical implementation\n",
    "SVM_GAMMA=0.2 # not specified in the paper, but taken from MIMLSVM canonical implementation\n",
    "SVM_C= 1# not specified in the paper, but taken from MIMLSVM canonical implementation\n",
    "SVM_DEGREE=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_df = load_or_get_from_cache(PATH_TO_MOVIES,PATH_TO_TAG_ASSIGNMENTS,PATH_TO_MOVIE_PLOTS,INTERIM_DATA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove this for production\n",
    "docs_df = sample_rows(docs_df,NB_DOCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_df['sentences'] = docs_df['plot'].map(lambda row: sentence_tokenizer.tokenize(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 1.14 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "docs_df['num_sentences'] = docs_df['sentences'].map( lambda sents: len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>title</th>\n",
       "      <th>unique_tags</th>\n",
       "      <th>num_users</th>\n",
       "      <th>num_unique_tags</th>\n",
       "      <th>plot</th>\n",
       "      <th>sentences</th>\n",
       "      <th>num_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5849</td>\n",
       "      <td>99178</td>\n",
       "      <td>Struck by Lightning (2012)</td>\n",
       "      <td>writers,teen,adapted-frombook,chris-colfer,reb...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>A high school boy, desperate to escape the idi...</td>\n",
       "      <td>[A high school boy, desperate to escape the id...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1370</td>\n",
       "      <td>3143</td>\n",
       "      <td>Hell in the Pacific (1968)</td>\n",
       "      <td>toshiro-mifune,john-boorman,toshirô-mifune,ene...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8</td>\n",
       "      <td>During World War II, a shot-down American pilo...</td>\n",
       "      <td>[During World War II, a shot-down American pil...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3083</td>\n",
       "      <td>7478</td>\n",
       "      <td>Swimming to Cambodia (1987)</td>\n",
       "      <td>cambodia,jonathan-demme,stylemonologue,one-man...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17</td>\n",
       "      <td>Spalding Gray sits behind a desk throughout th...</td>\n",
       "      <td>[Spalding Gray sits behind a desk throughout t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>Balto (1995)</td>\n",
       "      <td>sort-of-boring,simon-wells,dogsled,ei-muista,w...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>A half-wolf, half-husky named Balto gets a cha...</td>\n",
       "      <td>[A half-wolf, half-husky named Balto gets a ch...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4691</td>\n",
       "      <td>60943</td>\n",
       "      <td>Frozen River (2008)</td>\n",
       "      <td>smuggling,trailer-home,financial-problems,frie...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18</td>\n",
       "      <td>Takes place in the days before Christmas near ...</td>\n",
       "      <td>[Takes place in the days before Christmas near...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  movie_id                        title  \\\n",
       "0   5849     99178   Struck by Lightning (2012)   \n",
       "1   1370      3143   Hell in the Pacific (1968)   \n",
       "2   3083      7478  Swimming to Cambodia (1987)   \n",
       "3     11        13                 Balto (1995)   \n",
       "4   4691     60943          Frozen River (2008)   \n",
       "\n",
       "                                         unique_tags  num_users  \\\n",
       "0  writers,teen,adapted-frombook,chris-colfer,reb...        3.0   \n",
       "1  toshiro-mifune,john-boorman,toshirô-mifune,ene...        4.0   \n",
       "2  cambodia,jonathan-demme,stylemonologue,one-man...        8.0   \n",
       "3  sort-of-boring,simon-wells,dogsled,ei-muista,w...        5.0   \n",
       "4  smuggling,trailer-home,financial-problems,frie...       10.0   \n",
       "\n",
       "   num_unique_tags                                               plot  \\\n",
       "0               10  A high school boy, desperate to escape the idi...   \n",
       "1                8  During World War II, a shot-down American pilo...   \n",
       "2               17  Spalding Gray sits behind a desk throughout th...   \n",
       "3                5  A half-wolf, half-husky named Balto gets a cha...   \n",
       "4               18  Takes place in the days before Christmas near ...   \n",
       "\n",
       "                                           sentences  num_sentences  \n",
       "0  [A high school boy, desperate to escape the id...              4  \n",
       "1  [During World War II, a shot-down American pil...              6  \n",
       "2  [Spalding Gray sits behind a desk throughout t...              2  \n",
       "3  [A half-wolf, half-husky named Balto gets a ch...              8  \n",
       "4  [Takes place in the days before Christmas near...             12  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A high school boy, desperate to escape the idiocy of the people in his hometown, tries to create a way in which he can move to New York, attend the college of his dreams and do something other than live in the footsteps of his drunken, divorced mother.',\n",
       " \"Along the way he blackmails his fellow students into contributing to his literary magazine and discovers what it's like to feel accomplished.\",\n",
       " 'Does he get accepted into the college of his dreams?',\n",
       " 'Is he going to make a difference and follow his life goal?']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df.iloc[0]['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>num_users</th>\n",
       "      <th>num_unique_tags</th>\n",
       "      <th>num_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3196.091000</td>\n",
       "      <td>34120.533000</td>\n",
       "      <td>16.031000</td>\n",
       "      <td>11.740000</td>\n",
       "      <td>10.887000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1815.731887</td>\n",
       "      <td>36774.641119</td>\n",
       "      <td>27.787446</td>\n",
       "      <td>8.359562</td>\n",
       "      <td>7.838474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1605.250000</td>\n",
       "      <td>3714.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3270.500000</td>\n",
       "      <td>8550.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4693.250000</td>\n",
       "      <td>61018.750000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6307.000000</td>\n",
       "      <td>128671.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             index       movie_id    num_users  num_unique_tags  num_sentences\n",
       "count  1000.000000    1000.000000  1000.000000      1000.000000    1000.000000\n",
       "mean   3196.091000   34120.533000    16.031000        11.740000      10.887000\n",
       "std    1815.731887   36774.641119    27.787446         8.359562       7.838474\n",
       "min       0.000000       1.000000     2.000000         2.000000       1.000000\n",
       "25%    1605.250000    3714.250000     3.000000         4.000000       5.000000\n",
       "50%    3270.500000    8550.000000     6.000000         9.000000       9.000000\n",
       "75%    4693.250000   61018.750000    15.000000        20.000000      15.000000\n",
       "max    6307.000000  128671.000000   308.000000        25.000000      41.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok = TextTilingTokenizer(w=W, k=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_segments(candidates):\n",
    "    \n",
    "    try:\n",
    "        # we must manually insert \"\\n\\n\" because this is how \n",
    "        # texttilingtokenizer requires candidate boundaries to be \n",
    "        # represented.\n",
    "        segments = tok.tokenize(\"\\n\\n\".join(candidates))\n",
    "    except ValueError:\n",
    "        # this happens when the candidate list is too small for the \n",
    "        # text tiling tokenizer to be able to find segments. so just return\n",
    "        # the original sentences.\n",
    "        segments= candidates\n",
    "        \n",
    "    # now remove the artificially added chars\n",
    "    segments = [segment.replace(\"\\n\\n\",\" \").strip() for segment in segments]\n",
    "    \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.7 s, sys: 0 ns, total: 13.7 s\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "docs_df['segments'] = docs_df['sentences'].map(lambda candidates: extract_segments(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"A high school boy, desperate to escape the idiocy of the people in his hometown, tries to create a way in which he can move to New York, attend the college of his dreams and do something other than live in the footsteps of his drunken, divorced mother. Along the way he blackmails his fellow students into contributing to his literary magazine and discovers what it's like to feel accomplished. Does he get accepted into the college of his dreams? Is he going to make a difference and follow his life goal?\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df['segments'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "segments = docs_df['segments'].values\n",
    "documents = docs_df['plot'].values\n",
    "labelsets = truncate_labels(docs_df[\"unique_tags\"].map(lambda tagstring: tagstring.split(\",\")).values,MIN_LABEL_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I can't put this into a pipeline because NearestNeighbors is not a normal classifier, I think\n",
    "# I need to customize the pipeline object to be able to call the methods for that class.\n",
    "\n",
    "# TFIDF_VECTORIZER = COUNT_VECTORIZER + TFIDF_TRANSFORMER\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=MAX_NB_WORDS, preprocessor=PREPROC, stop_words=STOP_WORDS,norm=VECTORIZER_NORM)\n",
    "# segments => k-medoids\n",
    "clf = OneVsRestClassifier(SVC(kernel=SVM_KERNEL,gamma=SVM_GAMMA,C=SVM_C,degree=SVM_DEGREE),n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "segments_train, segments_test, documents_train, documents_test, y_train, y_test = train_test_split(segments,\n",
    "                                                                                                   documents,\n",
    "                                                                                                   labelsets,\n",
    "                                                                                                   test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the binarizer needs to be fit on all labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(labelsets)\n",
    "\n",
    "y_train = mlb.transform(y_train)\n",
    "y_test = mlb.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "433"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of individual tags\n",
    "len(mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((750, 433), (250, 433))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=300, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "tfidf_vectorizer.fit(documents_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_segments_train = vectorize_segments(segments_train, tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_segments_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6x300 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 67 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_segments_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB_DOCS is: 1000\n",
      "CPU times: user 0 ns, sys: 4 ms, total: 4 ms\n",
      "Wall time: 3.27 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# so that we know is the saved file refers to a sample or to the full file\n",
    "if NB_DOCS is None:\n",
    "    print(\"NB_DOCS is None\")\n",
    "    path_to_cache = INTERIM_DATA_ROOT.rstrip('/') + \"/mimlsvm/distance-matrix-train.p\"\n",
    "else:\n",
    "    print(\"NB_DOCS is: {}\".format(NB_DOCS))\n",
    "    path_to_cache = INTERIM_DATA_ROOT.rstrip('/') + \"/mimlsvm/distance-matrix-train-sample-{}.p\".format(NB_DOCS)\n",
    "    \n",
    "if os.path.isfile(path_to_cache):\n",
    "    dist_matrix_train = pickle.load(open(path_to_cache,\"rb\"))\n",
    "else:      \n",
    "    dist_matrix_train = make_distance_matrix_for_segments(tfidf_segments_train)\n",
    "    pickle.dump(dist_matrix_train, open(path_to_cache, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750, 750)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_matrix_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NB_MEDOIDS = int(len(tfidf_segments_train) * SAMPLE_TO_NB_MEDOIDS_RATIO)\n",
    "medoids_indices_train = k_medoids(dist_matrix_train,NB_MEDOIDS)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "medoids = tfidf_segments_train[medoids_indices_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medoids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_train_dataset(distance_matrix, medoid_indices):\n",
    "    \"\"\"\n",
    "    Returns a matrix where element Aij contains the distance from sample i to medoid j.\n",
    "\n",
    "    :param distance_matrix: MxM matrix with pairwise distances\n",
    "    :param medoid_indices: array of length N containing the indices of the medoids for each cluster\n",
    "    :return: distances to medoids (MxN matrix)\n",
    "    \"\"\"\n",
    "\n",
    "    return distance_matrix[:,medoid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,  11,  14,  25,  33,  34,  35,  39,  43, 228,  46, 641,  50,\n",
       "        51,  53,  55,  65,  66,  75,  79,  81,  83,  84,  85,  87,  97,\n",
       "       107, 111, 116, 122, 131, 134,  37, 139, 143, 144, 145, 331, 152,\n",
       "       748, 155, 156, 158, 176, 164, 182, 184, 195, 201,  63, 222, 223,\n",
       "       224,  92, 226, 236, 247, 262, 265, 274, 275, 276, 280, 283, 303,\n",
       "       248, 314, 316, 160, 731, 693, 342, 347, 330, 356, 359, 362, 364,\n",
       "       374, 381, 193, 394, 396, 403, 411, 413, 417, 420, 438, 440, 441,\n",
       "       447, 649, 186, 461, 191, 210, 474, 475, 476, 480, 481, 135, 471,\n",
       "       497, 498, 515, 527, 528, 545, 549, 227, 554, 556, 404, 566, 576,\n",
       "       398, 112, 591, 595, 392, 598, 601, 608, 610, 615,  16, 620, 363,\n",
       "       623, 629, 632, 637, 643, 644,   9, 659,  88, 671, 675, 676, 680,\n",
       "       704, 706, 318, 733, 739, 741, 631])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medoids_indices_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = make_train_dataset(dist_matrix_train,medoids_indices_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tfidf has been fit on the training set\n",
    "tfidf_segments_test = vectorize_segments(segments_test, tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_test_dataset(source_vectorized_segments, medoid_vectorized_segments):\n",
    "    \"\"\"\n",
    "    Calculates the distances from every source_document (reprsented by its segments) to every medoid\n",
    "    document (also represented by its segments) using the hausdorff distance.\n",
    "    \n",
    "    Returns a matrix where element Aij contains the distance from sample i to medoid j.\n",
    "\n",
    "    :param source_vectorized_segments: array of length M, where each element is a matrix with one row\n",
    "        for every segment in a source document\n",
    "    :param medoid_vectorized_segments: array of length N where each element is a matrix with one row\n",
    "        for every segment in a medoid document\n",
    "    :return: distances to medoids (MxN matrix)\n",
    "    \"\"\"\n",
    "    \n",
    "    num_test_samples = len(source_vectorized_segments)\n",
    "    num_medoids = len(medoid_vectorized_segments)\n",
    "    \n",
    "    test_dataset = np.zeros((num_test_samples,num_medoids))    \n",
    "    \n",
    "    for i,source_segments in enumerate(source_vectorized_segments):\n",
    "        for j,medoid_segments in enumerate(medoid_vectorized_segments):\n",
    "            test_dataset[i][j] = hausdorff(source_segments.toarray(),medoid_segments.toarray())\n",
    "            \n",
    "    return np.array(test_dataset)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = make_test_dataset(tfidf_segments_test,medoids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "X_test_norm = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm[350]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_preds = clf.predict(X_test)\n",
    "y_trues = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(y_preds[77],np.zeros(y_preds.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_trues,y_preds,average='micro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Global TF Kernel (Python 3)",
   "language": "python",
   "name": "global-tf-python-3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
