{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mimlsvm\n",
    "\n",
    "As described in Shen et al 2009: http://ieeexplore.ieee.org/document/5346261/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import sklearn\n",
    "import sys\n",
    "import string\n",
    "\n",
    "from hausdorff import hausdorff\n",
    "\n",
    "from nltk import TextTilingTokenizer\n",
    "from scipy.spatial.distance  import directed_hausdorff, pdist\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV,ParameterGrid, train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler,MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from tqdm import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = os.path.join(os.getcwd(), os.pardir, '../src')\n",
    "sys.path.append(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport data.movielens_20m_imdb\n",
    "%aimport helpers.labels,helpers.neighbours, helpers.segments\n",
    "%aimport utils.dataframes, utils.clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.movielens_20m_imdb import load_or_get_from_cache\n",
    "from helpers.labels import truncate_labels\n",
    "from helpers.neighbours import get_predicted_labels_from_neighbours\n",
    "from helpers.segments import make_distance_matrix_for_segments,vectorize_segments\n",
    "\n",
    "from utils.dataframes import sample_rows\n",
    "from utils.clusters import k_medoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERIM_DATA_ROOT = os.path.abspath(\"../../data/interim/movielens-ml20m-imdb/\")\n",
    "ML_ROOT = \"/media/felipe/SAMSUNG/movielens/ml-20m/\"\n",
    "IMDB_ROOT = \"/media/felipe/SAMSUNG/imdb/\"\n",
    "\n",
    "PATH_TO_MOVIES = ML_ROOT + \"/movies.csv\"\n",
    "PATH_TO_TAG_ASSIGNMENTS = ML_ROOT + \"/tags.csv\"\n",
    "PATH_TO_MOVIE_PLOTS = IMDB_ROOT+\"/plot.list\"\n",
    "\n",
    "# CONFIGS\n",
    "MAX_NB_WORDS = 300\n",
    "PREPROC=None\n",
    "STOP_WORDS='english'\n",
    "NORM=None\n",
    "\n",
    "MIN_LABEL_DF = 5 # like in the paper\n",
    "\n",
    "SAMPLE_TO_NB_MEDOIDS_RATIO = 0.2 # not specified in the paper, but taken from MIMLSVM canonical implementation\n",
    "SVM_KERNEL='poly'\n",
    "SVM_GAMMA=0.2 # not specified in the paper, but taken from MIMLSVM canonical implementation\n",
    "SVM_C= 1# not specified in the paper, but taken from MIMLSVM canonical implementation\n",
    "SVM_DEGREE=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = load_or_get_from_cache(PATH_TO_MOVIES,PATH_TO_TAG_ASSIGNMENTS,PATH_TO_MOVIE_PLOTS,INTERIM_DATA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove this for production\n",
    "docs_df = sample_rows(docs_df,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df['sentences'] = docs_df['plot'].map(lambda row: sentence_tokenizer.tokenize(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df['num_sentences'] = docs_df['sentences'].map( lambda sents: len(sents) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>title</th>\n",
       "      <th>unique_tags</th>\n",
       "      <th>num_users</th>\n",
       "      <th>num_unique_tags</th>\n",
       "      <th>plot</th>\n",
       "      <th>sentences</th>\n",
       "      <th>num_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>425</td>\n",
       "      <td>Blue Sky (1994)</td>\n",
       "      <td>tommy-lee-jones,affective-disorder,tony-richar...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>It's the early 1960s. Nuclear engineer Hank Ma...</td>\n",
       "      <td>[It's the early 1960s., Nuclear engineer Hank ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>4630</td>\n",
       "      <td>No Holds Barred (1989)</td>\n",
       "      <td>sportprofessional'-wrestling,thomas-j-wright,h...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>Rip is the World Wrestling Federation champion...</td>\n",
       "      <td>[Rip is the World Wrestling Federation champio...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3917</th>\n",
       "      <td>34542</td>\n",
       "      <td>Grizzly Man (2005)</td>\n",
       "      <td>documentary,werner-herzog,bears,biopic,eccentr...</td>\n",
       "      <td>37.0</td>\n",
       "      <td>25</td>\n",
       "      <td>A docudrama that centers on amateur grizzly be...</td>\n",
       "      <td>[A docudrama that centers on amateur grizzly b...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6277</th>\n",
       "      <td>119571</td>\n",
       "      <td>Teenage Dirtbag (2009)</td>\n",
       "      <td>regina-crosby,family-background,high-school</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>A popular high school girl is harassed by a de...</td>\n",
       "      <td>[A popular high school girl is harassed by a d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3066</th>\n",
       "      <td>7404</td>\n",
       "      <td>Anna (1987)</td>\n",
       "      <td>nudity-topless,betamax</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Anna is a middle-aged actress looking for work...</td>\n",
       "      <td>[Anna is a middle-aged actress looking for wor...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      movie_id                   title  \\\n",
       "207        425         Blue Sky (1994)   \n",
       "1999      4630  No Holds Barred (1989)   \n",
       "3917     34542      Grizzly Man (2005)   \n",
       "6277    119571  Teenage Dirtbag (2009)   \n",
       "3066      7404             Anna (1987)   \n",
       "\n",
       "                                            unique_tags  num_users  \\\n",
       "207   tommy-lee-jones,affective-disorder,tony-richar...        5.0   \n",
       "1999  sportprofessional'-wrestling,thomas-j-wright,h...        3.0   \n",
       "3917  documentary,werner-herzog,bears,biopic,eccentr...       37.0   \n",
       "6277        regina-crosby,family-background,high-school        2.0   \n",
       "3066                             nudity-topless,betamax        2.0   \n",
       "\n",
       "      num_unique_tags                                               plot  \\\n",
       "207                 6  It's the early 1960s. Nuclear engineer Hank Ma...   \n",
       "1999                7  Rip is the World Wrestling Federation champion...   \n",
       "3917               25  A docudrama that centers on amateur grizzly be...   \n",
       "6277                3  A popular high school girl is harassed by a de...   \n",
       "3066                2  Anna is a middle-aged actress looking for work...   \n",
       "\n",
       "                                              sentences  num_sentences  \n",
       "207   [It's the early 1960s., Nuclear engineer Hank ...             20  \n",
       "1999  [Rip is the World Wrestling Federation champio...              6  \n",
       "3917  [A docudrama that centers on amateur grizzly b...              7  \n",
       "6277  [A popular high school girl is harassed by a d...              2  \n",
       "3066  [Anna is a middle-aged actress looking for wor...              6  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = TextTilingTokenizer(w=5, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_segments(candidates,min_sentences=2):\n",
    "    \n",
    "    try:\n",
    "        # we must manually insert \"\\n\\n\" because this is how \n",
    "        # texttilingtokenizer requires candidate boundaries to be \n",
    "        # represented.\n",
    "        segments = tok.tokenize(\"\\n\\n\".join(candidates))\n",
    "    except ValueError:\n",
    "        # this happens when the candidate list is too small for the \n",
    "        # text tiling tokenizer to be able to find segments. so just return\n",
    "        # the original sentences.\n",
    "        segments= candidates\n",
    "        \n",
    "    # now remove the artificially added chars\n",
    "    segments = [segment.replace(\"\\n\\n\",\" \").strip() for segment in segments]\n",
    "    \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df['segments'] = docs_df['sentences'].map(lambda candidates: extract_segments(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(docs_df, open(INTERIM_DATA_ROOT.rstrip('/') + \"/segmented_docs_df.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = docs_df['segments'].values\n",
    "documents = docs_df['plot'].values\n",
    "labelsets = truncate_labels(docs_df[\"unique_tags\"].map(lambda tagstring: tagstring.split(\",\")).values,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can't put this into a pipeline because NearestNeighbors is not a normal classifier, I think\n",
    "# I need to customize the pipeline object to be able to call the methods for that class.\n",
    "\n",
    "# TFIDF_VECTORIZER = COUNT_VECTORIZER + TFIDF_TRANSFORMER\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=MAX_NB_WORDS, preprocessor=PREPROC, stop_words=STOP_WORDS,norm=NORM)\n",
    "# segments => k-medoids\n",
    "clf = OneVsRestClassifier(SVC(kernel=SVM_KERNEL,gamma=SVM_GAMMA,C=SVM_C,degree=SVM_DEGREE),n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_train, segments_test, documents_train, documents_test, y_train, y_test = train_test_split(segments,\n",
    "                                                                                                   documents,\n",
    "                                                                                                   labelsets,\n",
    "                                                                                                   test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the binarizer needs to be fit on all labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(labelsets)\n",
    "\n",
    "y_train = mlb.transform(y_train)\n",
    "y_test = mlb.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=300, min_df=1,\n",
       "        ngram_range=(1, 1), norm=None, preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "tfidf_vectorizer.fit(documents_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_segments_train = vectorize_segments(segments_train, tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix_train = make_distance_matrix_for_segments(tfidf_segments_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375, 375)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_matrix_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_MEDOIDS = int(len(tfidf_segments_train) * SAMPLE_TO_NB_MEDOIDS_RATIO)\n",
    "medoids_indices_train = k_medoids(dist_matrix_train,NB_MEDOIDS)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "medoids = tfidf_segments_train[medoids_indices_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medoids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_dataset(distance_matrix, medoid_indices):\n",
    "    \"\"\"\n",
    "    Returns a matrix where element Aij contains the distance from sample i to medoid j.\n",
    "\n",
    "    :param distance_matrix: MxM matrix with pairwise distances\n",
    "    :param medoid_indices: array of length N containing the indices of the medoids for each cluster\n",
    "    :return: distances to medoids (MxN matrix)\n",
    "    \"\"\"\n",
    "\n",
    "    return distance_matrix[:,medoid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  7,   8,  10,  14,  15,  18,  23,  24,  36,  47,  51,  54,  55,\n",
       "        62,  69,  71,  72,  74,  79,  80,  83,  84,  89,  93,  98,  99,\n",
       "       100, 116, 124,  37, 144, 150, 155, 163, 171,  27, 185, 112, 188,\n",
       "       197, 200,  35, 313, 211,   5, 217,  60, 220, 222, 224, 226, 231,\n",
       "       235, 239, 242, 253, 254, 262, 269, 272, 279, 162, 208,   6, 176,\n",
       "       282, 319, 321, 323, 227, 346, 348, 349, 362,  12])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medoids_indices_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = make_train_dataset(dist_matrix_train,medoids_indices_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf has been fit on the training set\n",
    "tfidf_segments_test = vectorize_segments(segments_test, tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_dataset(source_vectorized_segments, medoid_vectorized_segments):\n",
    "    \"\"\"\n",
    "    Calculates the distances from every source_document (reprsented by its segments) to every medoid\n",
    "    document (also represented by its segments) using the hausdorff distance.\n",
    "    \n",
    "    Returns a matrix where element Aij contains the distance from sample i to medoid j.\n",
    "\n",
    "    :param source_vectorized_segments: array of length M, where each element is a matrix with one row\n",
    "        for every segment in a source document\n",
    "    :param medoid_vectorized_segments: array of length N where each element is a matrix with one row\n",
    "        for every segment in a medoid document\n",
    "    :return: distances to medoids (MxN matrix)\n",
    "    \"\"\"\n",
    "    \n",
    "    num_test_samples = len(source_vectorized_segments)\n",
    "    num_medoids = len(medoid_vectorized_segments)\n",
    "    \n",
    "    test_dataset = np.zeros((num_test_samples,num_medoids))    \n",
    "    \n",
    "    for i,source_segments in enumerate(source_vectorized_segments):\n",
    "        for j,medoid_segments in enumerate(medoid_vectorized_segments):\n",
    "            test_dataset[i][j] = hausdorff(source_segments.toarray(),medoid_segments.toarray())\n",
    "            \n",
    "    return np.array(test_dataset)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = make_test_dataset(tfidf_segments_test,medoids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 15.71776889,  13.62023676,  10.38704136,  12.82214412,\n",
       "        10.57465503,  17.73685149,  12.02562574,  11.56034855,\n",
       "        12.20059657,  11.79844655,  15.49668636,  15.2526684 ,\n",
       "        17.42694854,  11.57759009,  21.38586592,  12.41514748,\n",
       "        15.53100654,  11.19993546,  11.17886795,  16.95841558,\n",
       "        15.11934932,  11.14643094,  11.06266367,  20.69020585,\n",
       "        10.04159697,  11.1355049 ,  12.91798264,  11.64691264,\n",
       "        20.4509587 ,  18.68276285,  17.60797368,  12.74959792,\n",
       "        11.29899783,  12.83522488,  11.83537744,  21.96676197,\n",
       "        10.17513773,  17.24596074,  14.28649236,  15.78405279,\n",
       "        23.0176805 ,  13.81791546,  10.02726717,  11.30980954,\n",
       "        23.04659591,  10.44440839,  23.65249612,  12.96481437,\n",
       "        11.55881968,  12.97768306,  10.69007906,  20.47765866,\n",
       "        14.95101036,  12.81259041,  27.77782411,  14.70965538,\n",
       "        11.55701458,  27.1245738 ,  11.22522191,  12.48769948,\n",
       "        16.43166835,  14.29239545,  12.97905173,   9.89899594,\n",
       "        15.22349217,  20.24072533,  11.72565893,  13.13155665,\n",
       "        11.96083565,  20.05820135,  10.20675326,  14.57769539,\n",
       "        15.3347883 ,  10.67531499,  15.81196382])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[350]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=4, gamma=0.2, kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "          n_jobs=-1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = clf.predict(X_test)\n",
    "y_trues = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(y_preds[77],np.zeros(y_preds.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_trues,y_preds,average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Global TF Kernel (Python 3)",
   "language": "python",
   "name": "global-tf-python-3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
