{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import sklearn\n",
    "import sys\n",
    "import string\n",
    "\n",
    "from nltk import TextTilingTokenizer\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV,ParameterGrid, train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler,MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from tqdm import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_dir = os.path.join(os.getcwd(), os.pardir, '../../')\n",
    "sys.path.append(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%aimport src.data.delicious_t140\n",
    "%aimport src.helpers.labels\n",
    "%aimport src.utils.dataframes, src.utils.clusters, src.utils.metrics, src.utils.distances, src.utils.plotting,src.helpers.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.features.delicious_t140 import clean_text_delicious\n",
    "from src.data.delicious_t140 import get_sample_from_cache\n",
    "from src.helpers.labels import truncate_labels\n",
    "from src.utils.dataframes import sample_rows\n",
    "from src.utils.metrics import ranking\n",
    "from src.utils.clusters import k_medoids\n",
    "from src.utils.distances import hausdorff\n",
    "from src.utils.plotting import plot_micro_f1_at_k\n",
    "\n",
    "from src.helpers.segments import make_distance_matrix_for_segments,vectorize_segments\n",
    "from src.helpers.embeddings import read_glove_wiki_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODELS_ROOT = os.path.abspath(\"../../../models/ranking/delicious-mimlsvm/\")\n",
    "DATA_ROOT = \"/media/felipe/SAMSUNG/delicious/delicioust140\"\n",
    "INTERIM_DATA_ROOT = os.path.abspath(\"../../../data/interim/delicious-t140/\")\n",
    "\n",
    "MAX_NB_WORDS = 500\n",
    "SEED= 42\n",
    "MIN_TAG_DF=10\n",
    "SAMPLE_FRAC=10\n",
    "EMBEDDINGS_DIM=100\n",
    "W=20 # Pseudosentence size (in words) - not specified in the paper, taken from TextTiling default values\n",
    "K=10 # Size (in sentences) of the block used in the block comparison method - not specified in the paper, taken from TextTiling default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cache_path = INTERIM_DATA_ROOT+\"/docs_df_with_segments-{}-sample-{}.p\".format(MAX_NB_WORDS,SEED)\n",
    "\n",
    "if os.path.isfile(cache_path):\n",
    "    print('cache hit')\n",
    "    docs_df = pickle.load(open(cache_path,\"rb\"))\n",
    "else:\n",
    "    print('cache miss. run again mimlsvm-tf-idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "segments = docs_df['segments'].values\n",
    "documents = docs_df['contents'].values\n",
    "labels = docs_df[\"tags\"].map(lambda tagstring: tagstring.split(\",\"))\n",
    "labels = truncate_labels(labels,MIN_TAG_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# segments, documents and labelsets are defined outside of the parameterGrid loop\n",
    "# because they're the same for every configuration    \n",
    "segments_train, segments_val, documents_train, documents_val, Y_train, Y_val = train_test_split(segments,\n",
    "                                                                                               documents,\n",
    "                                                                                               labels,\n",
    "                                                                                               test_size=0.15)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(labels)\n",
    "\n",
    "Y_train = mlb.transform(Y_train)\n",
    "Y_val = mlb.transform(Y_val)\n",
    "\n",
    "print('total number of train documents: {}'.format(len(documents_train)))\n",
    "print('total number of validation documents: {}'.format(len(documents_val)))\n",
    "print(\"total number of unique tags: {} \".format(len(mlb.classes_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(max_features=MAX_NB_WORDS)\n",
    "vect.fit(documents)\n",
    "\n",
    "feature_names = vect.get_feature_names()\n",
    "idf = vect.idf_\n",
    "idf_index = dict(zip(vect.get_feature_names(), idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# analyzer = preprocess + tokenize\n",
    "tokenize_func = vect.build_analyzer()\n",
    "\n",
    "def tokenize(string):\n",
    "    return tokenize_func(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train_tok = list()\n",
    "\n",
    "for document_segments in segments_train:\n",
    "    tokenized_segments = [tokenize(segment) for segment in document_segments]\n",
    "    X_train_tok.append(tokenized_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "X_val_tok = list()\n",
    "\n",
    "for document_segments in segments_val:\n",
    "    tokenized_segments = [tokenize(segment) for segment in document_segments]\n",
    "    X_val_tok.append(tokenized_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform into embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index = read_glove_wiki_weighted(\n",
    "    d=EMBEDDINGS_DIM,\n",
    "    weight_index=idf_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_bag_of_weighted_embeddings(tokens): \n",
    "    out = [embeddings_index[token] for token in tokens if token in embeddings_index.keys()]\n",
    "    return np.mean(np.array(out),axis=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cache_path = INTERIM_DATA_ROOT+\"/mimlsvm/mimlsvm-embeddings/X_train_boe_segments-sample-{}.p\".format(SAMPLE_FRAC)\n",
    "\n",
    "if os.path.isfile(cache_path):\n",
    "    print('cache hit')\n",
    "    X_train_boe_segments = pickle.load(open(cache_path,'rb'))\n",
    "else:\n",
    "    X_train_boe_segments = list()\n",
    "\n",
    "    for tokenized_document_segments in X_train_tok:\n",
    "        document_segments_boe = list()\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('error')\n",
    "            for seg in tokenized_document_segments:\n",
    "                if len(seg) == 0:\n",
    "                    print('empty segment')\n",
    "                    continue\n",
    "                else:\n",
    "                    try:\n",
    "                        boe=build_bag_of_weighted_embeddings(seg)\n",
    "                    except RuntimeWarning:\n",
    "                        print('segment entirely with OOV words')\n",
    "                        continue\n",
    "\n",
    "                    document_segments_boe.append(boe)\n",
    "\n",
    "        X_train_boe_segments.append(document_segments_boe)\n",
    "\n",
    "    pickle.dump(X_train_boe_segments,open(cache_path,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cache_path = INTERIM_DATA_ROOT+\"/mimlsvm/mimlsvm-embeddings/X_val_boe_segments-sample-{}.p\".format(SAMPLE_FRAC)\n",
    "\n",
    "if os.path.isfile(cache_path):\n",
    "    print('cache hit')\n",
    "    X_val_boe_segments = pickle.load(open(cache_path,'rb'))\n",
    "else:\n",
    "    X_val_boe_segments = list()\n",
    "\n",
    "    for tokenized_document_segments in X_val_tok:\n",
    "        document_segments_boe = list()\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('error')\n",
    "            for seg in tokenized_document_segments:\n",
    "                if len(seg) == 0:\n",
    "                    print('empty segment')\n",
    "                    continue\n",
    "                else:\n",
    "                    try:\n",
    "                        boe=build_bag_of_weighted_embeddings(seg)\n",
    "                    except RuntimeWarning:\n",
    "                        print('segment entirely with OOV words')\n",
    "                        continue\n",
    "\n",
    "                    document_segments_boe.append(boe)\n",
    "\n",
    "        X_val_boe_segments.append(document_segments_boe)\n",
    "\n",
    "    pickle.dump(X_val_boe_segments,open(cache_path,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_distance_matrix_for_embedding_segments(vectorized_segments, distance='hausdorff'):\n",
    "    \"\"\"\n",
    "    Returns the distance matrix for the documents having the given segments.\n",
    "\n",
    "    :param vectorized_segments: array of size M, where each element is a \"bag\" of segments: matrix of\n",
    "        shape (*,NUM_FEATURES), and each row on this matrix is the TF-IDF vector for one segment.\n",
    "    :param distance: how to compare the two bags\n",
    "    :return: pairwise distance matrix (MxM matrix)\n",
    "    \"\"\"\n",
    "    if distance.lower().strip() != 'hausdorff':\n",
    "        raise Exception(\"Only 'hausdorff' distance supported right now.\")\n",
    "\n",
    "    num_samples = len(vectorized_segments)\n",
    "\n",
    "    distance_function = lambda a, b: hausdorff(a, b)\n",
    "\n",
    "    distance_matrix = np.zeros((num_samples, num_samples))\n",
    "\n",
    "    for i, segments_for_document_a in enumerate(vectorized_segments):\n",
    "        for j, segments_for_document_b in enumerate(vectorized_segments):\n",
    "            distance = distance_function(segments_for_document_a, segments_for_document_b)\n",
    "            distance_matrix[i][j] = distance\n",
    "\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_train_dataset(distance_matrix, medoid_indices):\n",
    "    \"\"\"\n",
    "    Returns a matrix where element Aij contains the distance from sample i to medoid j.\n",
    "\n",
    "    :param distance_matrix: MxM matrix with pairwise distances\n",
    "    :param medoid_indices: array of length N containing the indices of the medoids for each cluster\n",
    "    :return: distances to medoids (MxN matrix)\n",
    "    \"\"\"\n",
    "\n",
    "    return distance_matrix[:,medoid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_validation_dataset(source_vectorized_segments, medoid_vectorized_segments):\n",
    "    \"\"\"\n",
    "    Calculates the distances from every source_document (reprsented by its segments) to every medoid\n",
    "    document (also represented by its segments) using the hausdorff distance.\n",
    "    \n",
    "    Returns a matrix where element Aij contains the distance from sample i to medoid j.\n",
    "\n",
    "    :param source_vectorized_segments: array of length M, where each element is a matrix with one row\n",
    "        for every segment in a source document\n",
    "    :param medoid_vectorized_segments: array of length N where each element is a matrix with one row\n",
    "        for every segment in a medoid document\n",
    "    :return: distances to medoids (MxN matrix)\n",
    "    \"\"\"\n",
    "    \n",
    "    num_test_samples = len(source_vectorized_segments)\n",
    "    num_medoids = len(medoid_vectorized_segments)\n",
    "    \n",
    "    test_dataset = np.zeros((num_test_samples,num_medoids))    \n",
    "    \n",
    "    for i,source_segments in enumerate(source_vectorized_segments):\n",
    "        for j,medoid_segments in enumerate(medoid_vectorized_segments):\n",
    "            test_dataset[i][j] = hausdorff(source_segments.toarray(),medoid_segments.toarray())\n",
    "            \n",
    "    return np.array(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### these were found by grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_parameters = [\n",
    "    {\n",
    "        'medoid_normalization':  [None],\n",
    "        'svm_kernel': ['poly'],\n",
    "        'svm_c':[1.0],\n",
    "        'svm_degree' :[3],\n",
    "        'svm_gamma':['auto'],\n",
    "        'vectorizer_norm': [None],\n",
    "        'nb_medoids_ratio': [0.2],\n",
    "        'max_features':[500]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for (i,configuration) in tqdm(enumerate(ParameterGrid(final_parameters))):\n",
    "                 \n",
    "    # nb_medoids depends upon the dataset length\n",
    "    ratio = configuration['nb_medoids_ratio']\n",
    "    nb_medoids = int(len(documents_train) * ratio)\n",
    "    \n",
    "    # these are the document segments to be used as medoids\n",
    "    medoids_indices_train = k_medoids(dist_matrix_train,nb_medoids)[0]\n",
    "\n",
    "    # a matrix where element Aij contains the distance from sample i to medoid j.\n",
    "    X_train = make_train_dataset(dist_matrix_train,medoids_indices_train)\n",
    "    \n",
    "    # VALIDATION SET\n",
    "\n",
    "    fitted_medoids = list()\n",
    "    for medoid_index in medoids_indices_train:\n",
    "        fitted_medoids.append(X_train_boe_segments[medoid_index]) \n",
    "    \n",
    "    X_val = make_validation_dataset(X_val_boe_segments,fitted_medoids)     \n",
    "        \n",
    "    svm = SVC(kernel=configuration['svm_kernel'],\n",
    "            gamma=configuration['svm_gamma'],\n",
    "            C=configuration['svm_c'],\n",
    "            degree=configuration['svm_degree'])\n",
    "    \n",
    "    clf = OneVsRestClassifier(CalibratedClassifierCV(svm,cv=2),n_jobs=-1)        \n",
    "\n",
    "    if configuration['medoid_normalization'] == 'standard':      \n",
    "        scaler = StandardScaler()\n",
    "        X_train_final = scaler.fit_transform(X_train)\n",
    "        X_val_final = scaler.transform(X_val)\n",
    "    elif configuration['medoid_normalization'] == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_final = scaler.fit_transform(X_train)\n",
    "        X_val_final = scaler.transform(X_val)\n",
    "    else:\n",
    "        X_train_final = X_train\n",
    "        X_val_final = X_val\n",
    "    \n",
    "    # y_train was defined outside the loop    \n",
    "    clf.fit(X_train,Y_train)\n",
    "    \n",
    "    # train score\n",
    "    Y_pred_train = clf.predict_proba(X_train)\n",
    "    \n",
    "    # validation score\n",
    "    Y_pred_val = clf.predict_proba(X_val)  \n",
    "    \n",
    "    print(\"iter: {}, configuration: {}\\n\".format(i,configuration))\n",
    "    \n",
    "    ks = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "    for k in ks:\n",
    "#         print(\"train micro-F1 @{}: {}\".format(k,ranking.micro_f1_at_k(Y_train,Y_pred_train,k=k,normalize=True)))\n",
    "        print(\"validation micro-F1 @{}: {}\".format(k,ranking.micro_f1_at_k(Y_val,Y_pred_val,k=k,normalize=True)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for (i,configuration) in enumerate(ParameterGrid(final_parameters)):\n",
    "    \n",
    "    tfidf_vectorizer = CountVectorizer(\n",
    "        max_features=configuration['max_features'], \n",
    "        norm=configuration['vectorizer_norm'])\n",
    "    \n",
    "    # TRAINING SET\n",
    "    tfidf_vectorizer.fit(documents_train)\n",
    "    tfidf_segments_train = vectorize_segments(segments_train, tfidf_vectorizer)\n",
    "        \n",
    "    # THE FOLLOWING BLOCK TAKES SOME TIME, BUT IT WILL ONLY RUN ONCE\n",
    "    \n",
    "    path_to_cache = MODELS_ROOT.rstrip('/') + \"/distance-matrix-train-{}-{}-{}.p\".format(\n",
    "        configuration['max_features'],\n",
    "        configuration['vectorizer_norm'],\n",
    "        SAMPLE_FRAC)\n",
    "    \n",
    "    if os.path.isfile(path_to_cache):\n",
    "        print('cache hit')\n",
    "        dist_matrix_train = pickle.load(open(path_to_cache,\"rb\"))\n",
    "    else:\n",
    "        print('Fitting distance matrix for norm={}'.format(configuration['vectorizer_norm']))\n",
    "        \n",
    "        dist_matrix_train = make_distance_matrix_for_segments(tfidf_segments_train)\n",
    "        pickle.dump(dist_matrix_train, open(path_to_cache, \"wb\"))\n",
    "    \n",
    "    # nb_medoids depends upon the dataset length\n",
    "    ratio = configuration['nb_medoids_ratio']\n",
    "    nb_medoids = int(len(tfidf_segments_train) * ratio)\n",
    "    \n",
    "    medoids_indices_train = k_medoids(dist_matrix_train,nb_medoids)[0]\n",
    "\n",
    "    X_train = make_train_dataset(dist_matrix_train,medoids_indices_train)\n",
    "    \n",
    "    # TEST SET\n",
    "    \n",
    "    tfidf_segments_test = vectorize_segments(segments_test, tfidf_vectorizer)\n",
    "          \n",
    "    # medoids trained on the training set\n",
    "    fitted_medoids = tfidf_segments_train[medoids_indices_train]\n",
    "    X_test = make_test_dataset(tfidf_segments_test,fitted_medoids)     \n",
    "        \n",
    "    svm = SVC(kernel=configuration['svm_kernel'],\n",
    "            gamma=configuration['svm_gamma'],\n",
    "            C=configuration['svm_c'],\n",
    "            degree=configuration['svm_degree'])\n",
    "    \n",
    "    clf = OneVsRestClassifier(CalibratedClassifierCV(svm,cv=2),n_jobs=-1)        \n",
    "\n",
    "    if configuration['medoid_normalization'] == 'standard':      \n",
    "        scaler = StandardScaler()\n",
    "        X_train_final = scaler.fit_transform(X_train)\n",
    "        X_test_final = scaler.transform(X_test)\n",
    "    elif configuration['medoid_normalization'] == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_final = scaler.fit_transform(X_train)\n",
    "        X_test_final = scaler.transform(X_test)\n",
    "    else:\n",
    "        X_train_final = X_train\n",
    "        X_test_final = X_test\n",
    "    \n",
    "    # y_train was defined outside the loop    \n",
    "    clf.fit(X_train,Y_train)\n",
    "    \n",
    "    # train score\n",
    "    Y_pred_train = clf.predict_proba(X_train)\n",
    "    \n",
    "    # validation score\n",
    "    Y_pred_test = clf.predict_proba(X_test)  \n",
    "    \n",
    "    print(\"iter: {}, configuration: {}\\n\".format(i,configuration))\n",
    "    \n",
    "    ks = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "    for k in ks:\n",
    "        print(\"train micro-F1 @{}: {}\".format(k,ranking.micro_f1_at_k(Y_train,Y_pred_train,k=k,normalize=True)))\n",
    "        print(\"validation micro-F1 @{}: {}\".format(k,ranking.micro_f1_at_k(Y_test,Y_pred_test,k=k,normalize=True)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "img = plt.gcf()\n",
    "ax = plt.gca()\n",
    "validation_scores = [\n",
    "\n",
    "]\n",
    "plot_micro_f1_at_k(validation_scores,ax)\n",
    "plt.gcf().set_size_inches(7,5)\n",
    "plt.gca().legend_.remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
