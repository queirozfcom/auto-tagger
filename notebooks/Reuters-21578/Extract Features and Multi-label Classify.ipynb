{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import nltk\n",
    "import sklearn\n",
    "import random\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reuters 21-578 modApte version\n",
    "> a collection of 10,788 documents from the Reuters financial newswire service, partitioned into a training set with 7769 documents and a test set with 3019 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /home/felipe/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/felipe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('reuters')\n",
    "nltk.download('punkt') # needed for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ZipFilePathPointer(u'/home/felipe/nltk_data/corpora/reuters.zip', u'reuters/')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = nltk.corpus.reuters\n",
    "dataset.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dataset.readme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10788"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training/5115'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileids = dataset.fileids()\n",
    "sample_fileid = [ fileids[i] for i in sorted(random.sample(xrange(len(fileids)), 1)) ][0]\n",
    "sample_fileid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ZipFilePathPointer(u'/home/felipe/nltk_data/corpora/reuters.zip', u'reuters/training/5115')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.abspath(sample_fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.words(sample_fileid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'U', u'.', u'S', u'.', u'SAYS', u'CANADA', ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.words(sample_fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'U.S. SAYS CANADA COMPLYING WITH LUMBER PACT\\n  The Commerce Department said that\\n  all Canadian firms had begun to pay an agreed to 15 pct\\n  surcharge on softwood shipped to U.S. markets.\\n      It made the statement after talks with Canadian officials\\n  about press reports and speculation in Canada that some\\n  exporters were not paying the charge.\\n      Canada and the United States agreed last December to the 15\\n  pct charge, ending a lengthy trade dispute over alleged\\n  Canadian subsidies to Canada\\'s softwood exporters.\\n      Commerce officials would not say if they found any Canadian\\n  companies had been evading the charge, but that following the\\n  talks they were convinced all exporters were complying with the\\n  agreement.\\n      Undersecretary of Commerce Bruce Smart said \"We are\\n  gratified to learn that companies in Canada have begun paying\\n  the export charge on lumber.\"\\n      He added the agreement was important to the health of the\\n  U.S. lumber industry and he intended to see that it was fully\\n  carried out.\\n  \\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.raw(sample_fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'U', u'.', u'S', u'.', u'SAYS', u'CANADA', ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.words(sample_fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'U', u'.', u'S', u'.', u'SAYS', u'CANADA', u'COMPLYING', u'WITH', u'LUMBER', u'PACT', u'The', u'Commerce', u'Department', u'said', u'that', u'all', u'Canadian', u'firms', u'had', u'begun', u'to', u'pay', u'an', u'agreed', u'to', u'15', u'pct', u'surcharge', u'on', u'softwood', u'shipped', u'to', u'U', u'.', u'S', u'.', u'markets', u'.'], [u'It', u'made', u'the', u'statement', u'after', u'talks', u'with', u'Canadian', u'officials', u'about', u'press', u'reports', u'and', u'speculation', u'in', u'Canada', u'that', u'some', u'exporters', u'were', u'not', u'paying', u'the', u'charge', u'.'], ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sents(sample_fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[u'U', u'.', u'S', u'.', u'SAYS', u'CANADA', u'COMPLYING', u'WITH', u'LUMBER', u'PACT', u'The', u'Commerce', u'Department', u'said', u'that', u'all', u'Canadian', u'firms', u'had', u'begun', u'to', u'pay', u'an', u'agreed', u'to', u'15', u'pct', u'surcharge', u'on', u'softwood', u'shipped', u'to', u'U', u'.', u'S', u'.', u'markets', u'.'], [u'It', u'made', u'the', u'statement', u'after', u'talks', u'with', u'Canadian', u'officials', u'about', u'press', u'reports', u'and', u'speculation', u'in', u'Canada', u'that', u'some', u'exporters', u'were', u'not', u'paying', u'the', u'charge', u'.'], [u'Canada', u'and', u'the', u'United', u'States', u'agreed', u'last', u'December', u'to', u'the', u'15', u'pct', u'charge', u',', u'ending', u'a', u'lengthy', u'trade', u'dispute', u'over', u'alleged', u'Canadian', u'subsidies', u'to', u'Canada', u\"'\", u's', u'softwood', u'exporters', u'.'], [u'Commerce', u'officials', u'would', u'not', u'say', u'if', u'they', u'found', u'any', u'Canadian', u'companies', u'had', u'been', u'evading', u'the', u'charge', u',', u'but', u'that', u'following', u'the', u'talks', u'they', u'were', u'convinced', u'all', u'exporters', u'were', u'complying', u'with', u'the', u'agreement', u'.'], [u'Undersecretary', u'of', u'Commerce', u'Bruce', u'Smart', u'said', u'\"', u'We', u'are', u'gratified', u'to', u'learn', u'that', u'companies', u'in', u'Canada', u'have', u'begun', u'paying', u'the', u'export', u'charge', u'on', u'lumber', u'.\"'], [u'He', u'added', u'the', u'agreement', u'was', u'important', u'to', u'the', u'health', u'of', u'the', u'U', u'.', u'S', u'.', u'lumber', u'industry', u'and', u'he', u'intended', u'to', u'see', u'that', u'it', u'was', u'fully', u'carried', u'out', u'.']]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.paras(sample_fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "corpus_train = []\n",
    "corpus_test = []\n",
    "for fileid in dataset.fileids():\n",
    "    document = dataset.raw(fileid)\n",
    "    if re.match('training/',fileid):\n",
    "        corpus_train.append(document)\n",
    "    else:\n",
    "        corpus_test.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7769, 3019)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_train),len(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessor(string):\n",
    "    repl = re.sub('&lt;','',string)\n",
    "    return repl.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "                min_df=10, # tweaking this parameter reduces the length of the feature vector\n",
    "                strip_accents='ascii',\n",
    "                preprocessor=preprocessor,\n",
    "                stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7769, 6462), (3019, 6462), (10788, 6462))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to use both corpuses for fitting because otherwise there may be words that only occur in the\n",
    "# training set or in the test set\n",
    "full_corpus = corpus_train + corpus_test\n",
    "vectorizer.fit(full_corpus)\n",
    "\n",
    "X_train_counts = vectorizer.transform(corpus_train)\n",
    "X_test_counts = vectorizer.transform(corpus_test)\n",
    "X_full_counts = vectorizer.transform(full_corpus)\n",
    "\n",
    "X_train_counts.shape,X_test_counts.shape, X_full_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#uncomment these to see how the vectorizer is analyzing, tokenizing and preprocessing documents\n",
    "\n",
    "#vectorizer.build_analyzer()(dataset.raw(fileid))\n",
    "#vectorizer.build_tokenizer()(\"ADVANCED INSTITUTIONAL &lt;AIMS> CUTS WORKFORCE\\n  Advanced Institutional \")\n",
    "#vectorizer.build_preprocessor()(\"ADVANCED INSTITUTIONAL &lt;AIMS> CUTS WORKFORCE\\n  Advanced Institutional \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts[0].toarray().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_counts[0].toarray().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7769, 6462), (3019, 6462), (10788, 6462))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = TfidfTransformer()\n",
    "# again, we need to fit the transformer to all documents (train and test)\n",
    "transformer.fit(X_full_counts)\n",
    "\n",
    "X_train_tfidf = transformer.transform(X_train_counts)\n",
    "X_test_tfidf = transformer.transform(X_test_counts)\n",
    "X_full_tfidf = transformer.transform(X_full_counts)\n",
    "\n",
    "X_train_tfidf.shape, X_test_tfidf.shape, X_full_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.       ,  0.       ,  0.       , ...,  0.0466051,  0.       ,  0.       ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf[0].toarray().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tfidf[0].toarray().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7769, 90), (3019, 90))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = []\n",
    "Y_test = []\n",
    "\n",
    "for (idx,fileid) in enumerate(dataset.fileids()):    \n",
    "    categories = '*'.join(dataset.categories(fileid))\n",
    "\n",
    "    if re.match('training/',fileid):\n",
    "        Y_train.append(categories)\n",
    "    else:\n",
    "        Y_test.append(categories)\n",
    "\n",
    "series_train = pd.Series(Y_train)\n",
    "Y_train_df = series_train.str.get_dummies(sep='*')\n",
    "\n",
    "series_test = pd.Series(Y_test)\n",
    "Y_test_df = series_test.str.get_dummies(sep='*')\n",
    "\n",
    "Y_train = Y_train_df.values\n",
    "Y_test = Y_test_df.values\n",
    "\n",
    "Y_train.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "meta_clf = OneVsRestClassifier(clf)\n",
    "\n",
    "meta_clf.fit(X_train_tfidf,Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = meta_clf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76201298701298703"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(Y_test,Y_pred,average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
