{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import html\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('../embedding-classif/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CLASSPATH\"] = \"/home/felipe/Downloads/stanford\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helpers import files_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles,bodies, labels = files_helper.read_stackoverflow_sample_small(join=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "\n",
    "title,body=titles[i], bodies[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title,body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizePost(title, body, unescapeXml = False):\n",
    "    # tokenize the string the same way I tokenized the whole dataset in\n",
    "    # https://github.com/queirozfcom/auto-tagger/blob/master/scala/autotagger-spark-2/src/main/scala/ReadSOStanfordTokenize.scala\n",
    "    \n",
    "    HTML_TAGS_PATTERN = re.compile(\"\"\"<[^>]+>\"\"\")\n",
    "    WHITESPACE_OR_NEWLINE_PATTERN = re.compile(\"\"\"\\s+\"\"\")\n",
    "    FORWARD_SLASH_PATTERN = re.compile(\"\"\"((?<!\\d)/(?!\\d))\"\"\") # slash not preceded a digit, not followed by a digit\n",
    "    DOT_PATTERN = re.compile(\"\"\"((?!<=\\d)\\.(?!\\d))\"\"\")\n",
    "    TRIPLE_DOTS_PATTERN = re.compile(\"\"\"\\.\\s\\.\\s\\.\"\"\")\n",
    "    \n",
    "    OPTIONS = {\"ptb3Escaping\":False}\n",
    "    \n",
    "    titleOut = \"\"\n",
    "    bodyOut = \"\"\n",
    "    \n",
    "    if unescapeXml:\n",
    "        titleOut = html.unescape(title).lower()\n",
    "        bodyOut = html.unescape(body).lower()\n",
    "    else:\n",
    "        titleOut = title.lower()\n",
    "        bodyOut = body.lower()        \n",
    "    \n",
    "    # this is done to separate title from body with a period, if non exists\n",
    "    if titleOut.endswith(\"?\") or titleOut.endswith(\"!\") or titleOut.endswith(\".\"):\n",
    "        pass # do nothing\n",
    "    else:\n",
    "        titleOut = titleOut + \".\"\n",
    "    \n",
    "    bodyOut = HTML_TAGS_PATTERN.sub(\" \", bodyOut) # take out htmltags\n",
    "    bodyOut = FORWARD_SLASH_PATTERN.sub(\" / \", bodyOut) # otherwise the tokenizer doesn't consider a \"/\" as a delimiter\n",
    "    bodyOut = DOT_PATTERN.sub(\" . \", bodyOut) # to make all dots be delimiters, not just some (except in numbers)\n",
    "    bodyOut = WHITESPACE_OR_NEWLINE_PATTERN.sub(\" \", bodyOut) # replace multiple whitespaces/newlines with a single one\n",
    "        \n",
    "    combinedOut = titleOut + \" \" + bodyOut\n",
    "    \n",
    "    tokenizer = StanfordTokenizer(options=OPTIONS)\n",
    "    \n",
    "    tokens = tokenizer.tokenize(combinedOut)\n",
    "      \n",
    "    tokenizedOut = \" \".join(tokens)    \n",
    "\n",
    "    # after tokenizing, remove spaces between triple dots\n",
    "    \n",
    "    tokenizedOut = re.sub(TRIPLE_DOTS_PATTERN,\"...\",tokenizedOut)\n",
    "    tokenizedOut = re.sub(WHITESPACE_OR_NEWLINE_PATTERN, \" \",tokenizedOut)\n",
    "    \n",
    "    return tokenizedOut.strip()\n",
    "\n",
    "\n",
    "tokenizePost(title,body,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Global TF Kernel (Python 3)",
   "language": "python",
   "name": "global-tf-python-3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
