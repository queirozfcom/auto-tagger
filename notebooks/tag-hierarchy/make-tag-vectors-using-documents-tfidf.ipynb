{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from difflib import SequenceMatcher,get_close_matches\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Dropout, Input, Flatten, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, GlobalAvgPool1D, GlobalMaxPooling1D\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "from numpy import linalg as LA\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from tqdm import *\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../helpers/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# my stuff in the helpers/ directory\n",
    "import embeddings_helper, files_helper, texts_helper, metrics_helper, tags_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED=np.random.randint(1,1000)\n",
    "SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PICKLE_DIR_ROOT = \"/media/felipe/ssd_vol/auto-tagger/data/tag-hierarchy/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts, labels = files_helper.read_stackoverflow_sample_stanford_tokenized(\"Medium-Small-Sample-Posts-Shuffled\",ssd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "VALIDATION_SPLIT = 0.2\n",
    "LABELS_MIN_DOC_COUNT = int(20)\n",
    "BATCH_SIZE=32\n",
    "EMBEDDING_DIM=100\n",
    "NUM_EPOCHS=10\n",
    "TOKENIZER_FILTERS='' # I will perform tokenization myself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS,\n",
    "                     filters=TOKENIZER_FILTERS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# word => word_index_position\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# word_index_position => word\n",
    "inverse_word_index = texts_helper.build_inverse_word_index(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "truncated_labels = tags_helper.truncate_labels(labels,LABELS_MIN_DOC_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lb = preprocessing.MultiLabelBinarizer()\n",
    "binary_labels = lb.fit_transform(truncated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make each document (sequence of word indices) be truncated to \n",
    "# MAX_SEQUENCE_LENGTH\n",
    "tokenized_texts = []\n",
    "\n",
    "for seq in sequences:\n",
    "    truncated_seq = seq[:MAX_SEQUENCE_LENGTH]\n",
    "    tokenized_txt = \" \".join([inverse_word_index[idx] for idx in truncated_seq])\n",
    "    tokenized_texts.append(tokenized_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extracting IDF weights to weight the embeddings\n",
    "\n",
    "# snooping\n",
    "vect = TfidfVectorizer(max_features=MAX_NB_WORDS).fit(tokenized_texts)\n",
    "\n",
    "feature_names = vect.get_feature_names()\n",
    "idf = vect.idf_\n",
    "\n",
    "# word => word IDF\n",
    "idf_index = dict(zip(vect.get_feature_names(), idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "document_vectors = vect.transform(tokenized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_WORDS = document_vectors[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# document_id => [tag1, tag2, tag3, ...]\n",
    "document_tag_index = tags_helper.get_document_tag_index(binary_labels,lb.classes_)\n",
    "\n",
    "# tag => [document_id1, document_id2, document_id3, ...]\n",
    "tag_document_index = tags_helper.get_tag_document_index(binary_labels, lb.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_vocabulary = lb.classes_\n",
    "\n",
    "with open(\"tag-vocabulary.txt\",\"w+\") as f:\n",
    "    for t in tag_vocabulary:\n",
    "        f.write(t+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1704"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tag_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(tag_vocabulary,open(PICKLE_DIR_ROOT+\"/tag_vocabulary.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"use with gulp . i read about gulp and was quite taken by the . i want to try it out for myself but i am running into a little problem . i am used to using with grunt and i have no idea how to get to play nice with gulp . i 've come across this article which suggests there is no need for a plugin when using gulp . unfortunately it does n't really explain how to go about it and the example it links to does n't help me much . is there anyone who knows how to go about this ?\",\n",
       " ['gulp', 'jekyll'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 2\n",
    "tokenized_texts[i],document_tag_index[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build tag_vectors_index\n",
    "\n",
    "> if a tag is the ONLY tag assigned to some document, that's probably a more representative document of this tag, than if this tag were only one among many others\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tag => tag_vector\n",
    "tag_vectors_index = dict()\n",
    "\n",
    "for (tag,document_ids) in tag_document_index.items():\n",
    "    \n",
    "    tag_vector = np.zeros(NUM_WORDS)\n",
    "    \n",
    "    for document_id in document_ids:\n",
    "        document_vector = document_vectors[document_id]\n",
    "        num_tags = len(document_tag_index[document_id])\n",
    "\n",
    "\n",
    "        weight = 1.0 / num_tags\n",
    "\n",
    "        weighted_document_vector = document_vector * weight\n",
    "        \n",
    "        tag_vector = tag_vector + weighted_document_vector\n",
    "        \n",
    "    tag_vectors_index[tag] = np.asarray(tag_vector).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(tag_vectors_index,open(PICKLE_DIR_ROOT+\"/tag_vectors_index.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalized_tag_vectors_index = dict()    \n",
    "\n",
    "## THIS SUCKS.. WHY?\n",
    "## normalize vectors to make cosine similarity more accurate  \n",
    "for (tag,tag_vector) in tag_vectors_index.items():    \n",
    "    normalized_tag_vectors_index[tag] = LA.norm(tag_vector, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _cosine_similarity(a,b):\n",
    "    return 1 - spatial.distance.cosine(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_top_k_most_similar_tags(index, target_tag_name, k):  \n",
    "    \"\"\"\n",
    "    index is a dict tag_name => tag_vector\n",
    "    target_tag_name is a string\n",
    "    k is an int\n",
    "    \"\"\"\n",
    "    \n",
    "    target_tag_vector = index[target_tag_name]\n",
    "\n",
    "    # calculating the similarites\n",
    "    similarities = list()\n",
    "    curr_best = np.NINF\n",
    "    curr_tag = None\n",
    "\n",
    "    for (tag_name,tag_vector) in index.items():\n",
    "        \n",
    "        if np.array_equal(tag_vector,target_tag_vector):\n",
    "            continue\n",
    "        \n",
    "        sim = _cosine_similarity(target_tag_vector,tag_vector)\n",
    "            \n",
    "        similarities.append((tag_name,sim))\n",
    "\n",
    "        if sim > curr_best and tag_name != target_tag_name:\n",
    "            curr_best = sim\n",
    "            curr_tag = tag_name\n",
    "\n",
    "    sorted_similarities = sorted(similarities,key=lambda t: t[1],reverse=True)\n",
    "    \n",
    "    return sorted_similarities[:k]\n",
    "\n",
    "\n",
    "get_top_k_most_similar_tags(tag_vectors_index,'sql',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_tags = ['sql-server-2008','sql','sql-server','mysql','database','postgresql','select','join','oracle','tsql','sqlite']\n",
    "\n",
    "pairs = [ (a,b) for a in target_tags for b in target_tags if a != b ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (tag_a,tag_b) in pairs:\n",
    "    tag_a_vector = tag_vectors_index[tag_a]\n",
    "    tag_b_vector = tag_vectors_index[tag_b]\n",
    "    \n",
    "    sim = _cosine_similarity(tag_a_vector,tag_b_vector)\n",
    "    \n",
    "    print(\"SIM '{}' '{}' => {}\".format(tag_a,tag_b,sim))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if all other tags are, on average, more similar to A than to B, then A is probably more generic than B\n",
    "# should it be normalized by the difference between A and B?\n",
    "# will fail for tags that are below B?\n",
    "\n",
    "sql_vector = tag_vectors_index['sql']\n",
    "avg_diff = list()\n",
    "\n",
    "for (tag_name,tag_vector) in tag_vectors_index.items():\n",
    "    avg_diff.append(_cosine_similarity(tag_a_vector,sql_vector))\n",
    "    \n",
    "np.mean(np.array(avg_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_vector = tag_vectors_index['sql-server']\n",
    "avg_diff = list()\n",
    "\n",
    "for (tag_name,tag_vector) in tag_vectors_index.items():\n",
    "    avg_diff.append(_cosine_similarity(tag_a_vector,sql_vector))\n",
    "    \n",
    "np.mean(np.array(avg_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_vector = tag_vectors_index['sql-server-2008']\n",
    "avg_diff = list()\n",
    "\n",
    "for (tag_name,tag_vector) in tag_vectors_index.items():\n",
    "    avg_diff.append(_cosine_similarity(tag_a_vector,sql_vector))\n",
    "    \n",
    "np.mean(np.array(avg_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sim(tag_a,tag_b):\n",
    "    \n",
    "    sim = _cosine_similarity(tag_vectors_index[tag_a],tag_vectors_index[tag_b])\n",
    "    \n",
    "    return (tag_a,tag_b,sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tags = [tag for (tag,_) in tag_vectors_index.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sims = Parallel(n_jobs=-1)(delayed(get_sim)(a,b) for a in all_tags for b in all_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims[:10]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similarity_dict = dict()\n",
    "\n",
    "for tag_a,tag_b, sim in sims:\n",
    "    \n",
    "    if similarity_dict.get(tag_a) is None:\n",
    "        similarity_dict[tag_a] = [(tag_b,sim)]\n",
    "    else:\n",
    "        similarity_dict[tag_a].append((tag_b,sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_dict[\"sql\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(similarity_dict[\"sql\"],key=lambda tpl : tpl[1],reverse=True)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_similarity_dict = dict()\n",
    "\n",
    "for (tag, similarities_to_other_tags) in similarity_dict.items():\n",
    "    \n",
    "    sorted_similarities = sorted(similarities_to_other_tags,key=lambda tpl: tpl[1],reverse=True)\n",
    "    sorted_without_self = sorted_similarities[1:]\n",
    "    \n",
    "    sorted_similarity_dict[tag] = sorted_without_self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_similarity_dict[\"sql\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(sorted_similarity_dict,open(PICKLE_DIR_ROOT+\"/sorted_similarity_dict\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_cobrinha('android','android-service',sorted_similarity_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_cobrinha('asp.net','asp.net-mvc',sorted_similarity_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_cobrinha('asp.net-mvc','asp.net-mvc-5',sorted_similarity_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what about unrelated stuff?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_cobrinha('java','arrays',sorted_similarity_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_cobrinha('ruby','python-2.7',sorted_similarity_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_cobrinha('database','python-2.7',sorted_similarity_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "what's the mean and stddev of the similarity between all tags and each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "running_avgs = []\n",
    "\n",
    "for (tag, similarities_to_other_tags) in similarity_dict.items():\n",
    "    sims = [tpl[1] for tpl in similarities_to_other_tags]\n",
    "    \n",
    "    avg = np.array(sims).mean()\n",
    "    \n",
    "    running_avgs.append(avg)\n",
    "    \n",
    "np.array(running_avgs).mean(),np.array(running_avgs).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make_global_similarity_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_similarity_index = dict()\n",
    "\n",
    "for tag in tag_vocabulary:\n",
    "    \n",
    "    similarities_with_current_tag = list()\n",
    "    \n",
    "    for other_tag, similarity_to_other_tag in similarity_dict[tag]:\n",
    "        \n",
    "        print(similarity_to_other_tag)\n",
    "    \n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_similarity_index = dict()\n",
    "\n",
    "for tag in tag_vocabulary:\n",
    "    \n",
    "    similarities_with_current_tag = list()\n",
    "    \n",
    "    for other_tag, similarity_to_other_tag in sorted_similarity_dict[tag]:\n",
    "        \n",
    "        if other_tag != tag:\n",
    "            similarities_with_current_tag.append(similarity_to_other_tag)\n",
    "\n",
    "    global_avg_sim_wrt_tag = np.array(similarities_with_current_tag).mean()\n",
    "    \n",
    "    global_similarity_index[tag] = global_avg_sim_wrt_tag\n",
    "            \n",
    "pickle.dump(global_similarity_index,open(PICKLE_DIR_ROOT+\"/global_similarity_index.p\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_similarity_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Global TF Kernel (Python 3)",
   "language": "python",
   "name": "global-tf-python-3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
