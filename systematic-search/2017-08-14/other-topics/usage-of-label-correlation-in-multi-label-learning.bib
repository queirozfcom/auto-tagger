% Encoding: UTF-8

@InProceedings{kang_etal_2006,
  author    = {Kang, Feng and Jin, Rong and Sukthankar, Rahul},
  title     = {Correlated Label Propagation with Application to Multi-label Learning},
  booktitle = {Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2},
  year      = {2006},
  series    = {CVPR '06},
  pages     = {1719--1726},
  address   = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  acmid     = {1153652},
  doi       = {10.1109/CVPR.2006.90},
  isbn      = {0-7695-2597-0},
  numpages  = {8},
  url       = {http://dx.doi.org/10.1109/CVPR.2006.90},
}

@InBook{guo_schuurmans_2012,
  pages     = {355--370},
  title     = {Semi-supervised Multi-label Classification},
  publisher = {Springer Berlin Heidelberg},
  year      = {2012},
  author    = {Guo, Yuhong and Schuurmans, Dale},
  editor    = {Flach, Peter A. and De Bie, Tijl and Cristianini, Nello},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-642-33486-3},
  abstract  = {Labeled data is often sparse in common learning scenarios, either because it is too time consuming or too expensive to obtain, while unlabeled data is almost always plentiful. This asymmetry is exacerbated in multi-label learning, where the labeling process is more complex than in the single label case. Although it is important to consider semi-supervised methods for multi-label learning, as it is in other learning scenarios, surprisingly, few proposals have been investigated for this particular problem. In this paper, we present a new semi-supervised multi-label learning method that combines large-margin multi-label classification with unsupervised subspace learning. We propose an algorithm that learns a subspace representation of the labeled and unlabeled inputs, while simultaneously training a supervised large-margin multi-label classifier on the labeled portion. Although joint training of these two interacting components might appear intractable, we exploit recent developments in induced matrix norm optimization to show that these two problems can be solved jointly, globally and efficiently. In particular, we develop an efficient training procedure based on subgradient search and a simple coordinate descent strategy. An experimental evaluation demonstrates that semi-supervised subspace learning can improve the performance of corresponding supervised multi-label learning methods.},
  booktitle = {Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2012, Bristol, UK, September 24-28, 2012. Proceedings, Part II},
  doi       = {10.1007/978-3-642-33486-3_23},
  url       = {https://doi.org/10.1007/978-3-642-33486-3_23},
}

@Article{chen_etal_2008,
  author = {Gang Chen and Yangqiu Song and Fei Wang and Changshui Zhang},
  title  = {Semi-supervised Multi-label Learning by Solving a Sylvester Equation},
  year   = {2008},
  url    = {https://pdfs.semanticscholar.org/955d/807225cc9cd2646efa9f23df77f17f4abfab.pdf},
}

@InBook{godbole_sarawagi_2004,
  pages     = {22--30},
  title     = {Discriminative Methods for Multi-labeled Classification},
  publisher = {Springer Berlin Heidelberg},
  year      = {2004},
  author    = {Godbole, Shantanu and Sarawagi, Sunita},
  editor    = {Dai, Honghua and Srikant, Ramakrishnan and Zhang, Chengqi},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-540-24775-3},
  abstract  = {In this paper we present methods of enhancing existing discriminative classifiers for multi-labeled predictions. Discriminative methods like support vector machines perform very well for uni-labeled text classification tasks. Multi-labeled classification is a harder task subject to relatively less attention. In the multi-labeled setting, classes are often related to each other or part of a is-a hierarchy. We present a new technique for combining text features and features indicating relationships between classes, which can be used with any discriminative algorithm. We also present two enhancements to the margin of SVMs for building better models in the presence of overlapping classes. We present results of experiments on real world text benchmark datasets. Our new methods beat accuracy of existing methods with statistically significant improvements.},
  booktitle = {Advances in Knowledge Discovery and Data Mining: 8th Pacific-Asia Conference, PAKDD 2004, Sydney, Australia, May 26-28, 2004. Proceedings},
  doi       = {10.1007/978-3-540-24775-3_5},
  url       = {https://doi.org/10.1007/978-3-540-24775-3_5},
}

@InProceedings{ramage_etal_2009,
  author    = {Ramage, Daniel and Hall, David and Nallapati, Ramesh and Manning, Christopher D.},
  title     = {Labeled LDA: A Supervised Topic Model for Credit Attribution in Multi-labeled Corpora},
  booktitle = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1},
  year      = {2009},
  series    = {EMNLP '09},
  pages     = {248--256},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid     = {1699543},
  isbn      = {978-1-932432-59-6},
  location  = {Singapore},
  numpages  = {9},
  url       = {http://dl.acm.org/citation.cfm?id=1699510.1699543},
}

@InProceedings{wang_etal_2009,
  author    = {C. Wang and Shuicheng Yan and Lei Zhang and H. J. Zhang},
  title     = {Multi-label sparse coding for automatic image annotation},
  booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2009},
  pages     = {1643-1650},
  month     = {June},
  abstract  = {In this paper, we present a multi-label sparse coding framework for feature extraction and classification within the context of automatic image annotation. First, each image is encoded into a so-called supervector, derived from the universal Gaussian Mixture Models on orderless image patches. Then, a label sparse coding based subspace learning algorithm is derived to effectively harness multi-label information for dimensionality reduction. Finally, the sparse coding method for multi-label data is proposed to propagate the multi-labels of the training images to the query image with the sparse ℓ1 reconstruction coefficients. Extensive image annotation experiments on the Corel5k and Corel30k databases both show the superior performance of the proposed multi-label sparse coding framework over the state-of-the-art algorithms.},
  doi       = {10.1109/CVPR.2009.5206866},
  issn      = {1063-6919},
  keywords  = {Gaussian processes;feature extraction;image coding;learning (artificial intelligence);Corel30k database;Corel5k database;automatic image annotation;dimensionality reduction;feature extraction;image classification;image encoding;image supervector;multilabel sparse coding;orderless image patches;query image;subspace learning algorithm;universal Gaussian Mixture Models;Asia;Feature extraction;Humans;Image coding;Image databases;Image reconstruction;Image representation;Image retrieval;Image segmentation;Scattering},
}

@Article{gibaja_ventura_2014,
  author    = {Gibaja, Eva and Ventura, Sebastián},
  title     = {Multi-label learning: a review of the state of the art and ongoing research},
  journal   = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  year      = {2014},
  volume    = {4},
  number    = {6},
  pages     = {411--444},
  issn      = {1942-4795},
  abstract  = {Multi-label learning is quite a recent supervised learning paradigm. Owing to its capabilities to improve performance in problems where a pattern may have more than one associated class, it has attracted the attention of researchers, producing an increasing number of publications. This study presents an up-to-date overview about multi-label learning with the aim of sorting and describing the main approaches developed till now. The formal definition of the paradigm, the analysis of its impact on the literature, its main applications, works developed, pitfalls and guidelines, and ongoing research are presented. WIREs Data Mining Knowl Discov 2014, 4:411–444. doi: 10.1002/widm.1139For further resources related to this article, please visit the WIREs website.Conflict of interest: The authors have declared no conflicts of interest for this article.},
  doi       = {10.1002/widm.1139},
  publisher = {Wiley Periodicals, Inc},
  url       = {http://dx.doi.org/10.1002/widm.1139},
}

@Article{barezi_etal_2017,
  author   = {Elham J. Barezi and James T. Kwok and Hamid R. Rabiee},
  title    = {Multi-Label learning in the independent label sub-spaces},
  journal  = {Pattern Recognition Letters},
  year     = {2017},
  volume   = {97},
  pages    = {8 - 12},
  issn     = {0167-8655},
  doi      = {http://dx.doi.org/10.1016/j.patrec.2017.06.024},
  keywords = {Large scale learning, Multi label learning, Independent learning, rank1},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167865517302295},
}

@Comment{jabref-meta: databaseType:bibtex;}
