@ARTICLE{257701,
author={C. E. Wu and Y. Hsu and Y. H. Liu},
journal={IEEE Transactions on Computers},
title={A quantitative evaluation of cache types for high-performance computer systems},
year={1993},
volume={42},
number={10},
pages={1154-1162},
abstract={Parallel accesses to the table lookaside buffer (TLB) and cache array are crucial for high-performance computer systems, and the choice of cache types is one of the most important factors affecting cache performance. The authors classify caches according to both index and tag. Since both index and tag could be either virtual (V) or real (R), their classification results in four combinations or cache types. The real address caches with virtual tags for high-performance computer systems in this study are prediction-based, since index bins are generated from a small array and predictions could be false. As a result, they also discuss and evaluate real address MRU caches with real tags, and propose virtually indexed MRU caches with real tags. Each of the four cache types and MRU caches are discussed and evaluated using trace-driven simulation. The results show that a virtually indexed MRU cache with real tags is a good choice for high-performance computer systems},
keywords={buffer storage;performance evaluation;cache array;cache types;high-performance computer;index;prediction-based;real;table lookaside buffer;tag;virtual;Books;Cache storage;Computational modeling;High performance computing;Performance analysis;Prefetching;Taxonomy},
doi={10.1109/12.257701},
ISSN={0018-9340},
month={Oct},}
@INPROCEEDINGS{650558,
author={Tien-Fu Chen},
booktitle={Proceedings 1998 Fourth International Symposium on High-Performance Computer Architecture},
title={Supporting highly speculative execution via adaptive branch trees},
year={1998},
pages={185-194},
abstract={Most of the prediction mechanisms predict a single path to continue the execution on a branch. Alternatively, we may exploit parallelism from either possible paths of a branch, discarding wrong paths once the branch is resolved. This paper proposes a concept of adaptive branch trees (ABT) to support highly speculative execution for processors with deeper pipelines and wide issue widths. The basic idea of the adaptive branch tree is to dynamically keep track of alternative branch paths and to speculatively execute the code on the most likely path. Hence, unlike branch prediction mechanisms, the ABT scheme would not miss out misprediction paths since the scheme can eventually go back to other alternative paths when the machine has explored more pending branches. The branch tree is realized by an adaptive branch tree table. A token is associated with each basic block and operations in the entire basic blocks are tagged with the token. With a novel token assignment strategy, we can reconfigure the ABT by a shift operation once one branch is resolved. Our experiment results on the SPEC95 benchmarks show that the proposed approach can achieve significant branch penalty reduction for wide-issue processors},
keywords={instruction sets;microprogramming;parallel architectures;parallel programming;program control structures;software performance evaluation;trees (mathematics);ABT scheme;SPEC95 benchmarks;adaptive branch tree table;adaptive branch trees;branch penalty reduction;branch prediction mechanisms;experiment;parallelism;prediction mechanisms;shift operation;speculative execution;token assignment strategy;wide-issue processors;Accuracy;Clocks;Computer science;Councils;Parallel processing;Pipelines;Shape},
doi={10.1109/HPCA.1998.650558},
month={Feb},}
@ARTICLE{790456,
author={Yudong Zhu and N. J. Pelc},
journal={IEEE Transactions on Medical Imaging},
title={A spatiotemporal model of cyclic kinematics and its application to analyzing nonrigid motion with MR velocity images},
year={1999},
volume={18},
number={7},
pages={557-569},
abstract={The authors present a method (DMESH) for nonrigid cyclic motion analysis using a series of velocity images covering the cycle acquired, for example, from phase-contrast magnetic resonance imaging. The method is based on fitting a dynamic finite-element mesh model to velocity samples of an extended region, at all time frames. The model offers a flexible tradeoff between accuracy and reproducibility with controllable built-in spatiotemporal smoothing, which is determined by the fineness of the initially defined mesh and the richness of included Fourier harmonics. The method can further provide a prediction of the analysis reproducibility, along with the estimated motion and deformation quantities. Experiments have been conducted to validate the method and to verify the reproducibility prediction. Use of the method for motion analysis using displacement information (e.g., from magnetic resonance tagging) has also been explored.},
keywords={biomechanics;biomedical MRI;cardiology;finite element analysis;image motion analysis;image sequences;kinematics;medical image processing;physiological models;DMESH;Fourier harmonics;MR velocity images;controllable built-in spatiotemporal smoothing;cyclic kinematics;displacement information;dynamic finite-element mesh model;magnetic resonance imaging;medical diagnostic imaging;nonrigid cyclic motion analysis;nonrigid motion analysis;spatiotemporal model;velocity images series;Finite element methods;Image analysis;Image motion analysis;Kinematics;Magnetic analysis;Magnetic resonance imaging;Motion analysis;Reproducibility of results;Smoothing methods;Spatiotemporal phenomena;Aortic Valve Stenosis;Biomechanics;Computer Simulation;Heart Ventricles;Humans;Image Processing, Computer-Assisted;Magnetic Resonance Imaging, Cine;Muscle Contraction;Muscle, Skeletal;Myocardial Contraction;Phantoms, Imaging;Reproducibility of Results},
doi={10.1109/42.790456},
ISSN={0278-0062},
month={July},}
@ARTICLE{815447,
author={P. Scott},
journal={IEEE Spectrum},
title={Aerospace and military [Technology 2000 analysis and forecast]},
year={2000},
volume={37},
number={1},
pages={97-102},
abstract={If aerospace has not met the best and brightest predictions of a half-century ago, the years ahead still promise plenty of action. As the millennium turns, China and perhaps France will soon join the United States and Russia in being able to put men and women into space aboard their own home-built transports. And the demand for satellite launch services is booming, as last year saw the first commercial flight of the Ariane 5 rocket and the first commercial liftoff from Sea Launch, the innovative ocean-based launch platform. NASA, meanwhile, is still smarting from the back-to-back losses of two Mars probes and the temporary shutdown of its orbiting Hubble Space Telescope. On the military front, the US Air Force continues to push for the F-22 fighter aircraft, even as some in Congress would declare it dead in favor of the Joint Strike Fighter. Advanced submarines are on the drawing boards of several countries, including the United States. Plans for these underwater behemoths, like their highly maneuverable airborne brethren, call for ever-increasing amounts of automation and electronics. Civil aviation, after taking much flak in 1999 for overcrowded skies and Y2K impacts on air travel, is pushing confidently into the next century. Global positioning systems (GPS) are inspiring advances in navigational equipment, and replacement systems for outmoded air traffic control set-ups draw ever closer to widescale deployment. A big obstacle remains, however, to fulfilling these goals in the next century. It is the same one as has delayed putting a person on Mars or building an orbiting space station-the price tag},
keywords={Global Positioning System;aerospace;air traffic control;artificial satellites;military equipment;rockets;technological forecasting;underwater vehicles;vehicles;Ariane 5 rocket;China;F-22 fighter aircraft;France;GPS;Hubble Space Telescope;Joint Strike Fighter;Mars probes;NASA;Sea Launch;US Air Force;United States;advanced submarines;automation;civil aviation;electronics;global positioning systems;navigational equipment;ocean-based launch platform;replacement air traffic control;satellite launch services;technology forecast;Aerospace electronics;Mars;Military aircraft;NASA;Probes;Rockets;Satellites;Technology forecasting;Telescopes;Underwater vehicles},
doi={10.1109/6.815447},
ISSN={0018-9235},
month={Jan},}
@INPROCEEDINGS{836228,
author={A. G. Hatzigeorgiou and M. Reczko},
booktitle={Neural Networks, 1999. IJCNN '99. International Joint Conference on},
title={Feature recognition on expressed sequence tags of human DNA},
year={1999},
volume={5},
pages={3490-3493 vol.5},
abstract={Expressed sequence tags (EST) are small parts of DNA, which are used to clone new genes. One main characteristic of EST is that they contain more than 1% sequencing errors. What we need to know is which parts of the EST contain information about proteins, the so called coding regions. In this paper we describe an error-tolerant program for the prediction of such coding regions in EST. The program is based on a combination of statistical methods and several artificial neural networks (ANN). 89.7% of the nucleotides of a independent test set with 127 EST's are predicted correctly as to whether they are coding or noncoding. These results are independent of the existence of homologous gene or protein sequences and representative for the application to the largest part of all EST},
keywords={DNA;biology computing;feature extraction;neural nets;pattern recognition;statistical analysis;ANN;EST;artificial neural networks;coding regions;error-tolerant program;expressed sequence tags;feature recognition;homologous gene sequences;human DNA;independent test set;nucleotides;protein sequences;proteins;sequencing errors;statistical methods;Bioinformatics;Cloning;DNA;Error correction;Genomics;Humans;Polymers;Proteins;Sequences;Technical Activities Guide -TAG},
doi={10.1109/IJCNN.1999.836228},
ISSN={1098-7576},
month={},}
@INPROCEEDINGS{745954,
author={F. Behloul and M. Janier and P. Croisille and C. Poirier and A. Boudraa and R. Unterreiner and J. C. Mason and D. Revel},
booktitle={Proceedings of the 20th Annual International Conference of the IEEE Engineering in Medicine and Biology Society. Vol.20 Biomedical Engineering Towards the Year 2000 and Beyond (Cat. No.98CH36286)},
title={Automatic assessment of myocardial viability based on PET-MRI data fusion},
year={1998},
pages={492-495 vol.1},
abstract={In this paper, a fusion system which combines data generated by tagged magnetic resonance imaging (MRI) and F18-FDG positron emission tomography (PET) is presented. MRI and PET complementarity leads to a very accurate assessment of the myocardial viability in patients with coronary heart disease. An accurate viability analysis allows a better prediction of the successes of the revascularization procedure. The fusion system is based on soft computing techniques. It is a modular network which consists of four adaptive network-based fuzzy inference systems (ANFIS) organized in a hierarchical way. The network is able to learn and adapt itself and integrates expert knowledge. It is considered a valuable tool for clinical and research applications},
keywords={biomedical MRI;cardiology;feature extraction;fuzzy neural nets;fuzzy set theory;image registration;image segmentation;inference mechanisms;knowledge representation;medical expert systems;medical image processing;muscle;positron emission tomography;sensor fusion;F18-FDG positron emission tomography;PET-MRI data fusion;adaptive network-based fuzzy inference systems;automatic assessment;coronary heart disease;expert knowledge;fuzzy set theory;hierarchically organized;image registration;image segmentation;intelligent data fusion;knowledge representation;modular network;myocardial viability;object extraction;revascularization procedure;soft computing techniques;tagged magnetic resonance imaging;viable tissue quantification;Blood flow;Computer networks;Fuzzy set theory;Heart;Image analysis;Magnetic resonance imaging;Myocardium;Packaging;Positron emission tomography;Stress},
doi={10.1109/IEMBS.1998.745954},
ISSN={1094-687X},
month={Oct},}
@INPROCEEDINGS{349978,
author={B. Carse and T. C. Fogarty},
booktitle={Proceedings of the First IEEE Conference on Evolutionary Computation. IEEE World Congress on Computational Intelligence},
title={A delayed-action classifier system for learning in temporal environments},
year={1994},
pages={670-673 vol.2},
abstract={This paper describes a modified version of the traditional classifier system called the Delayed Action Classifier System (DACS) which has been conceived for learning in environments that exhibit a rich temporal structure. DACS operates by delaying the action of appropriately tagged classifiers (called `delayed-action classifiers') by a number of execution cycles which is encoded on the action part of these classifiers. This modification allows the rule discovery strategy, in many instances a genetic algorithm, to simultaneously explore the spaces of action (what to do) and time (when to do it). Results of initial experiments, which appear encouraging, of applying DACS to a prediction problem are presented, and the possible application of the delayed-action idea to learning in real-time environments is discussed},
keywords={genetic algorithms;learning (artificial intelligence);pattern recognition;temporal logic;temporal reasoning;appropriately tagged classifiers;delayed-action classifier system;execution cycles;genetic algorithm;learning;rule discovery strategy;temporal environments;temporal structure;Clocks;Delay effects;Delay systems;Genetic algorithms;Learning systems;Mathematics;Production systems;Space exploration;Stability},
doi={10.1109/ICEC.1994.349978},
month={Jun},}
@INPROCEEDINGS{758060,
author={B. Peskin and M. Newman and D. McAllaster and V. Nagesha and H. Richards and S. Wegmann and M. Hunt and L. Gillick},
booktitle={1999 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings. ICASSP99 (Cat. No.99CH36258)},
title={Improvements in recognition of conversational telephone speech},
year={1999},
volume={1},
pages={53-56 vol.1},
abstract={This paper describes recent changes in Dragon's speech recognition system which have markedly improved performance on conversational telephone speech. Key changes include: the conversion to modified perceptual linear prediction (PLP)-based cepstra from mel-cepstra; the replacement of our usual IMELDA transformation by a new transform using “semi-tied covariance”; a new multi-pass adaptation protocol; probabilities on alternate pronunciations in the lexicon; the addition of word-boundary tags in our acoustic models and the redistribution of model parameters to build fewer output distributions but with more mixture components per model},
keywords={cepstral analysis;prediction theory;probability;speech recognition;telephony;Dragon speech recognition system;acoustic models;alternate pronunciations probability;conversational telephone speech recognition;lexicon;mixture components;model parameters redistribution;modified PLP-based cepstra;modified perceptual linear prediction;multi-pass adaptation protocol;output distributions;performance improvement;semi-tied covariance;transform;word-boundary tags;Acoustic testing;Broadcasting;Error analysis;Natural languages;Protocols;Signal processing;Speech recognition;Standards development;Switches;Telephony},
doi={10.1109/ICASSP.1999.758060},
ISSN={1520-6149},
month={Mar},}
@INPROCEEDINGS{619393,
author={M. Pandey and R. E. Bryant},
booktitle={Proceedings. International Workshop on Memory Technology, Design and Testing (Cat. NO.97TB100159)},
title={Formal verification of memory arrays using symbolic trajectory evaluation},
year={1997},
pages={42-49},
abstract={Verification of memory arrays is an important part of processor verification. Memory arrays include circuits such as on-chip caches, cache tags, register files, and branch prediction buffers having memory cores embedded within complex logic. These circuits are typically custom designed at the transistor-level to optimize area and performance. This makes it necessary to verify them at the transistor-level. Conventional array verification approaches are based on switch-level simulation. Such approaches do not work for arrays as it is infeasible to simulate the astronomical number of simulation patterns that are required to verify these designs. Therefore, formal methods are required to ensure the correctness of memory arrays. This paper describes the formal verification technique of Symbolic Trajectory Evaluation (STE), and its application to verify memory arrays. The paper describes techniques to overcome the limitations of STE in verifying large complex memory arrays. It shows how exploiting symmetry allows one to verify systems several orders of magnitude larger than otherwise possible. The results of verifying SRAM arrays, including a 256 Kbit circuit having over 1.5 million transistors, are presented. The paper also shows how judicious Boolean encodings can be used with STE to efficiently verify CAMs},
keywords={SRAM chips;buffer storage;cellular arrays;content-addressable storage;integrated circuit testing;256 Kbit;Boolean encodings;CAMs;SRAM arrays;branch prediction buffers;cache tags;formal verification;memory arrays;memory cores;on-chip caches;processor verification;register files;symbolic trajectory evaluation;transistor-level verification;Boolean functions;Cams;Circuit simulation;Costs;Design optimization;Encoding;Formal verification;Logic arrays;Phased arrays;Random access memory},
doi={10.1109/MTDT.1997.619393},
ISSN={1087-4852},
month={Aug},}
@INPROCEEDINGS{808414,
author={Q. Cao and P. Trancoso and J. L. Larriba-Pey and J. Torrellas and R. Knighten and Y. Won},
booktitle={Proceedings 1999 IEEE International Conference on Computer Design: VLSI in Computers and Processors (Cat. No.99CB37040)},
title={Detailed characterization of a quad Pentium Pro server running TPC-D},
year={1999},
pages={108-115},
abstract={While database workloads consume a major fraction of the cycles in today's machines, there are only a few public-domain performance studies that characterize in detail how these workloads exercise the machines. This fact is due to the complexity of setting up and tuning database workloads, the high cost of the equipment required to evaluate them, and the frequent use of proprietary systems. In this paper, we help redress this problem by presenting a detailed performance characterization of the TPC-D benchmark running on a 4-processor Pentium Pro SMP multiprocessor with Windows NT and Microsoft's SQL Server. We use the Pentium Pro built-in hardware event counters and software tools that monitor system activity. Our results show that TPC-D queries have a relatively low CPI. The CPIs, which are 1.27 on average for the 17 read-only queries, are comparable to values observed for technical workloads. The major sources of processor stall cycles are the instruction fetch bottleneck and data misses in the secondary cache. Kernel time is negligible, as queries spend less than 6% of their time on average in the kernel. Other results show that branch prediction is effective in TPC-D and that the exclusive state in the cache tags is largely unnecessary. Finally, we compare our results to the ones published for TPC-C},
keywords={file servers;multiprocessing systems;performance evaluation;Pentium Pro SMP multiprocessor;Pentium Pro server;TPC-D;TPC-D benchmark;database workloads;performance characterization;performance studies;Bismuth},
doi={10.1109/ICCD.1999.808414},
ISSN={1063-6404},
month={},}
@ARTICLE{34414,
author={D. Errede and M. Sheaff and H. Fenker and L. Lueking and P. Mantsch},
journal={IEEE Transactions on Nuclear Science},
title={Design and performance characteristics of the E769 beamline transition radiation detector},
year={1989},
volume={36},
number={1},
pages={106-111},
abstract={A transition radiation detector (TRD) was designed and built for E769, a Fermilab fixed target experiment for use in separating pions from protons or kaons in a 250-GeV/c positive beam at the Tagged Photon Laboratory. Requirements placed on the detector were that it operate in a high rate (≈2 MHz) environment and that it be relatively easy to build since it had to be ready approximately one year from the date of its inception. The short time available precluded exposing prototypes to a test beam, making it necessary to rely on source testing and Monte Carlo programs to predict the detector performance. When operated in the beam, the detector performance was in good agreement with these predictions. For a pion detection efficiency of 87%, the contamination by protons of a sample of TRD tagged pions was 2%},
keywords={meson detection and measurement;particle detectors;E769 beamline transition radiation detector;Fermilab fixed target experiment;kaons;pions;protons;Contamination;Laboratories;Mesons;Monte Carlo methods;Particle beams;Physics;Protons;Radiation detectors;Tagging;Testing},
doi={10.1109/23.34414},
ISSN={0018-9499},
month={Feb},}
@ARTICLE{366322,
author={Y. S. Kim and R. Eng},
journal={IEEE Transactions on Aerospace and Electronic Systems},
title={Time-of-arrival prediction model for transionospheric EMP},
year={1995},
volume={31},
number={1},
pages={409-413},
abstract={An ionospheric-delay correction model that measures the time of arrival (TOA) of transionospheric electromagnetic pulse (EMP) is presented. The pulse-broadening effect due to ionosphere is incorporated into the ionospheric group delay model to provide more accurate delay correction for measuring EMP TOA. The predictions of the model are compared with laboratory test results for various total electron content. This model also enables us to evaluate the sensitivity of TOA performance to changes in the receiver hardware parameters, e.g. filter bandwidth and the threshold levels in the time-tagging process of the EMP waveform.<>},
keywords={VHF radio propagation;delays;electromagnetic pulse;ionospheric electromagnetic wave propagation;prediction theory;satellite communication;EMP waveform;TOA performance;VHF;delay correction;electromagnetic pulse;filter bandwidth;ionospheric group delay model;ionospheric-delay correction model;laboratory test results;predictions;pulse-broadening effect;sensitivity;threshold levels;time-of-arrival prediction model;time-tagging process;total electron content;transionospheric EMP;Delay effects;EMP radiation effects;Electromagnetic measurements;Electromagnetic modeling;Ionosphere;Laboratories;Predictive models;Pulse measurements;Testing;Time measurement},
doi={10.1109/7.366322},
ISSN={0018-9251},
month={Jan},}
@INPROCEEDINGS{244124,
author={R. Eng and Y. S. Kim},
booktitle={MILCOM 92 Conference Record},
title={Time-of-arrival measurement for transionospheric EMP},
year={1992},
pages={1058-1061 vol.3},
abstract={An ionospheric-delay correction model that measures the time of arrival (TOA) of a transionospheric electromagnetic pulse (EMP) is presented. The pulse-broadening effect due to the dispersive ionosphere is incorporated into the ionospheric group-delay model to provide an improved delay correction in measurements of EMP time of arrival. The model's predictions are compared with laboratory test results for various total electron contents. This improved model also makes it possible to evaluate the sensitivity of TOA performance to changes in the receiver hardware parameters, e.g. filter bandwidth and the threshold levels in the time-tagging process},
keywords={electromagnetic pulse;ionospheric electromagnetic wave propagation;electromagnetic pulse;filter bandwidth;ionospheric group-delay model;ionospheric-delay correction model;pulse-broadening effect;receiver hardware parameters;threshold levels;time of arrival;time-tagging process;transionospheric EMP;Delay effects;Dispersion;EMP radiation effects;Electromagnetic measurements;Electromagnetic modeling;Ionosphere;Laboratories;Predictive models;Pulse measurements;Time measurement},
doi={10.1109/MILCOM.1992.244124},
month={Oct},}
@INPROCEEDINGS{712983,
author={A. A. Mironov and P. A. Pevzner},
booktitle={Proceedings. String Processing and Information Retrieval: A South American Symposium (Cat. No.98EX207)},
title={SST versus EST in gene recognition},
year={1998},
pages={60-64},
abstract={The EST data provide a powerful tool for identification of transcribed DNA sequences. However, since ESTs are relatively short, many exons are poorly covered by ESTs thus reducing the utility of EST data. Recently, SST (Signature Sequence Tags) fingerprints were proposed as an alternative to EST fingerprints. Given a fingerprint set of probes, SST of a clone is a subset of probes from the fingerprint set that hybridize with the clone. We demonstrate that besides being a powerful technique for screening cDNA libraries, SST technology provides for very accurate gene predictions. Even with a small fingerprint set (600-800 probes) SST-based gene recognition outperforms many conventional and EST-based methods. The increase in the size of fingerprint set to 1500 probes provides almost perfect gene recognition. Even more importantly, SST-based gene predictions miss very few exons and therefore provide an opportunity to bypass cDNA sequencing step on the way from finished genomic sequence to mutation detection in gene hunting projects. Since SST data can be obtained in a highly parallel and inexpensive way, SST technology has a potential of substituting EST technology for gene hunting},
keywords={DNA;biology computing;genetics;pattern recognition;EST;SST;SST fingerprints;Signature Sequence Tags;cDNA libraries;gene hunting;gene recognition;genomic sequence;mutation detection;transcribed DNA sequence identification;Bioinformatics;Biotechnology;Cloning;DNA;Databases;Genomics;Laboratories;Libraries;Probes;Sequences},
doi={10.1109/SPIRE.1998.712983},
month={Sep},}
@INPROCEEDINGS{7862986,
booktitle={2016 International Conference on Computer, Control, Informatics and its Applications (IC3INA)},
title={[Front cover]},
year={2016},
pages={c1-c1},
abstract={The following topics are dealt with: Wi-Fi based temperature monitoring system; Kalman filter implementation; multiple robots visual SLAM; speed detection; image processing; vehicle classification; lane categorization; Stratix V DE5-Net FPGA board; high performance computing; discrete-time model-based controller; Bayesian Twitter-based prediction;timing estimation; normalized 4-th order moment; OFDM-based cognitive radio systems; software size measurement; knowledge management portal; scene text detection; IEEE 802.11n; IEEE 802.11ac; distributed order-up-to inventory management; uncertain demand-system modelling; predictor-based dynamic soft VSC; time-delay systems; magnitude-constrained input signal; heart rate prediction; cycling cadence; feedforward neural network; computer vision; autonomous UAV; spatial co-location pattern discovery; multiple neighborhood relationship function; Kansei based interface design analysis; open source e-learning system; high education; asset management system functionality; bitcoin platform; multi-label classification; deep belief networks; virtual screening; multi-target drug; cancer subtype identification; deep learning approach; fault-tolerant control; nonlinear systems; projection optimization; compressive sensing framework; industrial control system security-malware botnet detection; AUV high-precision path following control system; PD-controller; an- maly detection; computational optimization; violent scenes detection; educational institution DNS network traffic; circle detection; Hough transform; Mexican hat filter; multilink manipulators; super-symmetric particle classication; noise labelling; role-based programming; adaptive IoT applications; smart dog feeder design; DSS01 COBIT5; part-of-speech tagging; Bahasa Indonesia; 2D spatial interpolation; water quality parameter distribution; XQuery evaluation; SLIM+; advanced-simple lightweight and intuitive multicast protocol and MANET.},
keywords={Hough transforms;Internet of Things;Kalman filters;OFDM modulation;PD control;SLAM (robots);autonomous aerial vehicles;autonomous underwater vehicles;belief networks;cognitive radio;compressed sensing;computer aided instruction;computer vision;delays;discrete time systems;fault tolerant control;field programmable gate arrays;filtering theory;further education;image classification;information retrieval;interpolation;invasive software;inventory management;knowledge management;learning (artificial intelligence);manipulators;medical signal processing;mobile ad hoc networks;multi-robot systems;natural language processing;nonlinear control systems;optimisation;parallel processing;portals;position control;recurrent neural nets;routing protocols;shape recognition;social networking (online);temperature;text detection;water quality;wireless LAN;2D spatial interpolation;AUV high-precision path following control system;Bahasa Indonesia;Bayesian Twitter-based prediction;DSS01 COBIT5;Hough transform;IEEE 802.11ac;IEEE 802.11n;Kalman filter implementation;Kansei based interface design analysis;MANET;Mexican hat filter;OFDM-based cognitive radio systems;PD-controller;SLIM+;Stratix V DE5-Net FPGA board;Wi-Fi based temperature monitoring system;XQuery evaluation;adaptive IoT applications;advanced-simple lightweight and intuitive multicast protocol;anomaly detection;asset management system functionality;autonomous UAV;bitcoin platform;cancer subtype identification;circle detection;compressive sensing framework;computational optimization;computer vision;cycling cadence;deep belief networks;deep learning approach;discrete-time model-based controller;distributed order-up-to inventory management;educational institution DNS network traffic;fault-tolerant control;feedforward neural network;heart rate prediction;high education;high performance computing;image processing;industrial control system security-malware botnet detection;knowledge management portal;lane categorization;magnitude-constrained input signal;multi-label classification;multilink manipulators;multiple neighborhood relationship function;multiple robots visual SLAM;noise labelling;nonlinear systems;normalized 4-th order moment;open source e-learning system;part-of-speech tagging;predictor-based dynamic soft VSC;projection optimization;role-based programming;scene text detection;smart dog feeder design;software size measurement;spatial co-location pattern discovery;speed detection;super-symmetric particle classication;time-delay systems;timing estimation;uncertain demand-system modelling;vehicle classification;violent scenes detection;virtual screening;water quality parameter distribution},
doi={10.1109/IC3INA.2016.7862986},
month={Oct},}
@INPROCEEDINGS{6062843,
booktitle={2011 14th IEEE International Conference on Computational Science and Engineering},
title={Table of contents},
year={2011},
pages={v-xii},
abstract={The following topics are dealt with: online authorship authentication; Web services; hierarchical branch prediction architecture; on-chip data memory performance estimation; domain service acquisition; enhanced application placement framework; cyclic dependency automatic management; regional spatial appearance model; video steganography; recurrent CMAC sliding mode adaptive control; batch range proof analysis; mutual authentical protocol; RFID system; Hilbert-order based star network expansion cloaking algorithm; automated 2D estimation; data transmission protocol reliability; load balance analysis; wireless sensor networks; multicore architecture; equal symmetric distributed fault-tolerant architecture; self-starting block Adams methods; unstructured semantic mesh definition; hull form optimization; nested template-based model; Chinese noun phrases chunking; large-scale multicore cluster systems; speculative thread partitioning; third order particle swarm optimization; Geiger-mode avalanche photodiodes; American option pricing; multimedia application; stagnation point flow; mathematical formula query language; delay aware multipath Doppler routing; multicore operating systems; GPU computing; reliable routing protocol; performance enhancement; Internet traffic classification; R/M integrated supply chain; SMSK spread spectrum modulation; mobile geo-tagging system; IEEE802.16e braodband access networks; service-oriented management; self-synchronized audio watermarking; authenticated encryption scheme; pervasive systems; pervasive networks; ubiquitous computing; telecommunication computing; articulatory speech synthesis; facial expression recognition; 3D face feature extraction; Chinese-American sign language translation; and cloud computing.},
keywords={Internet;Zigbee;adaptive control;audio watermarking;authorisation;broadband networks;cerebellar model arithmetic computers;cloud computing;computer graphic equipment;coprocessors;cryptography;face recognition;feature extraction;language translation;mobile computing;multiprocessing systems;particle swarm optimisation;radiofrequency identification;routing protocols;speech synthesis;spread spectrum communication;steganography;telecommunication computing;telecommunication security;telecommunication traffic;video signal processing;wireless sensor networks;3D face feature extraction;American option pricing;Chinese noun phrases chunking;Chinese-American sign language translation;GPU computing;Geiger-mode avalanche photodiodes;Hilbert-order based star network expansion cloaking algorithm;IEEE802.16e braodband access networks;Internet traffic classification;R/M integrated supply chain;RFID system;SMSK spread spectrum modulation;Web services;articulatory speech synthesis;authenticated encryption scheme;automated 2D estimation;batch range proof analysis;cloud computing;cyclic dependency automatic management;data transmission protocol reliability;delay aware multipath Doppler routing;domain service acquisition;enhanced application placement framework;equal symmetric distributed fault-tolerant architecture;facial expression recognition;hierarchical branch prediction architecture;hull form optimization;large-scale multicore cluster systems;load balance analysis;mathematical formula query language;mobile geo-tagging system;multicore architecture;multicore operating systems;multimedia application;mutual authentical protocol;nested template-based model;on-chip data memory performance estimation;online authorship authentication;performance enhancement;pervasive networks;pervasive systems;recurrent CMAC sliding mode adaptive control;regional spatial appearance model;reliable routing protocol;self-starting block Adams methods;self-synchronized audio watermarking;service-oriented management;speculative thread partitioning;stagnation point flow;telecommunication computing;third order particle swarm optimization;ubiquitous computing;unstructured semantic mesh definition;video steganography;wireless sensor networks},
doi={10.1109/CSE.2011.4},
month={Aug},}
@INPROCEEDINGS{7996304,
author={Huiru Yuan and Kun Yuan and Zhonghua Zhao},
booktitle={2017 International Conference on Service Systems and Service Management},
title={On predicting event propagation on Weibo},
year={2017},
pages={1-6},
abstract={The enormous amount of tweets generated every day contain remarkably theoretical and practical value in numerous researches and applications. Although the prediction of information propagation has been extensively studied, most previous work mainly focuses on whether the event will break out and when it will burst out. It is still a huge challenge to predict the final size due to the great uncertainty. This study aims at finding out the best combination of the method and features to predict the final size of the certain event. In the best parameter settings, we compare the method of Gradient Boosting Decision Tree (GBDT) with other methods both in same and different strategies, and find that the GBDT performs much better. In the process of feature extracting and computing, we find that the amount of the early uids, the amount of fans, and the field of the topic matter in the early stage of the prediction while other features begin to show differences over time. That is to say, we achieve the more accurate prediction with fewer features in the early stage of the event which make sense.},
keywords={Analytical models;Fans;Feature extraction;Market research;Predictive models;Tagging;Twitter;event prediction;feature importance;gradient boosting decision tree;prediction},
doi={10.1109/ICSSSM.2017.7996304},
month={June},}
@ARTICLE{7994718,
author={S. Fazeli and H. Drachsler and M. Bitter-Rijpkema and F. Brouns and W. van der Vegt and P. B. Sloep},
journal={IEEE Transactions on Learning Technologies},
title={User-centric Evaluation of Recommender Systems in Social Learning Platforms: Accuracy is Just the Tip of the Iceberg},
year={2017},
volume={PP},
number={99},
pages={1-1},
abstract={Recommender systems provide users with content they might be interested in. Conventionally, recommender systems are evaluated mostly by using prediction accuracy metrics only. But the ultimate goal of a recommender system is to increase user satisfaction. Therefore, evaluations that measure user satisfaction should also be performed before deploying a recommender system in a real target environment. Such evaluations are laborious and complicated compared to the traditional, data-centric evaluations, though. In this study, we carried out a user-centric evaluation of state-of-the-art recommender systems as well as a graph-based approach in the ecologically valid setting of an authentic social learning platform. We also conducted a data-centric evaluation on the same data to investigate the added value of user-centric evaluations and how user satisfaction of a recommender system is related to its performance in terms of accuracy metrics. Our findings suggest that user-centric evaluation results are not necessarily in line with data-centric evaluation results. We conclude that the traditional evaluation of recommender systems in terms of prediction accuracy only does not suffice to judge performance of recommender systems on the user side. Moreover, the user-centric evaluation provides valuable insights in how candidate algorithms perform on each of the five quality metrics for recommendations: usefulness, accuracy, novelty, diversity, and serendipity.},
keywords={Crowdsourcing;Measurement;Metadata;Prediction algorithms;Recommender systems;Social network services;Tagging;accuracy;evaluation;learning;performance;recommender systems;social},
doi={10.1109/TLT.2017.2732349},
ISSN={1939-1382},
month={},}
@ARTICLE{7872438,
author={B. Canion and S. McConchie and S. Landsberger},
journal={IEEE Transactions on Nuclear Science},
title={Point Kinetics Framework for Characterizing Prompt Neutron and Photon Signatures From Tagged Neutron Interrogation},
year={2017},
volume={64},
number={7},
pages={1761-1768},
abstract={The goal of this paper is to build a simple point kinetics model to predict the neutron and photon multiplicities induced by an associated particle imaging deuterium-tritium neutron generator interrogating uranium metal. A point kinetics framework is used to model the relationship between multiplication and radiation released from a 14.1-MeV neutron-induced fission chain in subcritical uranium. The goal of developing this point kinetics model is to tie the number of neutrons and photons released from a configuration to the multiplication of that configuration. The discussion is limited to building a model that describes the photon and neutron multiplicities released from a uranium inspection object, not considering the detection process. The addition of polyethylene shielding is also investigated. The point kinetics predictions are compared with the results from an MCNP-PoliMi simulation throughout as a way of measuring how closely the simple point kinetics model describes the complex propagation and transport of fission chain radiation in fissile material. The results show that the point kinetics does an excellent job of predicting the multiplicities released from the inspection objects, but at the cost of multiple additional inputs, some of which rely on the knowledge of geometric information from imaging.},
keywords={Monte Carlo methods;neutron detection;photodetectors;MCNP-PoliMi simulation;fissile material;fission chain radiation transport;geometric information;neutron multiplicity;neutron-induced fission chain;particle imaging deuterium-tritium neutron generator;photon multiplicity;photon number;photon signature characterization;point kinetics framework;point kinetics model;prompt neutron characterization;tagged neutron interrogation;Imaging;Kinetic theory;Mathematical model;Metals;Neutrons;Photonics;Uranium;Radiation detectors},
doi={10.1109/TNS.2017.2678519},
ISSN={0018-9499},
month={July},}
@INPROCEEDINGS{7953246,
author={X. Yang and Y. N. Chen and D. Hakkani-Tür and P. Crook and X. Li and J. Gao and L. Deng},
booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={End-to-end joint learning of natural language understanding and dialogue manager},
year={2017},
pages={5690-5694},
abstract={Natural language understanding and dialogue policy learning are both essential in conversational systems that predict the next system actions in response to a current user utterance. Conventional approaches aggregate separate models of natural language understanding (NLU) and system action prediction (SAP) as a pipeline that is sensitive to noisy outputs of error-prone NLU. To address the issues, we propose an end-to-end deep recurrent neural network with limited contextual dialogue memory by jointly training NLU and SAP on DSTC4 multi-domain human-human dialogues. Experiments show that our proposed model significantly outperforms the state-of-the-art pipeline models for both NLU and SAP, which indicates that our joint model is capable of mitigating the affects of noisy NLU outputs, and NLU model can be refined by error flows backpropagating from the extra supervised signals of system actions.},
keywords={interactive systems;learning (artificial intelligence);natural language processing;recurrent neural nets;NLU;SAP;dialogue manager;dialogue policy learning;end-to-end deep recurrent neural network;end-to-end joint learning;human-human dialogues;natural language understanding;system action prediction;user utterance;Hidden Markov models;Natural languages;Noise measurement;Pipelines;Predictive models;Tagging;Training;deep learning;dialogue manager;end-to-end;language understanding;spoken dialogue systems},
doi={10.1109/ICASSP.2017.7953246},
month={March},}
@INPROCEEDINGS{7952705,
author={Q. Huang and Y. Xu and P. J. B. Jackson and W. Wang and M. D. Plumbley},
booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Fast tagging of natural sounds using marginal co-regularization},
year={2017},
pages={2991-2995},
abstract={Automatic and fast tagging of natural sounds in audio collections is a very challenging task due to wide acoustic variations, the large number of possible tags, the incomplete and ambiguous tags provided by different labellers. To handle these problems, we use a co-regularization approach to learn a pair of classifiers on sound and text. The first classifier maps low-level audio features to a true tag list. The second classifier maps actively corrupted tags to the true tags, reducing incorrect mappings caused by low-level acoustic variations in the first classifier, and to augment the tags with additional relevant tags. Training the classifiers is implemented using marginal co-regularization, pair of which draws the two classifiers into agreement by a joint optimization. We evaluate this approach on two sound datasets, Freefield1010 and Task4 of DCASE2016. The results obtained show that marginal co-regularization outperforms the baseline GMM in both efficiency and effectiveness.},
keywords={audio signal processing;feature extraction;DCASE2016 dataset;Freefield1010 dataset;audio collections;co-regularization approach;low-level acoustic variations;low-level audio features;marginal co-regularization;natural sound tagging;Acoustics;Audio recording;Optimization;Prediction algorithms;Tagging;Training;annotation;co-regularization;natural sound},
doi={10.1109/ICASSP.2017.7952705},
month={March},}
@INPROCEEDINGS{7941935,
author={Y. Zuo and K. Yada and A. B. M. S. Ali},
booktitle={2016 3rd Asia-Pacific World Congress on Computer Science and Engineering (APWC on CSE)},
title={Prediction of Consumer Purchasing in a Grocery Store Using Machine Learning Techniques},
year={2016},
pages={18-25},
abstract={Over the past decades, prediction of costumers' purchase behavior has been significantly considered, and completely recognized as one of the most significant research topics in consumer behavior researches. While we attempt to measure response of purchase intention to the contextual factors such as customers' age, gender and income, product price and sale promotion, most of business models are basing on a linear equation to estimate weight of these factors due to the linear equation is not only intuitive for other academics to compare and replicate but also luminous to explain the results for business practitioners. Nevertheless, comparing with other research fields (e.g. pattern recognition and text classification), the prediction methods for purchase behavior are overconcentration of the linear models, especially linear discriminant analysis and logistic regression analysis. On the other hand, as more and more information and communication technologies (ICT, e.g. POS and sensor) are introduced into retail, marketing and management to collect business data, the volumes of data are increasing in exponential growth. Analysis based on linear models are insufficient to satisfy the requirement of academics and practitioners any more, and machine learning techniques have been increasingly attracted us to conduct them as an alternative approach for knowledge discovery and data mining. With regard to these issues, this paper employs two representative machine learning methods: Bayes classifier and support vector machine (SVM) and investigates the performance of them with the data in the real world.},
keywords={Bayes methods;consumer behaviour;data analysis;data mining;learning (artificial intelligence);pattern classification;purchasing;regression analysis;support vector machines;Bayes classifier;SVM;business data analysis;consumer behavior;consumer purchase prediction;customer purchase behavior;data mining;grocery store;knowledge discovery;linear discriminant analysis;linear equation;logistic regression analysis;machine learning;management;marketing;retail;support vector machine;Business;Fish;Kernel;Mathematical model;RFID tags;Support vector machines;In-store Behavior;Machine Learning;Multivariable Normality Test;Purchase Behavior;RFID},
doi={10.1109/APWC-on-CSE.2016.015},
month={Dec},}
@ARTICLE{7879356,
author={W. Hoiles and A. Aprem and V. Krishnamurthy},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Engagement and Popularity Dynamics of YouTube Videos and Sensitivity to Meta-Data},
year={2017},
volume={29},
number={7},
pages={1426-1437},
abstract={YouTube, with millions of content creators, has become the preferred destination for viewing videos online. Through the Partner program, YouTube allows content creators to monetize their popular videos. Of significant importance for content creators is which meta-level features (title, tag, thumbnail, and description) are most sensitive for promoting video popularity. The popularity of videos also depends on the social dynamics, i.e., the interaction of the content creators (or channels) with YouTube users. Using real-world data consisting of about 6 million videos spread over 25 thousand channels, we empirically examine the sensitivity of YouTube meta-level features and social dynamics. The key meta-level features that impact the view counts of a video include: first day view count, number of subscribers, contrast of the video thumbnail, Google hits, number of keywords, video category, title length, and number of upper-case letters in the title, respectively, and illustrate that these meta-level features can be used to estimate the popularity of a video. In addition, optimizing the meta-level features after a video is posted increases the popularity of videos. In the context of social dynamics, we discover that there is a causal relationship between views to a channel and the associated number of subscribers. Additionally, insights into the effects of scheduling and video playthrough in a channel are also provided. Our findings provide a useful understanding of user engagement in YouTube.},
keywords={content management;meta data;social networking (online);video signal processing;Google hits;Partner program;YouTube meta-level feature sensitivity;YouTube users;YouTube video popularity dynamics;content creators;meta-data;online video viewing;social dynamics;title length;upper-case letters;user engagement;video category;video thumbnail;Learning systems;Neurons;Sensitivity analysis;Videos;YouTube;Granger causality;YouTube;channel dynamics;machine learning;metadata;popularity prediction;sensitivity analysis;social media;user engagement},
doi={10.1109/TKDE.2017.2682858},
ISSN={1041-4347},
month={July},}
@ARTICLE{7583675,
author={M. Ni and Q. He and J. Gao},
journal={IEEE Transactions on Intelligent Transportation Systems},
title={Forecasting the Subway Passenger Flow Under Event Occurrences With Social Media},
year={2017},
volume={18},
number={6},
pages={1623-1632},
abstract={Subway passenger flow prediction is strategically important in metro transit system management. The prediction under event occurrences turns into a very challenging task. In this paper, we adopt a new kind of data source-social media-to tackle this challenge. We develop a systematic approach to examine social media activities and sense event occurrences. Our initial analysis demonstrates that there exists a moderate positive correlation between passenger flow and the rates of social media posts. This finding motivates us to develop a novel approach for improved flow forecast. We first develop a hashtag-based event detection algorithm. Furthermore, we propose a parametric and convex optimization-based approach, called optimization and prediction with hybrid loss function (OPL), to fuse the linear regression and the results of seasonal autoregressive integrated moving average (SARIMA) model jointly. The OPL hybrid model takes advantage of the unique strengths of linear correlation in social media features and SARIMA model in time series prediction. Experiments on events nearby a subway station show that OPL reports the best forecasting performance compared with other state-of-the-art techniques. In addition, an ensemble model is developed to leverage the weighted results from OPL and support vector machine regression together. As a result, the prediction accuracy and the robustness further increase.},
keywords={convex programming;flow;learning (artificial intelligence);optimisation;railways;regression analysis;social networking (online);support vector machines;time series;traffic engineering computing;OPL hybrid model;SARIMA model;data source;ensemble model;event occurrences;hashtag-based event detection algorithm;linear correlation;linear regression;metro transit system management;moderate positive correlation;optimization and prediction with hybrid loss function;parametric convex optimization;seasonal autoregressive integrated moving average model;social media features;social media posts;subway passenger flow forecasting;subway passenger flow prediction;subway station;support vector machine regression;time series prediction;Forecasting;Predictive models;Public transportation;Tagging;Twitter;Social media;event identification;social sensing;subway passenger flow prediction;transit ridership},
doi={10.1109/TITS.2016.2611644},
ISSN={1524-9050},
month={June},}
@ARTICLE{7480443,
author={E. Briscoe and S. Appling and J. Schlosser},
journal={IEEE Transactions on Computational Social Systems},
title={Technology Futures From Passive Crowdsourcing},
year={2016},
volume={3},
number={1},
pages={23-31},
abstract={Efforts to predict emerging, new, or disruptive technologies use various analyses and data sources to derive indicators and subsequent forecasts about technological innovations, including quantitative (such as bibliometric analysis) and qualitative methods (such as expert elicitation). We describe a novel approach for harnessing a collective (crowdsourced) predictive ability available through publicly made technology-related statements by automatically determining significant convergences on technology forecasts. We evaluate our approach using a corpus of science-related articles and demonstrate that passive crowdsourcing may be a powerful source of technology-related predictive intelligence.},
keywords={data mining;information analysis;outsourcing;scientific information systems;technology management;bibliometric analysis;collective-crowdsourced predictive ability;expert elicitation;passive crowdsourcing;qualitative method;quantitative method;science-related articles= corpus;technological innovations;technology forecasting;technology-related predictive intelligence;technology-related statements;Crowdsourcing;Market research;Media;Tagging;Technological innovation;Technology forecasting;Passive crowdsourcing;technology emergence prediction;technology forecasting;technology forecasting.},
doi={10.1109/TCSS.2016.2563460},
ISSN={2329-924X},
month={March},}
@ARTICLE{7384466,
author={M. F. Alhamid and M. Rawashdeh and H. Dong and M. A. Hossain and A. E. Saddik},
journal={IEEE Transactions on Human-Machine Systems},
title={Exploring Latent Preferences for Context-Aware Personalized Recommendation Systems},
year={2016},
volume={46},
number={4},
pages={615-623},
abstract={Context-aware recommendations offer the potential of exploiting social contents and utilize related tags and rating information to personalize the search for content considering a given context. Recommendation systems tackle the problem of trying to identify relevant resources from the vast number of choices available online. In this study, we propose a new recommendation model that personalizes recommendations and improves the user experience by analyzing the context when a user wishes to access multimedia content. We conducted empirical analysis on a dataset from last.fm to demonstrate the use of latent preferences for ranking items under a given context. Additionally, we use an optimization function to maximize the mean average precision measure of the resulted recommendation. Experimental results show a potential improvement to the quality of the recommendation in terms of accuracy when compared with state-of-the-art algorithms.},
keywords={collaborative filtering;multimedia systems;recommender systems;ubiquitous computing;collaborative filtering;context-aware personalized recommendation systems;information rating;item ranking;last.fm;latent preferences;mean average precision measure;multimedia content;optimization function;recommendation model;social contents;user experience;Context;Context modeling;Man machine systems;Matrix decomposition;Media;Motion pictures;Prediction algorithms;Collaborative filtering (CF);context;context awareness;context-based recommendation},
doi={10.1109/THMS.2015.2509965},
ISSN={2168-2291},
month={Aug},}
@ARTICLE{7852483,
author={M. Sun and K. H. Zeng and Y. C. Lin and A. Farhadi},
journal={IEEE Transactions on Image Processing},
title={Semantic Highlight Retrieval and Term Prediction},
year={2017},
volume={26},
number={7},
pages={3303-3316},
abstract={Due to the unprecedented growth of unedited videos, finding highlights relevant to a text query in a set of unedited videos has become increasingly important. We refer this task as semantic highlight retrieval and propose a query-dependent video representation for retrieving a variety of highlights. Our method consists of two parts: 1) “viralets”, a mid-level representation bridging between semantic [Fig. 1(a)] and visual [Fig. 1(c)] spaces and 2) a novel Semantic-MODulation (SMOD) procedure to make viralets query-dependent (referred to as SMOD viralets). Given SMOD viralets, we train a single highlight ranker to predict the highlightness of clips with respect to a variety of queries (two examples in Fig. 1), whereas existing approaches can be applied only in a few predefined domains. Other than semantic highlight retrieval, viralets can also be used to associate relevant terms to each video. We utilize this property and propose a simple term prediction method based on nearest neighbor search. To conduct experiments, we collect a viral video dataset1 including users’ comments, highlights, and/or original videos. Among a testing database with 1189 clips (13% highlights and 87% non-highlights), our highlight ranker achieves 41.2% recall at top-10 retrieved clips. It is significantly higher than the state-of-the-art domain-specific highlight ranker and its extension. Similarly, our method also outperforms all baseline methods on the publicly available video highlight dataset. Finally, our simple term prediction method utilizing viralets outperforms the state-of-the-art matrix factorization method (adapted from Kalayeh et al.). 1

Viral videos refer to popular online videos. We focus on user-generated viral videos, which typically contain short highlight marked by users.
},
keywords={Databases;Nearest neighbor searches;Prediction methods;Semantics;Tagging;Videos;Visualization;Video summarization;highlight retrieval},
doi={10.1109/TIP.2017.2666039},
ISSN={1057-7149},
month={July},}
@INPROCEEDINGS{7918987,
author={G. T. Liou and Y. R. Wang and C. Y. Chiang},
booktitle={2016 Conference of The Oriental Chapter of International Committee for Coordination and Standardization of Speech Databases and Assessment Techniques (O-COCOSDA)},
title={Text normalization for Mandarin TTS by using keyword information},
year={2016},
pages={73-78},
abstract={Text normalization (TN) in a text-to-speech system converts written-form words (i.e. non-standard words, NSWs) to spoken-form words (SFWs). Correctness of the conversion seriously effects intelligibility of synthesized speech. This paper proposes a data-driven text normalization by using keyword information for a Mandarin text-to-speech system (MTTS). The proposed method differs from conventional TN methods in the following aspects: 1) Instead of using rules formed by finite state automatas (FSAs) to recognize or tokenize NSWs from character strings, a data-driven approach to recognizing/tokenizing NSWs from word string is proposed. 2) An automatic method to collect keywords or to cluster words into word classes that assist in TN tasks is proposed to replace conventional handcrafted keywords by linguistic expertise. 3) A data-driven SFWs prediction by using keyword information is proposed to improve performance of conventional systems that work in a rule-based fashion with handcrafted keyword conversion rules, or that predict SFWs by information about characters. The experiments on part of the Academia Sinica Balanced Corpus and the texts from the internet new articles reported some improvements against existing conventional TN approaches.},
keywords={Internet;computational linguistics;finite state machines;knowledge based systems;natural language processing;pattern clustering;speech synthesis;text analysis;Academia Sinica Balanced Corpus;FSA;Internet new articles;MTTS;Mandarin TTS;Mandarin text-to-speech system;NSW;automatic method;character strings;data-driven SFW prediction;data-driven approach;data-driven text normalization;finite state automata;keyword collection;keyword conversion rules;keyword information;linguistic expertise;nonstandard words;rule-based fashion;spoken-form words;synthesized speech intelligibility;word clustering;word string;written-form words;Context;Gold;IEEE Sections;Pragmatics;Speech;Standardization;Tagging;Mandarin Chinese;TTS;text normalization},
doi={10.1109/ICSDA.2016.7918987},
month={Oct},}
@INPROCEEDINGS{7892466,
author={F. H. Rachman and R. Sarno and C. Fatichah},
booktitle={2016 3rd International Conference on Information Technology, Computer, and Electrical Engineering (ICITACEE)},
title={CBE: Corpus-based of emotion for emotion detection in text document},
year={2016},
pages={331-335},
abstract={Emotion Detection is a part of Natural Language Processing (NLP) that still evolve. Emotional Corpus that had been widely used are Wordnet Affect Emotion (WNA) and ANEW (Affective Norms for English Words). There are two ways to analyze the text based Emotion Detection: Categorical and Dimensional Model. Each model has different advantages and disadvantages. And each model has a different concept to predict emotion. The contribution of this research is forming automatic emotional corpus with merging two computational model. It called Corpus-Based of Emotion (CBE). CBE developed from ANEW and WNA with term similarity measure and distance of node approach. Latent Dirichlet Allocation (LDA) is used too for automatically expand CBE. The CBE attributes are a score of Valence (V), Arousal (A), Dominance (D) and categorical label emotion. Categorical label emotion based on six basic emotion of Ekman. Based on experiment results, it is known that CBE is able to improve the accuracy in detection of emotions. F-Measure using WNA+ANEW is 0.50 and F-Measure using CBE with expanding is 0.61.},
keywords={database management systems;emotion recognition;natural language processing;statistical analysis;text analysis;ANEW;CBE;Ekman;F-Measure;NLP;WNA;Wordnet affect emotion;affective norms-for-English words;arousal;categorical label emotion;categorical model;corpus-based-of-emotion;dimensional model;distance-of-node approach;dominance;emotion detection;emotion prediction;latent Dirichlet allocation;natural language processing;term similarity measure;text based emotion detection analysis;text document;valence;Computational modeling;Euclidean distance;Informatics;Mathematical model;Merging;Tagging;Testing;ANEW;Categorical model;Dimensional model;Emotion Detection;LDA;WNA;corpus of emotion},
doi={10.1109/ICITACEE.2016.7892466},
month={Oct},}
@INPROCEEDINGS{7889513,
author={R. Rezapour and L. Wang and O. Abdar and J. Diesner},
booktitle={2017 IEEE 11th International Conference on Semantic Computing (ICSC)},
title={Identifying the Overlap between Election Result and Candidates #x2019; Ranking Based on Hashtag-Enhanced, Lexicon-Based Sentiment Analysis},
year={2017},
pages={93-96},
abstract={The popularity and availability of Twitter as a service and a data source have fueled the interest in sentiment analysis. Previous research has shed light on the challenges that contextualizing effects and linguistic complexities pose for the accurate sentiment classification of tweets. We test the effect of adding manually-annotated, corpus-based hashtags to a sentiment lexicon, finding that this step in combination with negation detection increases prediction accuracy by about 7%. We then use our enhanced model to identify and rank the candidates of the Republican and Democratic Party of the 2016 New York primary election by the decreasing ratio of tweets that mentioned these individuals and had positive valence, and compare our results to the election outcome.},
keywords={pattern classification;politics;sentiment analysis;social networking (online);Democratic Party;New York primary election;Republican Party;Twitter;candidate identification;candidate ranking;data source;election outcome;election result;hashtag-enhanced lexicon-based sentiment analysis;linguistic complexity;manually-annotated corpus-based hashtag;negation detection;sentiment lexicon;tweet sentiment classification;Data mining;Pragmatics;Sentiment analysis;Standards;Tagging;Twitter;Voting;Lexicon Based Approach;Natural Language Processing;Opinion Mining;Sentiment Analysis;Twitter},
doi={10.1109/ICSC.2017.92},
month={Jan},}
@INPROCEEDINGS{7881883,
author={M. Mahdavi and M. Asadpour and S. M. Ghavami},
booktitle={2016 8th International Symposium on Telecommunications (IST)},
title={A comprehensive analysis of tweet content and its impact on popularity},
year={2016},
pages={559-564},
abstract={By the appearance of the online social networks, ordinary people have gained more chance to make and publish content. However, for audiences, as the number of these shared contents grows, the importance of detecting important and related ones increases. So, a significant question is how much would a shared content become popular among audiences, regardless of the source of content? In this paper, we investigate this question by performing a comprehensive analysis of tweet content and studying its impact on popularity of tweet. Here, the number of retweets of tweet is used as a popularity measure. We show that tweets with “social” content, have in general more chance of popularity due to their attraction for society. In contrast, tweets with “individual” content, have little chance to get popular. We collect a fair data set of tweets. In order to do more detailed investigation and access the semantic features, we set an annotation and labeling process. We analyze the informativeness of content-based features and use them to train predictive models. The results clearly show the importance of content-based features. They specifically support this idea that specifying whether a tweet is speaking about an individual or social subject, is the most informative content-based feature to predict the popularity i.e. the number of retweets.},
keywords={information analysis;social networking (online);content-based features;online social networks;predictive models;social content;tweet content analysis;Data collection;Feature extraction;Frequency measurement;Semantics;Tagging;Twitter;Online Social Network;Popularity Detection;Retweet Prediction;Tweet Content;Twitter},
doi={10.1109/ISTEL.2016.7881883},
month={Sept},}
@INPROCEEDINGS{7872963,
author={S. T. Chang and H. W. Huang and L. Y. Wu and C. C. Syu and C. J. Lin and Y. T. Liu and H. H. Li},
booktitle={2016 International Conference on Machine Learning and Cybernetics (ICMLC)},
title={Using text cloud technology to build predictive models of disease #8212; Taking osteoporosis case for example},
year={2016},
volume={2},
pages={644-648},
abstract={As warnings of diseases are easily neglected by people, the golden treatment time is missed. Osteoporosis is one of the examples that its symptoms, e.g. knee and back pain, are usually overlooked. In this paper, we implement an osteoporosis prediction system based on text cloud mining analysis to recognize potential osteoporosis patients. The prediction system recognize a potential osteoporosis patient according to his/her symptoms, status, and medical records. A library of use cases are firstly implemented according to collected osteoporosis patients. Similarity measure is calculated for a person and the cases in the library. A person with more similar to the cases of osteoporosis patients means he/she has a higher chance to suffer from osteoporosis.},
keywords={case-based reasoning;cloud computing;data mining;diseases;medical computing;text analysis;diseases;medical records;osteoporosis prediction system;potential osteoporosis patient recognition;similarity measure;symptoms;text cloud mining analysis;text cloud technology;Cognition;Medical diagnostic imaging;Osteoporosis;Predictive models;Tag clouds;Text mining;Case-Based Reasoning;Osteoporosis;Text Mining;Word Clouds},
doi={10.1109/ICMLC.2016.7872963},
month={July},}
@INPROCEEDINGS{7867561,
author={Chunkai Zhang and Yaqi Sun and Jianwei Guo and Tengfei Xiong},
booktitle={2016 IEEE Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)},
title={Mining dynamic association rules from multiple time-series data based on data of power plant},
year={2016},
pages={1968-1972},
abstract={Plenty of data are generated from sectors such as industrial manufacture, financial service, e-commerce, satellite remote sensing, sensor network and so on. Normally these data are often with time tags, which are called as time series steams. In this paper sliding window is used to limit the time series data; pretreatment process proceeds linear approximation to the sequences; the sequences after linearization are cut. In the sliding window we maintaining a global SWIU-tree (Incremental Updating tree based on Sliding Window) to store the synopsis structures of scanned datasets, to get rid of the unfrequent patterns and outdated patterns by pruning strategy. With the comparation between the existing algorithm and SWIU-tree algorithm on the data of actual thermal power plant, illustrating that SWIU-tree algorithm is effective and which is able to quickly and precisely dig the association rules among multiple time series streams.},
keywords={data mining;power engineering computing;thermal power stations;time series;SWIU-tree algorithm;dynamic association rules mining;incremental updating tree based on sliding window;linear approximation;pruning strategy;thermal power plant;time-series data;Prediction algorithms;Silicon;Association Rules;Sliding Window;Support Threshold;Time-Series Data},
doi={10.1109/IMCEC.2016.7867561},
month={Oct},}
@INPROCEEDINGS{7863019,
author={E. Tunggawan and Y. E. Soelistio},
booktitle={2016 International Conference on Computer, Control, Informatics and its Applications (IC3INA)},
title={And the winner is #x2026;: Bayesian Twitter-based prediction on 2016 U.S. presidential election},
year={2016},
pages={33-37},
abstract={This paper describes a Naive Bayesian predictive model for 2016 U.S. Presidential Election based on Twitter data. We use 33,708 tweets gathered since December 16, 2015 until February 29, 2016. We propose a simple way for data preprocessing which can still achieve 95.8% accuracy on predicting sentiments. The predicted sentiments are used to forecast the U.S. Republican and Democratic parties candidacies. The forecast is compared to the poll collected from RealClearPolitics.com with 26.7% accuracy. However, the true forecasting capacity of the method still have to be observed after the election process come to conclusion.},
keywords={Bayes methods;politics;sentiment analysis;social networking (online);2016 US presidential election;Bayesian Twitter-based prediction;Democratic parties candidacies;Republican parties candidacies;data preprocessing;naive Bayesian method;sentiment prediction;tweets;Bayes methods;Electronic mail;Mathematical model;Predictive models;Tagging;Twitter;Voting},
doi={10.1109/IC3INA.2016.7863019},
month={Oct},}
@INPROCEEDINGS{7846288,
author={Y. N. Chen and D. Hakanni-Tür and G. Tur and A. Celikyilmaz and J. Guo and L. Deng},
booktitle={2016 IEEE Spoken Language Technology Workshop (SLT)},
title={Syntax or semantics? knowledge-guided joint semantic frame parsing},
year={2016},
pages={348-355},
abstract={Spoken language understanding (SLU) is a core component of a spoken dialogue system, which involves intent prediction and slot filling and also called semantic frame parsing. Recently recurrent neural networks (RNN) obtained strong results on SLU due to their superior ability of preserving sequential information over time. Traditionally, the SLU component parses semantic frames for utterances considering their flat structures, as the underlying RNN structure is a linear chain. However, natural language exhibits linguistic properties that provide rich, structured information for better understanding. This paper proposes to apply knowledge-guided structural attention networks (K-SAN), which additionally incorporate non-flat network topologies guided by prior knowledge, to a language understanding task. The model can effectively figure out the salient substructures that are essential to parse the given utterance into its semantic frame with an attention mechanism, where two types of knowledge, syntax and semantics, are utilized. The experiments on the benchmark Air Travel Information System (ATIS) data and the conversational assistant Cortana data show that 1) the proposed K-SAN models with syntax or semantics outperform the state-of-the-art neural network based results, and 2) the improvement for joint semantic frame parsing is more significant, because the structured information provides rich cues for sentence-level understanding, where intent prediction and slot filling can be mutually improved.},
keywords={interactive systems;natural language processing;recurrent neural nets;speech recognition;speech-based user interfaces;ATIS data;K-SAN models;RNN;SLU;air travel information system data;assistant Cortana data;attention mechanism;intent prediction;knowledge-guided joint semantic frame parsing;knowledge-guided structural attention networks;linguistic properties;natural language;nonflat network topologies;recurrent neural networks;semantics;sentence-level understanding;slot filling;spoken dialogue system;spoken language understanding;syntax;Encoding;Filling;Knowledge engineering;Neural networks;Pragmatics;Semantics;Tagging;Spoken language understanding;deep learning;joint semantic frame parsing;knowledge-guided structural attention networks;spoken dialogue system},
doi={10.1109/SLT.2016.7846288},
month={Dec},}
@INPROCEEDINGS{7840880,
author={J. Alonso-Lorenzo and E. Costa-Montenegro and M. Fernández-Gavilanes},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={Language independent big-data system for the prediction of user location on Twitter},
year={2016},
pages={2437-2446},
abstract={Social media interactions have become increasingly important in today's world. A survey conducted in 2014 among adult Americans found that a majority of those surveyed use at least one social media site. Twitter, in particular, serves 310 million active users on a monthly basis, and thousands of tweets are published every second. The public nature of this data makes it a prime candidate for data mining. Twitter users publish 140-character long messages and have the ability to geo-tag these tweets using a variety of methods: GPS coordinates, IP geolocation and user-declared location. However, few users disclose their location, only between 1% and 3% of users provide location data, according to our empirical findings. In this article, we aim to aggregate information from different sources to provide an estimation on the location of any Twitter user. We use an hybrid approach, using techniques in the fields of Natural Language Processing and network theory. Tests have been conducted on two datasets, inferring the location of each individual user and then comparing it against the actual known location of users with geolocation information. The estimation error is the distance in kilometers between the estimation and the actual location. Furthermore, there is a comparison of the relative average error per country, to account for difference in country sizes. Our results improve those presented in different researches in the literature. Our research has as feature to be independent of the language used by the user, while most of works in the literature use just one language or a reduced set of languages. The article also showcases the evolution of our estimation approach and the impact that the modifications had on the results.},
keywords={Big Data;Global Positioning System;natural language processing;social networking (online);GPS coordinates;IP geolocation;Twitter user;adult Americans;data mining;geolocation information;language independent Big-Data system;natural language processing;social media interactions;social media site;user location prediction;user-declared location;Algorithm design and analysis;Data mining;Estimation;Geology;Natural language processing;Twitter;Big-data;Natural Language Processing;Network theory;Social networks;Twitter;User location},
doi={10.1109/BigData.2016.7840880},
month={Dec},}
@INPROCEEDINGS{7840826,
author={A. Mangal and N. Kumar},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={Using big data to enhance the bosch production line performance: A Kaggle challenge},
year={2016},
pages={2029-2035},
abstract={This paper describes our approach to the Bosch production line performance challenge run by Kaggle.com. Maximizing the production yield is at the heart of the manufacturing industry. At the Bosch assembly line, data is recorded for products as they progress through each stage. Data science methods are applied to this huge data repository consisting records of tests and measurements made for each component along the assembly line to predict internal failures. We found that it is possible to train a model that predicts which parts are most likely to fail. Thus a smarter failure detection system can be built and the parts tagged likely to fail can be salvaged to decrease operating costs and increase the profit margins.},
keywords={Big Data;assembling;automobile industry;failure analysis;production engineering computing;Big Data;Bosch assembly line;Bosch production line performance enhancement;data repository;data science method;internal failure prediction;manufacturing industry;operating cost reduction;production yield maximization;profit margins;Big data;Error analysis;Machine learning algorithms;Manufacturing;Numerical models;Predictive models;Production;Manufacturing automation;data science;failure analysis;predictive models},
doi={10.1109/BigData.2016.7840826},
month={Dec},}
@INPROCEEDINGS{7837994,
author={S. Yuan and X. Wu and Y. Xiang},
booktitle={2016 IEEE 16th International Conference on Data Mining (ICDM)},
title={Incorporating Pre-Training in Long Short-Term Memory Networks for Tweets Classification},
year={2016},
pages={1329-1334},
abstract={The paper presents deep learning models for tweets binary classification. Our approach is based on the Long Short-Term Memory (LSTM) recurrent neural network and hence expects to be able to capture long-term dependencies among words. We develop two models for tweets classification. The basic model, called LSTM-TC, takes word embeddings as input, uses the LSTM layer to derive semantic tweet representation, and applies logistic regression to predict tweet label. The basic LSTM-TC model, like other deep learning models, requires a large amount of well-labeled training data to achieve good performance. To address this challenge, we further develop an improved model, called LSTM-TC*, that incorporates a large amount of weakly-labeled data for classifying tweets. We present two approaches of constructing the weakly-labeled data. One is based on hashtag information and the other is based on the prediction output of some traditional classifier that does not need a large amount of well-labeled training data. Our LSTM-TC* model first learns tweet representation based on the weakly-labeled data, and then trains the logistic regression classifier based on the small amount of well-labeled data. Experimental results show that: (1) the proposed method can be successfully used for tweets classification and outperform existing state-of-the-art methods, (2) pre-training tweet representation, which utilizes weakly-labeled tweets, can significantly improve the accuracy of tweets classification.},
keywords={learning (artificial intelligence);pattern classification;recurrent neural nets;regression analysis;social networking (online);LSTM-TC model;deep learning models;hashtag information;logistic regression;logistic regression classifier;long short-term memory networks;long-term dependencies;recurrent neural network;semantic tweet representation;tweet label;tweet representation;tweets binary classification;weakly-labeled data;weakly-labeled tweets;well-labeled training data;Data models;Logic gates;Logistics;Semantics;Tagging;Training;Twitter;Deep learning;LSTM;Pre-training;Tweets classification},
doi={10.1109/ICDM.2016.0181},
month={Dec},}
@INPROCEEDINGS{7836702,
author={H. Hours and E. Fleury and M. Karsai},
booktitle={2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)},
title={Link Prediction in the Twitter Mention Network: Impacts of Local Structure and Similarity of Interest},
year={2016},
pages={454-461},
abstract={The creation of social ties is driven by several factors which can arguably be related to individual preferences and to the common social environment of individuals. Effects of homophily and triadic closure mechanisms are claimed to be important in terms of initiating new social interactions and in turn to shape the global social structure. This way they eventually provide some potential to predict the creation of social ties between disconnected people sharing common friends or common subjects of interest. In this paper we analyze a large Twitter data corpus and quantify similarities between people by considering the set of their common friends and the set of their commonly shared hashtags in order to predict mention links among them. We show that these similarity measures are correlated among connected people and that the combination of contextual and local structural features provides better predictions as compared to cases where they are considered separately. These results help us to better understand the evolution of egocentric and global social networks and provide advances in the design of better recommendation systems and resource allocation plans.},
keywords={recommender systems;resource allocation;social networking (online);Twitter data corpus;Twitter mention network;egocentric social networks;global social networks;interest similarity;link prediction;mention links prediction;recommendation systems;resource allocation plans;Acceleration;Conferences;Measurement;Resource management;Tagging;Twitter;Twitter data;evolving social networks;homophily;link prediction;triadic closure},
doi={10.1109/ICDMW.2016.0071},
month={Dec},}
@INPROCEEDINGS{7836764,
author={K. Bhattacharjee and L. Petzold},
booktitle={2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)},
title={What Drives Consumer Choices? Mining Aspects and Opinions on Large Scale Review Data Using Distributed Representation of Words},
year={2016},
pages={908-915},
abstract={With the increasing popularity of online review sites, developing methods to mine and analyze information contained in the vast amounts of noisy user-generated reviews becomes a necessity. In this work, we develop a method to uncover the various aspects of a product or service reviewed by a user, and the opinions associated with them, in an automated fashion. We use the neural network model Word2Vec to build a vector space representation of a large corpus of user-generated, online restaurant reviews, and harness these distributed representations for aspect-based sentiment analysis. User generated text data is intrinsically noisy, with misspellings, informal language, and digressions. Because of the many variations in spelling and expression, the data is also very sparse. Despite these inherent challenges we are able to represent the reviews by key drivers of consumer sentiment, allowing for highly accurate sentiment prediction using a method that is both scalable and human interpretable.},
keywords={Web sites;catering industry;consumer behaviour;data mining;neural nets;sentiment analysis;Word2Vec;aspect-based sentiment analysis;consumer choices;consumer sentiment analysis;digressions;distributed representations;distributed word representation;informal language;information analysis;large scale review data;misspellings;neural network model;online restaurant reviews;online review sites;opinion mining;sentiment prediction;user-generated corpus;user-generated text data;vector space representation;Business;Conferences;Data mining;Neural networks;Noise measurement;Sentiment analysis;Tagging},
doi={10.1109/ICDMW.2016.0133},
month={Dec},}
@INPROCEEDINGS{7817038,
author={M. Dias and K. Becker},
booktitle={2016 IEEE/WIC/ACM International Conference on Web Intelligence (WI)},
title={An Heuristics-Based, Weakly-Supervised Approach for Classification of Stance in Tweets},
year={2016},
pages={73-80},
abstract={Stance detection is the task of automatically identifying if the text author is in favor or against a subject or target. This paper presents a weakly supervised approach for stance detection in tweets based solely on their contents. The approach relies on a set of heuristics used to automatically label tweets with regard to stance, which has a twofold purpose: a) automatic creation of a training corpus to develop a predictive model using a supervised learning algorithm, and b) to complement the predictive model when determining the stance of tweets. The paper analyzes the performance of the approach considering six distinct stance targets. We achieved promising results, with weighted F-measure varying from 52% to 67%.},
keywords={learning (artificial intelligence);pattern classification;social networking (online);heuristics-based approach;predictive model;stance classification;stance detection;supervised learning algorithm;tweets;weakly-supervised approach;Labeling;Prediction algorithms;Predictive models;Supervised learning;Tagging;Training;Twitter;Automatic Labeling;Sentiment Analysis;Stance Detection;Twitter},
doi={10.1109/WI.2016.0021},
month={Oct},}
@INPROCEEDINGS{7809391,
author={H. Alharthi and D. Outioua and O. Baysal},
booktitle={2016 IEEE/ACM 3rd International Workshop on CrowdSourcing in Software Engineering (CSI-SE)},
title={Predicting Questions' Scores on Stack Overflow},
year={2016},
pages={1-7},
abstract={Developer support forums are becoming more popular than ever. Crowdsourced knowledge is an essential resource for many developers yet it can raise concerns about the quality of the shared content. Most existing research efforts address the quality of answers posted by Q&A community members. In this paper, we explore the quality of questions and propose a method of predicting the score of questions on Stack Overflow based on sixteen factors related to questions' format, content and interactions that occur in the post. We performed an extensive investigation to understand the relationship between the factors and the scores of questions. The multiple regression analysis shows that the question's length of the code, accepted answer score, number of tags and the count of views, comments and answers are statistically significantly associated with the scores of questions. Our findings can offer insights to community-based Q&A sites for improving the content of the shared knowledge.},
keywords={human computer interaction;knowledge management;question answering (information retrieval);regression analysis;community-based Q&A;questions scores prediction;regression analysis;shared knowledge content;stack overflow;Analytical models;Conferences;Correlation;Data analysis;Predictive models;Regression analysis;XML;Crowdsourced knowledge; content quality; questions; pre- diction model; regression analysis},
doi={10.1109/CSI-SE.2016.009},
month={May},}
@INPROCEEDINGS{7790214,
author={Q. Kong and W. Mao and C. Liu},
booktitle={2016 4th International Conference on Cloud Computing and Intelligence Systems (CCIS)},
title={Popularity prediction based on interactions of online contents},
year={2016},
pages={1-5},
abstract={The interactive behavior of Web users often makes some online contents more popular than others. Thus the popularity of online contents can help us understand public interest and attention behind user interactions. Modeling and predicting the popularity of online contents is an important research topic and can facilitate many practical applications in different domains. Previous work on popularity modeling and prediction usually treat each online content separately, and neglect the interaction information between online contents, represented as interaction relations. In this paper, we explore the interaction relations between online contents, specifically competition and cooperation relations, for popularity prediction. We first define the interaction relations between different online contents and propose a method for the calculation of interaction information. We then apply the non-negative matrix factorization (NMF) technique to get a low dimensional representation of interaction features for online contents, which are used by classifiers for popularity prediction. We finally evaluate the proposed approach using two datasets from SinaWeibo (i.e., original tweets and topic hashtags). The experimental results show that interaction features alone can yield relatively good performance, and by incorporating interaction features into traditional feature based methods, our method can further improve popularity prediction results.},
keywords={Internet;content management;matrix decomposition;pattern classification;NMF;SinaWeibo;Web user interactive behavior;interaction features;interaction information;nonnegative matrix factorization technique;online content interactions;Data mining;Feature extraction;Predictive models;Tagging;Twitter;interactions of online contents;popularity prediction;social media analytics;user interactions},
doi={10.1109/CCIS.2016.7790214},
month={Aug},}
@INPROCEEDINGS{7781488,
author={C. Conrad and V. Kešelj},
booktitle={2016 IEEE 18th Conference on Business Informatics (CBI)},
title={Predicting Political Donations Using Twitter Hashtags and Character N-Grams},
year={2016},
volume={02},
pages={1-7},
abstract={We describe a novel approach for predicting politicaldonations and performing psychographic segmentation basedon social data linked to election donation records. The role ofmicroblogs in enterprise informatics, specifically in relation tocustomer relationship systems is highlighted. Algorithms trainedon social data can be used to interpret and detect prospects' psychographicinformation. Contrasted with past approaches whichfocused exclusively on a single source of social data, the methodbeing presented allows us to use an objective gold standard bylinking Twitter and election records. Two experiments were conductedusing data collected from 438 Twitter users, half of whichare linked with donation event records collected from the UnitedStates Federal Election Commission. Probabilistic, entropy andkernel approaches were tested for predictive accuracy, while theCNG technique is explored as an alternative. The CNG algorithmwas found to predict political affiliation 17 percentage pointsabove the majority classifier, exceeding benchmarks suggestedby the literature. A NaïveBayes word n-gram approach wasfound to outperform CNG at predicting donations by predictingpolitical donations. Insufficient performance and poor reliabilityof standard word n-gram techniques in opinion detection revealskepticism about past work on political affiliation analysis fromsocial data alone. This suggests that prospecting systems maybenefit from constructing algorithms using data linked to external sources.},
keywords={Bayes methods;data handling;government;politics;social networking (online);CNG technique;Naïve Bayes word n-gram approach;Twitter hashtags;United States Federal Election Commission;character n-grams;customer relationship systems;data collection;election donation records;enterprise informatics;entropy approach;kernel approach;microblogs;political affiliation analysis;political donations prediction;predictive accuracy;probabilistic approach;prospect psychographic information detection;psychographic segmentation;social data;Data mining;Probabilistic logic;Standards;Tagging;Twitter;Voting;business analytics;customer segmentation;data mining;political science;predictive analytics;social media;twitter profiling},
doi={10.1109/CBI.2016.42},
month={Aug},}
@INPROCEEDINGS{7755557,
author={S. P. Singh and A. Kumar and L. Singh and M. Bhargava and K. Goyal and B. Sharma},
booktitle={2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT)},
title={Frequency based spell checking and rule based grammar checking},
year={2016},
pages={4435-4439},
abstract={English is a language that is spoken by around 380-420 million people on this planet and understanding it is not at all easy. The meaning of a sentence varies according to the context and the tone of the speaker. To convey the thoughts efficiently, the knowledge of the language and its various rules is very important as thoughts take the form of words and the words take the form of action. One should aim to minimize the errors while using the language. Lesser is the number of mistakes, better will be the communication. To aid in achieving this goal, we are creating a frequency based spell checker and a rule based grammar checker for English language. The grammar checker focuses on detecting and correcting tense related mistakes.},
keywords={grammars;knowledge based systems;natural language processing;English language;frequency based spell checking;rule based grammar checking;tense related mistakes;Context;Dictionaries;Grammar;Optimization;Prediction algorithms;Tagging;Grammar Checker;JSON (Java Script Object Notation);N-Gram;Spell Checker;Suggestion Prediction;Tense},
doi={10.1109/ICEEOT.2016.7755557},
month={March},}
@INPROCEEDINGS{7752374,
author={K. Dey and S. Agrawal and R. Malviya and S. Kaushik},
booktitle={2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)},
title={Assessment of effectiveness of content models for approximating Twitter social connection structures},
year={2016},
pages={1071-1078},
abstract={This paper explores the social quality (goodness) of community structures formed across Twitter users, where social links within the structures are estimated based upon semantic properties of user-generated content (corpus). We examined the overlap of the community structures of the constructed graphs, and followership-based social communities, to find the social goodness of the links constructed. Unigram, bigram and LDA content models were empirically investigated for evaluation of effectiveness, as approximators of underlying social graphs, such that they maintain the community social property. Impact of content at varying granularities, for the purpose of predicting links while retaining the social community structures, was investigated. 100 discussion topics, spanning over 10 Twitter events, were used for experiments. The unigram language model performed the best, indicating strong similarity of word usage within deeply connected social communities. This observation agrees with the phenomenon of evolution of word usage behavior, that transform individuals belonging to the same community tending to choose the same words, made by [1], and raises a question on the literature that use, without validation, LDA for content-based social link prediction over other content models. Also, semantically finer-grained content was observed to be more effective compared to coarser-grained content.},
keywords={approximation theory;graph theory;social networking (online);LDA content models;Twitter events;Twitter social connection structure approximation;Twitter users;coarser-grained content;community social property;content model effectiveness assessment;content-based social link prediction;followership-based social communities;social community structures;social goodness;social graphs;social quality;unigram language model;user-generated content;Prediction algorithms;Predictive models;Semantics;Tagging;Twitter;User-generated content},
doi={10.1109/ASONAM.2016.7752374},
month={Aug},}
@INPROCEEDINGS{7752315,
author={S. H. Kumar and J. Pujara and L. Getoor and D. Mares and D. Gupta and E. Riloff},
booktitle={2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)},
title={Unsupervised models for predicting strategic relations between organizations},
year={2016},
pages={711-718},
abstract={Microblogging sites like Twitter provide a platform for sharing ideas and expressing opinions. The widespread popularity of these platforms and the complex social structure that arises within these communities provides a unique opportunity to understand the interactions between users. The political domain, especially in a multi-party system, presents compelling challenges, as political parties have different levels of alignment based on their political strategies. We use Twitter to understand the nuanced relationships between differing political entities in Latin America. Our model incorporates diverse signals from the content of tweets and social context from retweets, mentions and hashtag usage. Since direct communications between entities are relatively rare, we explore models based on the posts of users who interact with multiple political organizations. We present a quantitative and qualitative analysis of the results of models using different features, and demonstrate that a model capable of using sentiment strength, social context, and issue alignment has superior performance to less sophisticated baselines.},
keywords={politics;social networking (online);Latin America;Twitter;hashtag usage;issue alignment;microblogging sites;multiparty system;multiple political organizations;political domain;political parties;political strategies;sentiment strength;social context;strategic relation prediction;unsupervised models;Biological system modeling;Computational modeling;Organizations;Predictive models;Tagging;Twitter},
doi={10.1109/ASONAM.2016.7752315},
month={Aug},}
@INPROCEEDINGS{7752337,
author={S. N. Firdaus and C. Ding and A. Sadeghian},
booktitle={2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)},
title={Retweet prediction considering user's difference as an author and retweeter},
year={2016},
pages={852-859},
abstract={Social network is a hot topic of interest for the researchers in the field of computer science in recent years. The vast amount of data generated by these social networks play a very important role in information diffusion. Social network data are generated by its users. So, user's behavior and activities are being investigated by the researchers to get a logical view of social network platform. This research proposed a novel retweet prediction model which considers difference in user's behavior as an author (as reflected in the tweets) and a retweeter (as reflected in the retweets) and do the prediction accordingly. The proposed retweet prediction strategy taking this difference into consideration, gave better prediction accuracy than the conventional strategy. The findings of this research explains that in social networks, some users show different behavior indifferent roles and these differences may have impact on future research.},
keywords={information retrieval;social networking (online);author;computer science;information diffusion;retweet prediction model;retweet prediction strategy;retweeter;social network data;social network platform;user behavior;user difference;Computer science;Correlation;Feature extraction;Predictive models;Tagging;Twitter;personality;retweet prediction;topic model},
doi={10.1109/ASONAM.2016.7752337},
month={Aug},}
@INPROCEEDINGS{7603488,
author={A. Jiang},
booktitle={2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)},
title={Collision avoidance anti-collision algorithm based on subset partition},
year={2016},
pages={2001-2008},
abstract={Tag collision is a very important issue in RFID system. In order to improve the identification efficiency of the system, this paper proposes a collision avoidance anti-collision algorithm based on subset partition (CABSP). The CABSP narrows the identification range and reduces the probability of tag collision by splitting tag subsets. On this basis, the process of tag identification is divided into two phases including of tag slot reservation phase and tag reading phase, the reader eliminates the idle slots of the tag reading phase by detecting whether there is a reservation collision in the slot reservation phase, and improves the identification efficiency by selecting the tag to be recognized according to the collision avoidance mechanism. Simulation results show that the CABSP algorithm reduces the collision slots in the process of tag identification, and the system throughput can be increased to 90.4%, the performance is better than other existing algorithms in the literature in terms of the identification efficiency and communication complexity.},
keywords={computational complexity;probability;radiofrequency identification;telecommunication congestion control;CABSP algorithm;RFID system;collision avoidance anticollision algorithm-subset partition;collision avoidance mechanism;collision slot reduction;communication complexity;identification efficiency;reservation collision;slot reservation phase;splitting tag subsets;tag collision probability;tag identification process;tag reading phase;tag slot reservation phase;Algorithm design and analysis;Collision avoidance;Estimation;Heuristic algorithms;Partitioning algorithms;Prediction algorithms;Radiofrequency identification;RFID;anti-collision algorithm;collision avoidance;slot reservation;subset division},
doi={10.1109/FSKD.2016.7603488},
month={Aug},}
@INPROCEEDINGS{7551384,
author={A. Jain and C. Lin},
booktitle={2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)},
title={Back to the Future: Leveraging Belady's Algorithm for Improved Cache Replacement},
year={2016},
pages={78-89},
abstract={Belady's algorithm is optimal but infeasible because it requires knowledge of the future. This paper explains how a cache replacement algorithm can nonetheless learn from Belady's algorithm by applying it to past cache accesses to inform future cache replacement decisions. We show that the implementation is surprisingly efficient, as we introduce a new method of efficiently simulating Belady's behavior, and we use known sampling techniques to compactly represent the long history information that is needed for high accuracy. For a 2MB LLC, our solution uses a 16KB hardware budget (excluding replacement state in the tag array). When applied to a memory-intensive subset of the SPEC 2006 CPU benchmarks, our solution improves performance over LRU by 8.4%, as opposed to 6.2% for the previous state-of-the-art. For a 4-core system with a shared 8MB LLC, our solution improves performance by 15.0%, compared to 12.0% for the previous state-of-the-art.},
keywords={cache storage;4-core system;Belady algorithm;Belady behavior;LLC;cache accesses;cache replacement;memory-intensive subset;replacement state;sampling techniques;tag array;Art;Benchmark testing;Hardware;History;Marine vehicles;Optimized production technology;Prediction algorithms;Belady's Algorithm;Cache replacement},
doi={10.1109/ISCA.2016.17},
ISSN={1063-6897},
month={June},}
@INPROCEEDINGS{7538983,
author={K. Aono and N. Lajnef and F. Faridazar and S. Chakrabartty},
booktitle={2016 IEEE International Symposium on Circuits and Systems (ISCAS)},
title={Infrastructural health monitoring using self-powered Internet-of-Things},
year={2016},
pages={2058-2061},
abstract={By incorporating sensing capabilities in passive radio-frequency identification (RFID) tagging technology it is possible to extend the coverage of Internet-of-Things (IoT) to monitor the health of different segments of a large civil infrastructure like pavement highway, buildings or a multi-span bridge. The challenge in this regard is to deliver energy to the RFID sensors that are embedded inside the structures in a manner that they can continuously sense for occurrence of any rare structural events. This paper summarizes some of the progress that has been made to-date in the area of self-powered RFID sensor networks within the concept of IoT. The core sensor uses a self-powering method which directly harvests computational and storage energy from slight strain-variations in the structure. The event signatures can then be stored on a non-volatile memory and remotely retrieved at a later period of time. In this “sense now retrieve later” paradigm, self-powering is only used for continuous sensing and data-logging of essential statistics; whereas, data retrieval and reconfiguration is achieved using a low-cost commercial RFID system. Another advantage of using a commercial RFID system for data retrieval is that the related standards and FCC compliance are well established and the technology can be easily integrated with other IoT network infrastructure.},
keywords={Internet of Things;condition monitoring;data loggers;energy harvesting;information retrieval;radiofrequency identification;structural engineering computing;wireless sensor networks;FCC compliance;IoT network infrastructure;civil infrastructure;computational energy harvesting;continuous sensing;data reconfiguration;data retrieval;data-logging;infrastructural health monitoring;low-cost commercial RFID system;nonvolatile memory;radio-frequency identification tagging technology;self-powered Internet-of-Things;self-powered RFID sensor networks;sense now retrieve later paradigm;storage energy harvesting;strain-variations;Logic gates;Mechanical sensors;Monitoring;Radio frequency;Radiofrequency identification;Strain;Fatigue prediction;Internet-of-Things;NFC;RFID;infrastructural health monitoring;piezo-floating gate sensor;self-powered sensor},
doi={10.1109/ISCAS.2016.7538983},
month={May},}
@INPROCEEDINGS{7495916,
author={S. C. Çelik and Ö. D. İncel},
booktitle={2016 24th Signal Processing and Communication Application Conference (SIU)},
title={Semantic place prediction from mobile phone sensors},
year={2016},
pages={1021-1024},
abstract={Semantic place prediction problem is the process of giving semantic names, such as school, home, office, to locations. Different than the localization problem where the coordinates of a place are predicted, the aim is to semantically characterize the location. While the GPS coordinates of a place can be utilized in solving the problem, phone usage patterns of the users in that location can be used as well. In this paper, the aim is to semantically classify locations of smart phone users utilizing the data collected from wireless interfaces and the motion sensors available on the phones with machine learning algorithms. The efficiency of features extracted from raw data is analysed in terms of metrics such as accuracy, using different classification algorithms. The results show that, while random forest and decision tree algorithms achieve 66% accuracy with only temporal features, adding features from device properties and activity features increases the accuracy up to 99%.},
keywords={Global Positioning System;decision trees;feature extraction;learning (artificial intelligence);sensors;smart phones;GPS coordinate;classification algorithm;decision tree algorithm;features extraction;localization problem;machine learning algorithm;mobile phone sensor;motion sensor;phone usage pattern;random forest algorithm;semantic name;semantic place prediction;smart phone user;wireless interface;Feature extraction;Global Positioning System;IEEE 802.11 Standard;Semantics;Sensors;Smart phones;mobile phone sensing;semantic place tagging},
doi={10.1109/SIU.2016.7495916},
month={May},}
@ARTICLE{7495049,
author={Z. Liu and B. J. Jansen},
journal={IEEE Transactions on Computational Social Systems},
title={Understanding and Predicting Question Subjectivity in Social Question and Answering},
year={2016},
volume={3},
number={1},
pages={32-41},
abstract={The explosive popularity of social networking sites has provided an additional venue for online information seeking. By posting questions in their status updates, more and more people are turning to social networks to fulfill their information needs. Given that understanding individuals' information needs could improve the performance of question answering, in this paper, we model the task of intent detection as a binary classification problem, and thus for each question, two classes are defined: subjective and objective. We use a comprehensive set of lexical, syntactical, and contextual features to build the classifier and the experimental results show satisfactory classification performance. By applying the classifier on a larger dataset, we then present in-depth analyses to compare subjective and objective questions, in terms of the way they are being asked and answered. We find that the two types of questions exhibited very different characteristics, and further validate the expected benefits of differentiating questions according to their subjectivity orientations.},
keywords={information needs;pattern classification;question answering (information retrieval);social networking (online);binary classification problem;classifier;contextual features;information needs;intent detection;lexical features;objective class;online information seeking;question subjectivity prediction;question subjectivity understanding;social networking sites;social question and answering;subjective class;subjectivity orientations;syntactical features;Context;Feature extraction;Predictive models;Tagging;Training data;Twitter;Information seeking;Twitter;social question and answering (social Q&A);social search;subjectivity analysis social network},
doi={10.1109/TCSS.2016.2564400},
ISSN={2329-924X},
month={March},}
@INPROCEEDINGS{7490980,
author={Yuhua Fan},
booktitle={2015 4th International Conference on Computer Science and Network Technology (ICCSNT)},
title={Scene classification based on knowledge sharing and latent structural constraints},
year={2015},
volume={01},
pages={1356-1360},
abstract={We address the issue when applying local discriminative information to construct the mid-level representing for scene classification. It is often caused by the confusion of local semantic similar when convolving local cues with input scene images. This problem makes mid-level semantic representation for images, not distinctive enough to classify those unseen data correctly. We first learn knowledge sharing for exploring distinctive local semantic information based on latent semantic analysis and label ranking. Then, we employ the latent structural constraints of scene configuration for the semantic prediction. Finally, we infer the scene semantic using Bayes rules. Extensive Experimental results have illustrated the effectiveness and efficient of our method for local semantic annotation and scene classification.},
keywords={Computer vision;Image retrieval;Linear programming;Semantics;Tagging;Training;Visualization;Scene classification;knowledge sharing;structural constraints},
doi={10.1109/ICCSNT.2015.7490980},
month={Dec},}
@INPROCEEDINGS{7490061,
author={P. Bakliwal and C. V. Jawahar},
booktitle={2015 Fifth National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG)},
title={Active learning based image annotation},
year={2015},
pages={1-4},
abstract={Automatic image annotation is the computer vision task of assigning a set of appropriate textual tags to a novel image. The aim is to eventually bridge the semantic gap of visual and textual representations with the help of these tags. This also has applications in designing scalable image retrieval systems and providing multilingual interfaces. Though a wide varieties of powerful machine learning algorithms have been explored for the image annotation problem in the recent past, nearest neighbor techniques still yield superior results to them. A challenge ahead of the present day annotation schemes is the lack of sufficient training data. In this paper, an active Learning based image annotation model is proposed. We leverage the image-to-image and image-to-tag similarities to decide the best set of tags describing the semantics of an image. The advantages of the proposed model includes: (a). It is able to output the variable number of tags for images which improves the accuracy. (b). It is effectively able to choose the difficult samples that needs to be manually annotated and thereby reducing the human annotation efforts. Studies on Corel and IAPR TC-12 datasets validate the effectiveness of this model.},
keywords={computer vision;image representation;image retrieval;learning (artificial intelligence);active learning;automatic image annotation;computer vision task;image retrieval systems;image semantics;image-to-image similarities;image-to-tag similarities;machine learning algorithms;multilingual interfaces;nearest neighbor techniques;semantic gap;textual representations;textual tags;visual representations;Hidden Markov models;Prediction algorithms;Semantics;Training;Training data;Uncertainty;Vocabulary},
doi={10.1109/NCVPRIPG.2015.7490061},
month={Dec},}
@INPROCEEDINGS{7489313,
author={M. Garcia and P. Rose and R. Sung and S. El-Tawab},
booktitle={2016 IEEE Systems and Information Engineering Design Symposium (SIEDS)},
title={Secure Smart Parking at James Madison University via the Cloud Environment (SPACE)},
year={2016},
pages={271-276},
abstract={Parking at James Madison University has been a challenge for years. With the increase of enrollment, this continues to be a more difficult problem. Currently at JMU, a few parking garages (e.g. Champions Drive Parking Deck & Grace Street Parking Deck) have some capability of counting how many vehicles are entering or leaving the garage, but the technology has not been widely embraced across campus. JMU is in need of a smart system that allows a student/faculty/staff/visitor to know how many available parking spaces are in a particular lot at any given time. With this information, users are given a suggestion for a place to park. The JMU Secure Smart Parking via the Cloud Environment initiative aims to address this parking problem. Using a Radio-frequency identification scanner, our system is able to count the number of vehicles entering and leaving each parking lot on campus. The current decal that is provided to each vehicle will be replaced with a smart decal with an embedded tag inside of it. We also developed a secure, mobile-friendly web application and mobile apps (both on an iOS and Android platforms) that can inform the driver of the number of available spots in a desired parking lot. By harnessing the power of the MEAN stack (Mongo, Express, Angular, and Node) and cloud-based computing, the JMU community will gain a near real-time perspective of the parking situation on-campus.},
keywords={Internet;cloud computing;intelligent transportation systems;mobile computing;radiofrequency identification;smart phones;JMU secure smart parking;James Madison University;MEAN stack;SPACE;cloud environment;cloud-based computing;embedded tag;mobile application;mobile-friendly Web application;parking garages;radio-frequency identification scanner;Cloud computing;Conferences;Databases;Prediction algorithms;Radiofrequency identification;Servers;Vehicles;Cloud Computing;Intelligent Transportation;Probabilistic Decision;Secure System;Smart City},
doi={10.1109/SIEDS.2016.7489313},
month={April},}
@INPROCEEDINGS{7486758,
author={B. Zhu and J. Wang and G. Zeng},
booktitle={2016 2nd International Conference on Control, Automation and Robotics (ICCAR)},
title={A non-integral-Q Algorithm for RFID system in anti-collision},
year={2016},
pages={374-377},
abstract={RFID technology is one of four basic technologies in the Internet of Things. In order to realize its intensive and large-scale uses, the key point is to increase the identification speed of tags. The conventional algorithm only uses integral Q which decreases the efficiency dramatically. In order to decrease the convergence time and rise the slot efficiency, this article proposes a novel anti-collision algorithm based on non-integral Q-a key parameter determining the frame length. By turning the rounding Q in the conventional Q-Algorithm into rounding 2Q, the frame length can get closer to its optimal value. Monte-Carlo simulation result shows that the improved algorithm can keep the throughout rate in a relatively high level (about 0.31) and decrease the total number of time-slots by about 30%.},
keywords={Internet of Things;Monte Carlo methods;radiofrequency identification;Internet of Things;Monte-Carlo simulation;RFID system;anticollision algorithm;nonintegral Q;nonintegral-Q algorithm;Algorithm design and analysis;Heuristic algorithms;Prediction algorithms;Protocols;Radiation detectors;Radiofrequency identification;Throughput;EPC-C1G2;Q-algorithm;RFID;anti-collision algorithm},
doi={10.1109/ICCAR.2016.7486758},
month={April},}
@INPROCEEDINGS{7484182,
author={S. Delbruel and D. Frey and F. Taïani},
booktitle={2016 IEEE International Conference on Cloud Engineering (IC2E)},
title={Exploring the Use of Tags for Georeplicated Content Placement},
year={2016},
pages={172-181},
abstract={A large portion of today's Internet traffic originates from streaming and video services. Such services rely on a combination of distributed datacenters, powerful content delivery networks (CDN), and multi-level caching. In spite of this infrastructure, storing, indexing, and serving these videos remains adaily engineering challenge that requires increasing efforts on the part of providers and ISPs. In this paper, we explore how the tags attached to videos by users could help improve this infrastructure, and lead to better performance on a global scale. Our analysis shows that tags can be interpreted as markers of a video's geographic diffusion, with some tags strongly linked to well identified geographic areas. Based on our findings, we demonstrate the potential of tags to help predict distribution of a video's views, and present results suggesting that tags canhelp place videos in globally distributed datacenters. We show in particular that even a simplistic approach based on tags can help predict a minimum of 65.9% of a video's views for a majority of videos, and that a simple tag-based placement strategy is able to improve the hit rate of a distributed on-line video service by up to 6.8% globally over a naive random allocation.},
keywords={Internet;video signal processing;CDN;ISP;Internet traffic;content delivery networks;distributed online video service;geographic diffusion;georeplicated content placement;globally distributed datacenters;multilevel caching;naive random allocation;tag-based placement strategy;tags;video services;Electronic mail;Entropy;Google;Internet;Pediatrics;Streaming media;YouTube;User-generated content;YouTube;prediction;tag},
doi={10.1109/IC2E.2016.37},
month={April},}
@ARTICLE{7483548,
author={J. C. Gomez and T. Tommasi and S. Zoghbi and M. F. Moens},
journal={IEEE Latin America Transactions},
title={What Would They Say? Predicting User #039;s Comments in Pinterest},
year={2016},
volume={14},
number={4},
pages={2013-2019},
abstract={When we refer to an image that attracts our attention, it is natural to mention not only what is literally depicted in the image, but also the sentiments, thoughts and opinions that it invokes in ourselves. In this work we deviate from the standard mainstream tasks of associating tags or keywords to an image, or generating content image descriptions, and we introduce the novel task of automatically generate user comments for an image. We present a new dataset collected from the social media Pinterest and we propose a strategy based on building joint textual and visual user models, tailored to the specificity of the mentioned task. We conduct an extensive experimental analysis of our approach on both qualitative and quantitative terms, which allows to assess the value of the proposed approach and shows its encouraging results against several existing image-to-text methods.},
keywords={image processing;social networking (online);image descriptions;image-to-text methods;social media Pinterest;standard mainstream;user comment prediction;visual user models;Buildings;Facebook;Media;Pins;Standards;User-generated content;Visualization;Deep-Learning Representation;Multimodal Clustering;Pinterest;Social Media;User Generated Content},
doi={10.1109/TLA.2016.7483548},
ISSN={1548-0992},
month={April},}
@INPROCEEDINGS{7474819,
author={A. Hernández and V. Sanchez and G. Sánchez and H. Pérez and J. Olivares and K. Toscano and M. Nakano and V. Martinez},
booktitle={2016 IEEE International Conference on Industrial Technology (ICIT)},
title={Security attack prediction based on user sentiment analysis of Twitter data},
year={2016},
pages={610-617},
abstract={In recent years, security attacks on the web have been perpetrated by hacker activist organizations that aim to destabilize (using different techniques) web services in a specific context for which they are motivated. Predicting these attacks is an important task that helps to consider what actions should be taken if the attack is latent. Although there are applications to detect security threats on the web, currently there is no system that can predict or forecast whether the attacks can reach consummation. This paper presents a sentiment analysis method on Twitter content to predict future attacks on the web. The method is based on the daily collection of tweets from two sets of users; those who use the platform as a means of expression for views on relevant issues, and those who use it to present contents related to security attacks in the web. Daily information is converted into data that can be analysed statistically to predict whether there is a possibility of an attack. The latter is done by analyzing the collective sentiment of users and groups of hacking activists in response to a global event.},
keywords={Web services;computer crime;sentiment analysis;social networking (online);Twitter data;Web services;hacker activist organizations;security attack prediction;security threats detection;tweets;user sentiment analysis;Computer crime;Context;Sentiment analysis;Tagging;Twitter;Web services;Twitter;attack;hacking;prediction;security;social networks;user sentiment analysis},
doi={10.1109/ICIT.2016.7474819},
month={March},}
@INPROCEEDINGS{7476215,
author={Y. Zuo and K. Yada},
booktitle={2015 2nd Asia-Pacific World Congress on Computer Science and Engineering (APWC on CSE)},
title={Using statistical learning theory for purchase behavior prediction via direct observation of in-store behavior},
year={2015},
pages={1-6},
abstract={The advanced investigation of in-store behavior has been dramatically intrigued researchers in the last decade, since the radio frequency identification (RFID) technology was introduced into the grocery stores for information acquisition via tracking the customers. Shopping carts and baskets with a small RFID tag attached are recognized as surrogates instead of enumerators to trail the customers, which can provide an objective and direct observation to measure the in-store behavior of customers. The main purpose of this article is to improve the understanding of purchase behavior of customers with emphasis on meaningful knowledge from the study of their in-store behavior. For this purpose, we present an extraction of purchase behavior using statistical learning theory on dealing with the RFID data, which can describe the purchase behavior into a process of the customers' in-store behavior. In this paper, we introduce three types of factors - personal factor, attitudinal factor and behavioral factor to train a model of purchase behavior, and investigate their effect on purchase behavior among heterogeneous customers grouping by their past purchase incidence. Finally, due to the additional effect of choice switching on purchase intention is also investigated basing on the primary situation, we suppose that these investigation outcomes can result from measuring customers' response to sales promotion due to micro details of RFID data, which can also be applicable to business decision-making implications from the perspective of customers.},
keywords={consumer behaviour;learning (artificial intelligence);promotion (marketing);purchasing;radiofrequency identification;sales management;statistical analysis;RFID tag;attitudinal factor;behavioral factor;business decision-making implications;choice switching;customer purchase behavior understanding improvement;customer response measurement;customer tracking;direct observation;grocery stores;heterogeneous customers;in-store behavior;information acquisition;personal factor;purchase behavior prediction;purchase intention;radio frequency identification technology;sales promotion;shopping baskets;shopping carts;statistical learning theory;Decision making;Predictive models;Radiofrequency identification;Statistical learning;Support vector machines;Switches;In-store Behavior;Purchase Behavior Prediction;RFID Data;Statistical Learning Theory;Switching Effect},
doi={10.1109/APWCCSE.2015.7476215},
month={Dec},}
@INPROCEEDINGS{7439322,
author={J. Bassi and S. Manna and Y. Sun},
booktitle={2016 IEEE Tenth International Conference on Semantic Computing (ICSC)},
title={Construction of a Geo-Location Service Utilizing Microblogging Platforms},
year={2016},
pages={162-165},
abstract={The popularity of social media over the past several years, especially sites such as Twitter, has presented a network for up to the minute information on events across the globe. The information presented on these sites can be extremely helpful in the case of an emergency, however, the vast amount of data to examine and the low adoption of geo-tagging on this site makes it difficult, if not impossible, for emergency services to respond to information gathered from social media. Taking this into consideration, this paper presents a proof of concept for identifying and retrieving different geo-locations from Tweets and extracting the GPS coordinates from this data to approximately plot them in a map.},
keywords={Global Positioning System;geographic information systems;graphical user interfaces;information retrieval;social networking (online);GPS coordinate extraction;Twitter;emergency services;geo-location identification;geo-location retrieval;geo-location service construction;geo-tagging;information gathering;microblogging platforms;social media;tweets;Data mining;Emergency services;Graphical user interfaces;Libraries;Natural language processing;Prediction algorithms;Twitter;Tweets;emergency services;geo-location prediction;google maps;tool},
doi={10.1109/ICSC.2016.60},
month={Feb},}
@INPROCEEDINGS{7427649,
author={W. N. Robinson and T. Deng and Z. Qi},
booktitle={2016 49th Hawaii International Conference on System Sciences (HICSS)},
title={Developer Behavior and Sentiment from Data Mining Open Source Repositories},
year={2016},
pages={3729-3738},
abstract={Developer sentiment may wax and wane as a project progresses. Open-source projects that attract and retain developers tend to be successful. It may be possible to predict project success, in part, if one can measure developer behavior and sentiment -- projects with active, happy developers are more likely to succeed. We have analyzed GitHub.com projects in an attempt to model these concepts. We have data mined 124 projects from GitHub.com. The projects were automatically mined using sequence mining methods to derive a behavioral model of developer activities. The projects were also mined for developer sentiment. Finally, a regression model shows how sentiment varies with behavioral differences -- a change in behavior is correlated with a change in sentiment. The relationship between sentiment and success is not directly explored, herein. This research project is a preliminary step in a larger research project aimed at understanding and monitoring FLOSS projects using a process modeling approach.},
keywords={data mining;human factors;project management;public domain software;regression analysis;sentiment analysis;software development management;FLOSS projects;GitHub.com projects;behavioral differences;behavioral model;data mining;developer behavior;developer sentiment;open source repositories;open-source projects;process modeling;project success prediction;regression model;sequence mining methods;Analytical models;Data mining;Monitoring;Open source software;Sentiment analysis;Tagging;GitHub;data mining;developers;sentiment analysis;text mining},
doi={10.1109/HICSS.2016.465},
ISSN={1530-1605},
month={Jan},}
@INPROCEEDINGS{7403624,
author={N. Dugué and A. Perez and M. Danisch and F. Bridoux and A. Daviau and T. Kolubako and S. Munier and H. Durbano},
booktitle={2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)},
title={A reliable and evolutive web application to detect social capitalists},
year={2015},
pages={741-744},
abstract={On Twitter, social capitalists use dedicated hashtags and mutual subscriptions to each other in order to gain followers and to be retweeted. Their methods are successful enough to make them appear as influent users. Indeed, applications dedicated to the influence measurement such as Klout and Kred give high scores to most of these users. Meanwhile, their high number of retweets and followers are not due to the relevance of the content they tweet, but to their social capitalism techniques. In order to be able to detect these users, we train a classifier using a dataset of social capitalists and regular users. We then implement this classifier in a web application that we call DDP. DDP allows users to test whether a Twitter account is a social capitalist or not and to visualize the data we use to make the prediction. DDP allows administrator to crawl data from a lot of users automatically. Furthermore, administrators can manually label Twitter accounts as social capitalists or regular users to add them into the dataset. Finally, administrators can train new classifiers in order to take into account the new Twitter accounts added to the dataset, and thus making evolve the classifier with these new recently collected data. The web application is thus a way to collect data, make evolve the knowledge about social capitalists and to keep detecting them efficiently.},
keywords={Internet;pattern classification;social networking (online);DDP;Twitter account;Web application;classifier;social capitalism techniques;social capitalist detection;Data visualization;Databases;Logistics;Standards;Tag clouds;Twitter},
doi={10.1145/2808797.2808799},
month={Aug},}
@INPROCEEDINGS{7404780,
author={C. Ding and L. Xie and J. Yan and W. Zhang and Y. Liu},
booktitle={2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)},
title={Automatic prosody prediction for Chinese speech synthesis using BLSTM-RNN and embedding features},
year={2015},
pages={98-102},
abstract={Prosody affects the naturalness and intelligibility of speech. However, automatic prosody prediction from text for Chinese speech synthesis is still a great challenge and the traditional conditional random fields (CRF) based method always heavily relies on feature engineering. In this paper, we propose to use neural networks to predict prosodic boundary labels directly from Chinese characters without any feature engineering. Experimental results show that stacking feed-forward and bidirectional long short-term memory (BLSTM) recurrent network layers achieves superior performance over the CRF-based method. The embedding features learned from raw text further enhance the performance.},
keywords={embedded systems;linguistics;recurrent neural nets;speech intelligibility;speech synthesis;text analysis;BLSTM recurrent network layer;BLSTM-RNN;CHINESE SPEECH SYNTHESIS;CRF-based method;Chinese character;Chinese speech synthesis;automatic prosody prediction;bidirectional long short-term memory recurrent network layer;conditional random field-based method;feature engineering;feed-forward;prosodic boundary label;speech intelligibility;speech naturalness;Hidden Markov models;Logic gates;Neural networks;Speech;Speech synthesis;Tagging;Training;BLSTM;automatic prosody prediction;embedding features;neural network;speech synthesis},
doi={10.1109/ASRU.2015.7404780},
month={Dec},}
@INPROCEEDINGS{7403707,
author={R. M. Filho and J. M. Almeida and G. L. Pappa},
booktitle={2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)},
title={Twitter population sample bias and its impact on predictive outcomes: A case study on elections},
year={2015},
pages={1254-1261},
abstract={In the past years a lot of effort has been spent analyzing online social network data to understand how the world reality is reflected in the "virtual" world. Twitter is by far the network most used in these studies, given its policy of public data availability. However, a big discussion is still on on whether the data available is enough to make user characterization or event outcomes prediction, and what are the pitfalls people do not usually account for. In this direction, we propose a new methodology for drawing representative samples from Twitter data, which is divided into four phases: (i) user filtering, (ii) user demographic characterization, (iii) user sampling, and (iv) event prediction. The methodology is tested into a common scenario in Twitter event outcome prediction: elections. The methodology was tested with municipality elections from six different Brazilian cities, and compared to official election results. Results show it is worth further investigating the topic, but that a very hight number of messages is required to match real data distributions.},
keywords={data analysis;government data processing;social networking (online);Brazilian cities;Twitter event outcome prediction;Twitter population sample bias;elections;event prediction;municipality elections;user demographic characterization;user filtering;user sampling;Cities and towns;Media;Nominations and elections;Sentiment analysis;Tagging;Twitter},
doi={10.1145/2808797.2809328},
month={Aug},}
@INPROCEEDINGS{7403709,
author={N. Dokoohaki and F. Zikou and D. Gillblad and M. Matskin},
booktitle={2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)},
title={Predicting Swedish elections with Twitter: A case for stochastic link structure analysis},
year={2015},
pages={1269-1276},
abstract={The question that whether Twitter data can be leveraged to forecast outcome of the elections has always been of great anticipation in the research community. Existing research focuses on leveraging content analysis for positivity or negativity analysis of the sentiments of opinions expressed. This is while, analysis of link structure features of social networks underlying the conversation involving politicians has been less looked. The intuition behind such study comes from the fact that density of conversations about parties along with their respective members, whether explicit or implicit, should reflect on their popularity. On the other hand, dynamism of interactions, can capture the inherent shift in popularity of accounts of politicians. Within this manuscript we present evidence of how a well-known link prediction algorithm, can reveal an authoritative structural link formation within which the popularity of the political accounts along with their neighbourhoods, shows strong correlation with the standing of electoral outcomes. As an evidence, the public time-lines of two electoral events from 2014 elections of Sweden on Twitter have been studied. By distinguishing between member and official party accounts, we report that even using a focus-crawled public dataset, structural link popularities bear strong statistical similarities with vote outcomes. In addition we report strong ranked dependence between standings of selected politicians and general election outcome, as well as for official party accounts and European election outcome.},
keywords={politics;social networking (online);European election outcome;Swedish election prediction;Twitter data;authoritative structural link formation;conversation density;election outcome forecasting;electoral events;focus-crawled public dataset;interaction dynamism;link prediction algorithm;member accounts;negativity analysis;official party accounts;opinion sentiments;party members;politician account popularity;positivity analysis;public time-lines;statistical similarities;stochastic link structure analysis;strong-ranked dependence;structural link popularities;Data mining;Electronic mail;Europe;Nominations and elections;Tagging;Twitter},
doi={10.1145/2808797.2808915},
month={Aug},}
@INPROCEEDINGS{7395833,
author={R. Lemahieu and S. Van Canneyt and C. De Boom and B. Dhoedt},
booktitle={2015 IEEE International Conference on Data Mining Workshop (ICDMW)},
title={Optimizing the Popularity of Twitter Messages through User Categories},
year={2015},
pages={1396-1401},
abstract={In this paper, we investigate how the category of a Twitter user can be used to better predict and optimize the popularity of tweets. The contributions of this paper are threefold. First, we compare the influence of content features on the popularity of tweets for different user categories. Second, we present a regression model to predict the popularity of tweets given the content features as input. To construct this model, we interpolate a generic regression model, which is trained on all data, and a category-specific model, which is only trained on tweets from users of the same category as the user of the given tweet. In this way we can combine the advantage of the robustness of a generic model, with the ability of category-specific models to pick up on category-specific influence of content features. The third contribution is the investigation of the feasibility of boosting the popularity of a tweet by setting up an experiment in which we proactively adapt content features in order to optimize the popularity of tweets. Based on this research, we conclude that the introduction of user categories leads to a more precise analysis and better predictions. In the hands-on experiment, we observed a gain in popularity by proactively adapting content features.},
keywords={interpolation;regression analysis;social networking (online);Twitter message popularity optimization;category-specific model;content features;generic regression model;user categories;Adaptation models;Data models;Entertainment industry;Media;Predictive models;Tagging;Twitter;Twitter;optimization;popularity prediction;user categories},
doi={10.1109/ICDMW.2015.39},
month={Nov},}
@INPROCEEDINGS{7358435,
author={S. Alami and O. Elbeqqali},
booktitle={2015 10th International Conference on Intelligent Systems: Theories and Applications (SITA)},
title={Cybercrime profiling: Text mining techniques to detect and predict criminal activities in microblog posts},
year={2015},
pages={1-5},
abstract={The exponential development in online social media allows users around the globe the possibility to share and communicate information and ideas freely in different formats of data via internet. This emerging media has become a dominant communication tool and it has been used as a communication channel in several events, especially “The Arab Spring” and BOSTON'S attack etc. In order to develop useful profiles of different cybercriminals, text mining techniques is an effective way to detect and predict criminal activities in microblog posts taking account the problems of data sparseness and semantic gap. The hashtags used on Twitter (e.g., #arabspring, #BostonAttack) contains outstanding indicators to detect events and trending topics especially to target and detect suspicious topics and eventual illegal events. Similarity approach is used in text analysis to detect suspicious posts in microblog publications. The evaluation of our proposed approach is done within real posts.},
keywords={computer crime;data mining;social networking (online);text analysis;criminal activities detection;criminal activities prediction;cybercrime profiling;cybercriminals;data sparseness;microblog posts;microblog publications;online social media;semantic gap;suspicious posts detection;text analysis;text mining techniques;Internet;Media;Semantics;Tagging;Text mining;Twitter;Cybercrime;NCD Normalized Compression Distance;Profiling;Semantic Web;Similarity;Social Media;Suspicious Profile;Text Analysis;Text Mining},
doi={10.1109/SITA.2015.7358435},
month={Oct},}
@INPROCEEDINGS{7359515,
author={T. I. Modipa and M. H. Davel},
booktitle={2015 Pattern Recognition Association of South Africa and Robotics and Mechatronics International Conference (PRASA-RobMech)},
title={Predicting vowel substitution in code-switched speech},
year={2015},
pages={154-159},
abstract={The accuracy of automatic speech recognition (ASR) systems typically degrades when encountering code-switched speech. Some of this degradation is due to the unexpected pronunciation effects introduced when languages are mixed. Embedded (foreign) phonemes typically show more variation than phonemes from the matrix language: either approximating the embedded language pronunciation fairly closely, or realised as any of a set of phonemic counterparts from the matrix language. In this paper we describe a technique for predicting the phoneme substitutions that are expected to occur during code-switching, using non-acoustic features only. As case study we consider Sepedi/English code switching and analyse the different realisations of the English schwa. A code-switched speech corpus is used as input and vowel substitutions identified by auto-tagging this corpus based on acoustic characteristics. We first evaluate the accuracy of our auto-tagging process, before determining the predictability of our auto-tagged corpus, using non-acoustic features.},
keywords={acoustic signal processing;speech recognition;ASR systems;English code switching;English schwa;Sepedi code switching;acoustic characteristics;automatic speech recognition systems;code-switched speech;code-switched speech corpus;corpus auto-tagging;embedded foreign language phonemes;matrix language;non-acoustic features;nonacoustic features;phoneme substitution prediction;pronunciation effects;vowel substitution prediction;vowel substitutions;Acoustics;Hidden Markov models;Matrices;Speech;Speech coding;Speech recognition;Switches},
doi={10.1109/RoboMech.2015.7359515},
month={Nov},}
@INPROCEEDINGS{7351630,
author={P. Koutras and A. Zlatintsi and E. Iosif and A. Katsamanis and P. Maragos and A. Potamianos},
booktitle={2015 IEEE International Conference on Image Processing (ICIP)},
title={Predicting audio-visual salient events based on visual, audio and text modalities for movie summarization},
year={2015},
pages={4361-4365},
abstract={In this paper, we present a new and improved synergistic approach to the problem of audio-visual salient event detection and movie summarization based on visual, audio and text modalities. Spatio-temporal visual saliency is estimated through a perceptually inspired frontend based on 3D (space, time) Gabor filters and frame-wise features are extracted from the saliency volumes. For the auditory salient event detection we extract features based on Teager-Kaiser Energy Operator, while text analysis incorporates part-of-speech tagging and affective modeling of single words on the movie subtitles. For the evaluation of the proposed system, we employ an elementary and non-parametric classification technique like KNN. Detection results are reported on the MovSum database, using objective evaluations against ground-truth denoting the perceptually salient events, and human evaluations of the movie summaries. Our evaluation verifies the appropriateness of the proposed methods compared to our baseline system. Finally, our newly proposed summarization algorithm produces summaries that consist of salient and meaningful events, also improving the comprehension of the semantics.},
keywords={Gabor filters;estimation theory;feature extraction;mathematical operators;text analysis;video signal processing;Gabor filter;Teager-Kaiser energy operator;affective modelling;audio modality;audio-visual salient event prediction;frame-wise feature extraction;movie summarization;part-of-speech tagging;text analysis;text modality;visual modality;visual saliency estimation;Color;Feature extraction;Motion pictures;Semantics;Text analysis;Three-dimensional displays;Visualization;Visual saliency;affective text analysis;audio-visual salient events;auditory saliency;movie summarization},
doi={10.1109/ICIP.2015.7351630},
month={Sept},}
@INPROCEEDINGS{7344887,
author={Y. Lewenberg and Y. Bachrach and S. Volkova},
booktitle={2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA)},
title={Using emotions to predict user interest areas in online social networks},
year={2015},
pages={1-10},
abstract={We examine the relation between the emotions users express on social networks and their perceived areas of interests, based on a sample of Twitter users. Our methodology relies on training machine learning models to classify the emotions expressed in tweets, according to Ekman's six high-level emotions. We then used raters, sourced from Amazon's Mechanical Turk, to examine several Twitter profiles and to determine whether the profile owner is interested in various areas, including sports, movies, technology and computing, politics, news, economics, science, arts, health and religion. We find that the propensity of a user to express various emotions correlates with their perceived degree of interest in various areas. We present several models that use the emotional distribution of a Twitter user, as reflected by their tweets, to predict whether they are interested or disinterested in a topic or to determine their degree of interest in a topic.},
keywords={emotion recognition;learning (artificial intelligence);pattern classification;psychology;social aspects of automation;social networking (online);Amazon Mechanical Turk;Ekman six high-level emotions;Twitter profiles;Twitter users;arts;computing;economics;emotional distribution;emotions classification;health;movies;news;online social networks;politics;raters;religion;science;sports;technology;training machine learning models;tweets;user degree-of-interest;user expression;user interest areas prediction;Advertising;Electronic mail;Media;Predictive models;Tagging;Twitter},
doi={10.1109/DSAA.2015.7344887},
month={Oct},}
@INPROCEEDINGS{7332379,
author={M. Fantuzzi and D. Masotti and A. Costanzo},
booktitle={2015 International EURASIP Workshop on RFID Technology (EURFID)},
title={Electromagnetic prediction of antenna layout impact on UWB localization and sensing},
year={2015},
pages={16-21},
abstract={This paper presents a compact antenna solution able to perform communication and RF energy harvesting functionalities, in order to fulfill the future wireless network requirement of simultaneous wireless information and power transfer. The system is thought for next generation passive RFID tags: it combines a single-port antenna and a diplexer, both designed on paper substrate to operate in the European UWB and UHF bands, for communication/localization and energy harvesting purposes, respectively. The impact of the antenna layout on system performance is discussed, from both communication and energy harvesting points of view.},
keywords={5G mobile communication;UHF antennas;energy harvesting;inductive power transmission;multiplexing equipment;next generation networks;radiofrequency identification;substrates;ultra wideband antennas;ultra wideband communication;European UHF band;European UWB band;RF energy harvesting functionalities;UWB localization;UWB sensing;diplexer;electromagnetic prediction;future wireless network requirement;next fifth generation cellular networks;next generation passive RFID tags;paper substrate;simultaneous wireless information-and-power transfer;single-port antenna layout impact;system performance;Dipole antennas;Energy harvesting;Layout;Ports (Computers);UHF antennas;Ultra wideband antennas;UHF antennas;Ultra wideband communication;radiofrequency energy harvesting;ultra wideband antennas},
doi={10.1109/EURFID.2015.7332379},
month={Oct},}
@INPROCEEDINGS{7321473,
author={S. Wawrzyniak and W. Niemiro},
booktitle={2015 Federated Conference on Computer Science and Information Systems (FedCSIS)},
title={Clustering approach to the problem of human activity recognition using motion data},
year={2015},
pages={411-416},
abstract={This paper describes authors' solution to the task set in AAIA'15 Data Mining Competition: Tagging Firefighter Activities at a Fire Scene (https://knowledgepit.fedcsis.org/contest/view.php?id=106). Method involves LDA classification on a preprocessed time series data with a unique label transformation technique using K-Means clustering. Data were collected from accelerometer and gyroscope readings.},
keywords={data mining;image classification;image motion analysis;pattern clustering;time series;AAIA'15 data mining competition;LDA classification;accelerometer;gyroscope;human activity recognition;k-means clustering approach;motion data;preprocessed time series data;unique label transformation technique;Computers;Gyroscopes;Prediction algorithms},
doi={10.15439/2015F424},
month={Sept},}
@INPROCEEDINGS{7294415,
author={R. Makki and S. Brooks and E. E. Milios},
booktitle={2014 International Conference on Information Visualization Theory and Applications (IVAPP)},
title={Context-specific sentiment lexicon expansion via minimal user interaction},
year={2014},
pages={178-186},
abstract={One of the important factors in the performance of sentiment analysis methods is having a comprehensive sentiment lexicon. However, since sentiment words have different polarities not only in different domains, but also in different contexts within the same domain, constructing such context-specific sentiment lexicons is not an easy task. The high costs of manually constructing such lexicons motivate researchers to create automatic methods for finding sentiment words and assigning their polarities. However, existing methods may encounter ambiguous cases with contradictory evidence which are hard to automatically resolve. To address this problem, we aim to engage the user in the process of polarity assignment and improve the quality of the generated lexicon via minimal user effort. A novel visualization is employed to present the results of the automatic algorithm, i.e., the extracted sentiment pairs along with their polarities. User interactions are provided to facilitate the supervision process. The results of our user study demonstrate (1) involving the user in the polarity assignment process improves the quality of the generated lexicon significantly, and (2) participants in the study preferred our visual interface and conveyed that it is easier to use compared to a text-based interface.},
keywords={Color;Context;Image color analysis;Prediction algorithms;Sentiment analysis;Tag clouds;Visualization;Context-Dependent Sentiment Lexicon;User Interaction;Visualization},
month={Jan},}
@INPROCEEDINGS{7284479,
author={X. Jin and D. Wei and Y. Xu and L. Jin and X. Huang},
booktitle={2015 IEEE 5th International Conference on Electronics Information and Emergency Communication},
title={A novel RFID tag estimation algorithm based on DFSA},
year={2015},
pages={26-29},
abstract={Tag collision is a very important problem in RFID system that plenty of collisions can result in failed transmission. The tag estimation algorithm is one of the most popular algorithms to resolve this problem. This paper proposes a novel tag estimation algorithm we called Dynamic Parameter Tag Estimation (DPTE) algorithm by which the estimation accuracy of frame length could be improved and the hardware costs could be decreased. This new algorithm introduces an accumulative parameter to trigger frame length changing only when the parameter reaches some predefined values. Compared with existed tag estimation algorithms, the new algorithm has even better performances on system efficiency, estimation accuracy and adjusting frequency of frame length.},
keywords={radiofrequency identification;DFSA;DPTE;RFID;dynamic parameter tag estimation;estimation accuracy;system efficiency;tag collision;Accuracy;Algorithm design and analysis;Estimation;Frequency estimation;Heuristic algorithms;Prediction algorithms;Radiofrequency identification;ALOHA;Dynamic Parameter Tag Estimation (DPTE) Algorithm;Tag Estimation Algorithm},
doi={10.1109/ICEIEC.2015.7284479},
month={May},}
@INPROCEEDINGS{7277302,
author={D. E. M. A. AbuZeina and M. H. Alsaheb},
booktitle={2013 Taibah University International Conference on Advances in Information Technology for the Holy Quran and Its Sciences},
title={Capturing the Common Syntactical Rules for the Holy Quran: A Data Mining Approach},
year={2013},
pages={670-680},
abstract={This paper presents a novel approach to capture the common syntactical rules for the Holy Quran . By syntactical rules, we mean the common relationships between the words' tags that highly show up in the Quran. Arabic, like other language, has a number of tags which include nouns, verbs, and pronouns with a number of sub-types of each one of them. In this paper we used data mining approach to extract the common syntactical rules which will be offered to the natural language processing applications. Stanford part of speech tagger (29 tags) will be used to tag the Quran words. Then, the data mining too called WEKA (PredictiveApriori algorithm) will be used to find the famous syntactical rules. The extracted syntactical rules have a property that it is not necessary to have adjacent words tags. That is, long distance relation. The most common syntactical rule found is: tag1=RP tag2=NN tag3=WP 91 ⇒ tag4=VBD 90 acc:(0.97912)Which can be seen in the Quran sentence. This phrase (which is part of an ayah) appeared in 89 ayahs in 20 different surahs; the study used Mushaf Al-Madinah Al-Munawwarah (published by the King Fahd Complex for Printing the Holy Quran ).},
keywords={data mining;literature;natural language processing;text analysis;Arabic language;Holy Quran;Mushaf Al-Madinah Al-Munawwarah;Quran sentence;Quran words;Stanford part of speech tagger;WEKA;ayahs;common syntactical rules;data mining;natural language processing applications;predictive apriori algorithm;pronouns;surahs;verbs;words tags;Computers;Data mining;Natural language processing;Prediction algorithms;Printing;Speech;Arabic Language;Data Mining;Morphological Structures;Part of Speech Taggers.;Syntactical Rules;The Holy Quran},
doi={10.1109/NOORIC.2013.105},
month={Dec},}
@INPROCEEDINGS{7275490,
author={T. Nakanishi and M. Fujimoto and R. Nagao and S. Tatsukawa and T. Wada and H. Okada and K. Mutsuura},
booktitle={2014 International Conference on Indoor Positioning and Indoor Navigation (IPIN)},
title={Moving correction method of a mobile robot using passive RFID system based on obstacle prediction},
year={2014},
pages={246-254},
abstract={Recently, indoor mobile robot navigation systems have been developed all over the world as the technology for assisting aged and physically handicapped people. Especially, the systems using passive RFID with features of low cost and simple composition have attracted attention over the years. However, in these systems, there are two serious problems. 1) A mobile robot performs the meandering moving. 2) A mobile robot is difficult to avoid obstacles. Hence, a moving correction method is required very much to solve the above two problems. In this paper, we propose a new moving correction method of a mobile robot using only passive RFID to improve the moving trajectory. This method reduces the serpentine moving of mobile robot and avoids obstacles by predicting obstacles and walls in front of the mobile robot. To show the effectiveness of the proposed system, we carry out the performance evolutions by computer simulations and experiments. As the results, we show that the proposed method can reduce the serpentine moving of the mobile robot compared with the conventional method and also can avoid obstacles.},
keywords={collision avoidance;geriatrics;handicapped aids;human-robot interaction;mobile robots;radiofrequency identification;aged people assistance;indoor mobile robot navigation systems;moving correction method;moving trajectory improvement;obstacle avoidance;obstacle prediction;passive RFID system;physically handicapped people assistance;serpentine moving reduction;Collision avoidance;Estimation;Indoor navigation;Mobile robots;Passive RFID tags;RFID;continuous moving;mobile robot navigation system;moving correction;position estimation},
doi={10.1109/IPIN.2014.7275490},
month={Oct},}
@INPROCEEDINGS{7180106,
author={J. Goderie and B. M. Georgsson and B. v. Graafeiland and A. Bacchelli},
booktitle={2015 IEEE/ACM 12th Working Conference on Mining Software Repositories},
title={ETA: Estimated Time of Answer Predicting Response Time in Stack Overflow},
year={2015},
pages={414-417},
abstract={Question and Answer (Q&A) sites help developers dealing with the increasing complexity of software systems and third-party components by providing a platform for exchanging knowledge about programming topics. A shortcoming of Q&A sites is that they provide no indication on when an answer is to be expected. Such an indication would help, for example, the developers who posed the questions in managing their time. We try to fill this gap by investigating whether and how answering time for a question posed on Stack Overflow, a prominent example of Q&A websites, can be predicted considering its tags. To this aim, we first determine the types of answers to be considered valid answers to the question, after which the answering time was predicted based on similarity of the set of tags. Our results show that the classification is correct in 30%-35% of the cases.},
keywords={question answering (information retrieval);software metrics;Stack Overflow;question and answer sites;software system complexity;third-party components;Communities;Correlation;Prediction algorithms;Time factors;Time measurement;Training;response time;stack overflow},
doi={10.1109/MSR.2015.52},
ISSN={2160-1852},
month={May},}
@INPROCEEDINGS{7153609,
author={K. Andreadou and S. Papadopoulos and Y. Kompatsiaris},
booktitle={2015 13th International Workshop on Content-Based Multimedia Indexing (CBMI)},
title={Web image size prediction for efficient focused image crawling},
year={2015},
pages={1-6},
abstract={In the context of using Web image content for analysis and retrieval, it is typically necessary to perform large-scale image crawling. A serious bottleneck in such set-ups pertains to the fetching of image content, since for each web page a large number of HTTP requests need to be issued to download all included image elements. In practice, however, only the relatively big images (e.g., larger than 400 pixels in width and height) are potentially of interest, since most of the smaller ones are irrelevant to the main subject or correspond to decorative elements (e.g., icons, buttons). Given that there is often no dimension information in the HTML img tag of images, to filter out small images, an image crawler would still need to issue a GET request and download the respective files before deciding whether to index them. To address this limitation, in this paper, we explore the challenge of predicting the size of images on the Web based only on their URL and information extracted from the surrounding HTML code. We present two different methodologies: The first one is based on a common text classification approach using the n-grams or tokens of the image URLs and the second one relies on the HTML elements surrounding the image. Eventually, we combine these two techniques, and achieve considerable improvement in terms of accuracy, leading to a highly effective filtering component that can significantly improve the speed and efficiency of the image crawler.},
keywords={Internet;Web sites;hypermedia markup languages;image retrieval;text analysis;HTML code;HTML element;HTTP request;Web image content;Web image size prediction;Web page;decorative element;dimension information;filtering component;image URL;image crawler;image element;information extraction;large-scale image crawling;text classification;Data mining;Feature extraction;HTML;Training;Uniform resource locators;Vegetation;Web pages},
doi={10.1109/CBMI.2015.7153609},
ISSN={1949-3983},
month={June},}
@INPROCEEDINGS{7129551,
author={J. Liu and K. Zhao and S. Khan and M. Cameron and R. Jurdak},
booktitle={2015 31st IEEE International Conference on Data Engineering Workshops},
title={Multi-scale population and mobility estimation with geo-tagged Tweets},
year={2015},
pages={83-86},
abstract={Recent outbreaks of Ebola and Dengue viruses have again elevated the significance of the capability to quickly predict disease spread in an emergent situation. However, existing approaches usually rely heavily on the time-consuming census processes, or the privacy-sensitive call logs, leading to their unresponsive nature when facing the abruptly changing dynamics in the event of an outbreak. In this paper we study the feasibility of using large-scale Twitter data as a proxy of human mobility to model and predict disease spread. We report that for Australia, Twitter users' distribution correlates well the census-based population distribution, and that the Twitter users' travel patterns appear to loosely follow the gravity law at multiple scales of geographic distances, i.e. national level, state level and metropolitan level. The radiation model is also evaluated on this dataset though it has shown inferior fitness as a result of Australia's sparse population and large landmass. The outcomes of the study form the cornerstones for future work towards a model-based, responsive prediction method from Twitter data for disease spread.},
keywords={data privacy;diseases;geographic information systems;microorganisms;mobile computing;social networking (online);Dengue virus;Ebola virus;Twitter users travel pattern;census-based population distribution;disease spread;geo-tagged Tweet;geographic distance;gravity law;human mobility;large-scale Twitter data;metropolitan level;mobility estimation;multiscale population;national level;privacy-sensitive call log;radiation model;state level;time-consuming census process;Australia;Data models;Diseases;Gravity;Sociology;Statistics;Twitter},
doi={10.1109/ICDEW.2015.7129551},
month={April},}
@ARTICLE{6910312,
author={B. Harrison and S. G. Ware and M. W. Fendt and D. L. Roberts},
journal={IEEE Transactions on Emerging Topics in Computing},
title={A Survey and Analysis of Techniques for Player Behavior Prediction in Massively Multiplayer Online Role-Playing Games},
year={2015},
volume={3},
number={2},
pages={260-274},
abstract={While there has been much research done on player modeling in single-player games, player modeling in massively multiplayer online role-playing games (MMORPGs) has remained relatively unstudied. In this paper, we survey and evaluate three classes of player modeling techniques: 1) manual tagging; 2) collaborative filtering; and 3) goal recognition. We discuss the strengths and weaknesses that each technique provides in the MMORPG environment using desiderata that outline the traits an algorithm should posses in an MMORPG. We hope that this discussion as well as the desiderata help future research done in this area. We also discuss how each of these classes of techniques could be applied to the MMORPG genre. In order to demonstrate the value of our analysis, we present a case study from our own work that uses a model-based collaborative filtering algorithm to predict achievements in World of Warcraft. We analyze our results in light of the particular challenges faced by MMORPGs and show how our desiderata can be used to evaluate our technique.},
keywords={behavioural sciences computing;collaborative filtering;computer games;MMORPG environment;World of Warcraft;desiderata;goal recognition;manual tagging;massively multiplayer online role-playing games;model-based collaborative filtering algorithm;player behavior prediction;player modeling techniques;single-player games;Collaboration;Computational modeling;Data models;Games;Licenses;Prediction algorithms;Predictive models;Computational Modeling;Computational modeling;Games;Machine Learning;data mining;games;machine learning;performance evaluation},
doi={10.1109/TETC.2014.2360463},
ISSN={2168-6750},
month={June},}
@INPROCEEDINGS{7058047,
author={Wenrong Zeng and X. Chen and Hong Cheng},
booktitle={2014 International Conference on Data Science and Advanced Analytics (DSAA)},
title={Pseudo labels for imbalanced multi-label learning},
year={2014},
pages={25-31},
abstract={The classification with instances which can be tagged with any of the 2L possible subsets from the predefined L labels is called multi-label classification. Multi-label classification is commonly applied in domains, such as multimedia, text, web and biological data analysis. The main challenge lying in multi-label classification is the dilemma of optimising label correlations over exponentially large label powerset and the ignorance of label correlations using binary relevance strategy (1-vs-all heuristic). The classification with label powerset usually encounters with highly skewed data distribution, called imbalanced problem. While binary relevance strategy reduces the problem from exponential to linear, it totally neglects the label correlations. In this artical, we propose a novel strategy of introducing Balanced Pseudo-Labels (BPL) which build more robust classifiers for imbalanced multi-label classification, which embeds imbalanced data in the problems innately. By incorporating the new balanced labels we aim to increase the average distances among the distinct label vectors. In this way, we also code the label correlation implicitly in the algorithm. Another advantage of the proposed method is that it can combined with any classifier and it is proportional to linear label transformation. In the experiment, we choose five multi-label benchmark data sets and compare our algorithm with the most state-of-art algorithms. Our algorithm outperforms them in standard multi-label evaluation in most scenarios.},
keywords={learning (artificial intelligence);pattern classification;BPL;balanced pseudo-label;binary relevance strategy;data distribution;imbalanced data;imbalanced problem;label correlation;label powerset;label vector;linear label transformation;multilabel benchmark data set;multilabel classification;multilabel learning;pseudo label;robust classifier;Classification algorithms;Correlation;Kernel;Linear programming;Power line communications;Prediction algorithms;Vectors},
doi={10.1109/DSAA.2014.7058047},
month={Oct},}
@INPROCEEDINGS{7051426,
author={Y. P. Hung and H. Y. Yeh and I. B. Liao and C. M. Pan and C. Y. Chiang},
booktitle={2014 17th Oriental Chapter of the International Committee for the Co-ordination and Standardization of Speech Databases and Assessment Techniques (COCOSDA)},
title={An investigation on linguistic features for Mandarin prosody generation},
year={2014},
pages={1-5},
abstract={This paper seeks to investigate the usability of two fully-automatic machine-extracted linguistic features from an unlimited text input, in a prosody generation of Mandarin text-to-speech system (MTTS). One is the base-phrase chunk feature, labeled by a conditional random field (CRF)-based base-phrase chunker. Another is the punctuation confidence (PC), calculated for each lexical word (LW) boundary from input text tagged with Chinese word boundaries, part of speech (POS) and base-phrase chunk, measuring the likelihood of inserting a punctuation mark (PM) at a word boundary. Owing to the fact that a PM in text is highly correlated with a prosodic break, and base-phrases play an important role in human language understanding, the two features potentially could provide useful information for prosody generation. To examine potential usefulness of the proposed linguistic features, the performances of neural network-based prosody generator - with and without the proposed features - were evaluated. Both objective and subjective tests showed that the prosody generator with the proposed linguistic features performed better than the one without the proposed features. So the proposed PC and base-phrase chunking information are promising features for Mandarin prosody generation.},
keywords={computational linguistics;natural language processing;neural nets;text analysis;CRF-based base-phrase chunker;Chinese word boundaries;LW boundary;MTTS;Mandarin prosody generation;Mandarin text-to-speech system;PC;PM;POS;conditional random field-based base-phrase chunker;fully-automatic machine-extracted linguistic features;lexical word boundary;neural network-based prosody generator;part of speech;punctuation confidence;punctuation mark;Entropy;Predictive models;Shape;Software;Syntactics;Mandarin prosody generation;break prediction;linguistic feature;punctuation confidence;text-to-speech},
doi={10.1109/ICSDA.2014.7051426},
month={Sept},}
@INPROCEEDINGS{7045956,
author={S. Yagcioglu and E. Erdem and A. Erdem},
booktitle={2015 IEEE Winter Conference on Applications of Computer Vision},
title={City Scale Image Geolocalization via Dense Scene Alignment},
year={2015},
pages={726-732},
abstract={Predicting where a photo was taken is quite important and yet a challenging task for computer vision algorithms. Our motivation is to solve this difficult problem in a city scale setting by employing a data-driven approach. In order to pursue this goal, we developed a fast and robust scene matching method that follows a coarse-to-fine strategy. In particular, we combine scene retrieval via global features and dense scene alignment and use a large set of geo-tagged images of downtown San Francisco in our evaluation. The experimental results show that the proposed approach, despite its simplicity, is surprisingly effective and achieves comparable results with the state-of-the-art.},
keywords={computer vision;feature extraction;image matching;image retrieval;San Francisco;city scale image geolocalization;coarse-to-fine matching strategy;computer vision algorithm;data-driven approach;dense scene alignment;geo-tagged images;global features;scene matching method;scene retrieval;Cities and towns;Digital signal processing;Geology;Image color analysis;Prediction algorithms;Robustness;Visualization},
doi={10.1109/WACV.2015.102},
ISSN={1550-5790},
month={Jan},}
@INPROCEEDINGS{7025628,
author={J. J. Thiagarajan and K. N. Ramamurthy and P. Sattigeri and P. T. Bremer and A. Spanias},
booktitle={2014 IEEE International Conference on Image Processing (ICIP)},
title={Automatic image annotation using inverse maps from semantic embeddings},
year={2014},
pages={3107-3111},
abstract={Human annotation in large scale image databases is time-consuming and error-prone. Since it is very hard to mine image databases using just visual features or textual descriptors, it is common to transform the image features into a semantically meaningful space. In this paper, we propose to perform image annotation in a semantic space inferred based on sparse representations. By constructing a semantic embedding for the visual features, that is constrained to be close to the tag embedding, we show that a robust inverse map can be used to predict the tags. Experiments using standard datasets show the effectiveness of the proposed approach in automatic image annotation when compared to existing methods.},
keywords={feature extraction;image representation;visual databases;automatic image annotation;human annotation;image databases;image features;robust inverse map;semantic embeddings;semantic space;sparse representations;tag embedding;visual features;Feature extraction;Prediction algorithms;Semantics;Sparse matrices;Training;Vectors;Visualization;Image annotation;RBF interpolation;embedding;inverse map;sparse coding},
doi={10.1109/ICIP.2014.7025628},
ISSN={1522-4880},
month={Oct},}
@INPROCEEDINGS{6999394,
author={J. Pánek and J. Hajič and D. Hoksza},
booktitle={2014 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
title={Template-based prediction of ribosomal RNA secondary structure},
year={2014},
pages={18-20},
abstract={Determining the structure of ribosomal RNAs (rRNAs) is one of the crucial steps in understanding the process of protein synthesis, for which rRNAs are one of the basic components. Nevertheless, due to extreme technical difficulties, spatial (3D) structures have been resolved experimentally for only 14 organisms. Also, computational prediction of 3D rRNA structure is almost impossible, and prediction of secondary structure (the list of base pairs in the folded RNA), an important intermediate step between sequence and 3D structure that is used broadly in modeling of RNA structures, is in the case of rRNAs hindered by both extreme sequence length and high structure complexity. Here we present a proof-of-concept for an rRNA secondary structure prediction method that utilizes known structures as structural templates. Our template-based prediction algorithm determines those regions of the sequence for which structure is being predicted that are conserved well enough so that their secondary structure can be copied over from the template. The structure of the remaining, unconserved regions is predicted using a thermodynamic folding model. Applying a baseline implementation of our algorithm to the E. coli 16S rRNA, we have achieved state-of-the-art recall and precision using the structure of T. thermophilus 16S rRNA as a template.},
keywords={RNA;cellular biophysics;microorganisms;molecular biophysics;proteins;E. coli 16S rRNA;T. thermophilus 16S rRNA;extreme sequence length;high structure complexity;proof-of-concept;protein synthesis;ribosomal RNA secondary structure prediction method;spatial 3D structures;state-of-the-art recall precision;structural templates;template-based prediction;template-based prediction algorithm;thermodynamic folding model;unconserved regions;Algorithm design and analysis;Bioinformatics;Educational institutions;Prediction algorithms;Predictive models;RNA;Tagging},
doi={10.1109/BIBM.2014.6999394},
month={Nov},}
@INPROCEEDINGS{6984802,
author={E. R. Fernandes and I. M. Rodrigues and R. L. Milidiú},
booktitle={2014 Brazilian Conference on Intelligent Systems},
title={Portuguese Part-of-Speech Tagging with Large Margin Structure Learning},
year={2014},
pages={25-30},
abstract={Part-of-Speech Tagging is a fundamental task on many Natural Language Processing systems. This task consists in identifying the syntactic category, i.e. the part of speech, of each word in a sentence. Despite the fact that the current state-of-the-art accuracy for this task is around 97%, any improvement has an immediate impact on more complex tasks, like Parsing, Semantic Role Labeling and Information Extraction. Thus, it is still relevant to explore this task. In this paper, we introduce a part-of-speech tagger based on the Structure Learning framework that reduces the smallest known error on the Portuguese Mac-Morpho corpus by 7.8%. We also apply our tagger to a recently revised version of Mac-Morpho. Our system accuracy on this latter version is competitive with a semi-supervised Neural Network trained on Mac-Morpho plus a very large non-annotated corpus. Additionally, our system is simpler than previous systems and uses a very limited feature set. Our system employs a Large Margin training criteria to derive a structure predictor that is more robust on unseen data.},
keywords={grammars;identification technology;learning (artificial intelligence);natural language processing;neural nets;speech processing;Portuguese Mac-Morpho corpus;Portuguese part-of-speech tagging;information extraction;large margin training criteria;margin structure learning;natural language processing systems;nonannotated corpus;parsing;part-of-speech tagger;semantic role labeling;semisupervised neural network;structure learning framework;structure predictor;syntactic category;Accuracy;Hidden Markov models;Natural language processing;Prediction algorithms;Predictive models;Tagging;Training;Machine Learning;Natural Language Processing;POS Tagging;Structure Learning},
doi={10.1109/BRACIS.2014.16},
month={Oct},}
@INPROCEEDINGS{6973949,
author={Y. Lu and P. Aarabi},
booktitle={2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
title={Extracting deep social relationships from photos},
year={2014},
pages={453-456},
abstract={Hidden within the relative location of tags in images is a relational model that can identify how close two individuals are, or, the affinity of a person to an object or a brand. Based on this model we can 1) better understand the relationship between users/tags, 2) find photos where a user is pictured but not tagged in, and 3) enable searching “inside” images by clicking on any location within an image to start a search. This paper proposes a method of modeling the relationship between objects based on their spatial arrangement in a set of tagged images. Based on the relative coordinates of each object tag, we compute a joint relativity between each tag pair, generate a social relationship graph and propose an efficient image search method using the joint Relativity graph. We evaluated our approach with real world data from Facebook, showing a direct relationship between the number of tagged photos and the amount of information obtained from these photos, and an average correlation coefficient of 0.8 between user-generated relativity scores and those obtained by our algorithm.},
keywords={correlation methods;digital photography;graph theory;image retrieval;social networking (online);Facebook;average correlation coefficient;deep social relationships extraction;image search method;images tags;joint relativity graph;object tag;relational model;social networks;social relationship graph;spatial arrangement;tagged photos;user-generated relativity scores;Computational modeling;Correlation coefficient;Data mining;Facebook;Method of moments;Predictive models;link prediction;social networks;social relationship modeling and extraction},
doi={10.1109/SMC.2014.6973949},
ISSN={1062-922X},
month={Oct},}
@INPROCEEDINGS{6927547,
author={D. I. Ignatov and S. Nikolenko and T. Abaev and N. Konstantinova},
booktitle={2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)},
title={Online Recommender System for Radio Station Hosting: Experimental Results Revisited},
year={2014},
volume={1},
pages={229-236},
abstract={We present a new recommender system developed for the Russian interactive radio network FMhost based on a previously proposed model. The underlying model combines a collaborative user-based approach with information from tags of listened tracks in order to match user and radio station profiles. It follows an adaptive online learning strategy based on the user history. We compare the proposed algorithms and an industry standard technique based on singular value decomposition (SVD) in terms of precision, recall, and NDCG measures, experiments show that in our case the fusion-based approach shows the best results.},
keywords={groupware;interactive devices;music;radio stations;recommender systems;FMhost;Russian interactive radio network;SVD;adaptive online learning strategy;collaborative user-based approach;fusion-based approach;industry standard technique;online recommender system;radio station hosting;radio station profiles;singular value decomposition;Broadcasting;Educational institutions;Mathematical model;Prediction algorithms;Recommender systems;Servers;Training;hybrid recommender system;information fusion;interactive radio network;music recommender systems;quality of service},
doi={10.1109/WI-IAT.2014.38},
month={Aug},}
@INPROCEEDINGS{6906657,
author={S. P. H and B. Omman},
booktitle={2014 First International Conference on Networks Soft Computing (ICNSC2014)},
title={Feature selection techniques for gender prediction from blogs},
year={2014},
pages={355-359},
abstract={The goal of this paper is to identify gender of blog authors. Features such as POS tags, unigram (words+punctuations), bigrams and word classes are considered. To synthesis/rank features we are using Mutual information, Chi-square and Information gain methods. The dataset is the collection of 3227 blogs originally derived from blogs set, and among them 1679 were written by male and 1548 were written by female. The results were obtained using 10-cross fold validation. Unigram of words gave better accuracy of 78.81% in comparison with the other features. We found that chi-square is the best in ranking features. The classification is done using Multinomial Naïve Bayes Classifier, and different kernel functions of SVM such as PolyKernel, Puk, Normalized PolyKernel and RBFkernel.},
keywords={Bayes methods;Web sites;feature selection;gender issues;pattern classification;support vector machines;10-cross fold validation;Chi-square;POS tags;RBFkernel;SVM;bigrams;blog authors;feature ranking;feature selection techniques;feature synthesis;gender prediction;information gain methods;multinomial naïve Bayes classifier;mutual information;normalized PolyKernel;unigram;word classes;Accuracy;Blogs;Feature extraction;Kernel;Speech;Support vector machines;Writing;Classification;Multinomial Naïve Bayes;SVM classifier;WEKA classifier;feature selection},
doi={10.1109/CNSC.2014.6906657},
month={Aug},}
@INPROCEEDINGS{6890217,
author={C. Guo and X. Tian and T. Mei},
booktitle={2014 IEEE International Conference on Multimedia and Expo (ICME)},
title={User specific friend recommendation in social media community},
year={2014},
pages={1-6},
abstract={Social networks nowadays have become an important form of communication in which users can post their current status or share their lives by mobile phones or the Web. In this paper, we develop an effective and efficient model to estimate continuous tie strength between users for friend recommendation with the heterogeneous data from social media community. We categorize those multimodal data into two classes: interaction data (e.g., comments, marking favorite photos) and similarity data(e.g., common friends, groups, tags, geo, visual). We propose to use asymmetric relationship in the interaction data for tie strength estimation instead of using the conventional symmetric ones. Furthermore, by exploring the behavior of users in a social media community, we find that the tie strength between users can be approximately modeled as a linear function of their social connections. Based on this observation, we propose an effective and highly efficient user specific linear model for the tie strength estimation. The experiments on a popular social network show promising results and demonstrate the effectiveness of our proposed method.},
keywords={Internet;data handling;mobile handsets;multimedia computing;recommender systems;social networking (online);Web;continuous tie strength estimation;interaction data;mobile phones;multimodal data categorization;similarity data;social media community;social networks;user specific friend recommendation;Communities;Data models;Estimation;Kernel;Social network services;Vectors;Visualization;Friend prediction;Multimedia},
doi={10.1109/ICME.2014.6890217},
ISSN={1945-7871},
month={July},}
@INPROCEEDINGS{6889846,
author={G. Song and Y. Ye},
booktitle={2014 International Joint Conference on Neural Networks (IJCNN)},
title={A new ensemble method for multi-label data stream classification in non-stationary environment},
year={2014},
pages={1776-1783},
abstract={Most existing approaches for the data stream classification focus on single-label data in non-stationary environment. In these methods, each instance can only be tagged with one label. However, in many realistic applications, each instance should be tagged with more than one label. To address the challenge of classifying multi-label stream in evolving environment, we propose a novel Multi-Label Dynamic Ensemble (MLDE) approach. The proposed MLDE integrates a number of Multi-Label Cluster-based Classifiers (MLCCs). MLDE includes an adaptive ensemble method and an ensemble voting method with two important weights, subset accuracy weight and similarity weight. Experimental results reveal that MLDE achieves better performance than state-of-the-art multi-label stream classification algorithms.},
keywords={data handling;pattern classification;pattern clustering;MLCC;MLDE approach;multilabel cluster based classifiers;multilabel data stream classification;multilabel dynamic ensemble;new ensemble method;nonstationary environment;realistic applications;voting method;Accuracy;Classification algorithms;Clustering algorithms;Heuristic algorithms;Prediction algorithms;Testing;Training;Concept drift;Data stream classification;Ensemble learning;Multi-label classification},
doi={10.1109/IJCNN.2014.6889846},
ISSN={2161-4393},
month={July},}
@INPROCEEDINGS{6881850,
author={F. Prasser and F. Kohlmayer and K. A. Kuhn},
booktitle={2014 IEEE 27th International Symposium on Computer-Based Medical Systems},
title={A Benchmark of Globally-Optimal Anonymization Methods for Biomedical Data},
year={2014},
pages={66-71},
abstract={Collaboration and data sharing have become core elements of biomedical research. At the same time, there is a growing understanding of privacy threats related to data sharing, especially when sensitive data from distributed sources become available for linkage. Statistical disclosure control comprises well-known data anonymization techniques that allow the protection of data by introducing fuzziness. To protect datasets from different types of threats, different privacy criteria are commonly implemented. Data anonymization is an important measure, but it is computationally complex, and it can significantly reduce the expressiveness of data. To attenuate these problems, a number of algorithms has been proposed, which aim at increasing data quality or improving efficiency. Previous evaluations of such algorithms lack a systematic approach, as they focus on specific algorithms, specific privacy criteria, and specific runtime environments. Therefore, it is difficult for decision makers to decide which algorithm is best suited for their requirements. As a first step towards a comprehensive and systematic evaluation of anonymity algorithms, we report on our ongoing efforts for providing an open source benchmark. In this contribution, we focus on optimal algorithms utilizing global recoding with full-domain generalization. We present a systematic evaluation of domain-specific algorithms and generic search methods for a broad set of privacy criteria, including k-anonymity, l-diversity, t-closeness and d-presence, and their use in multiple real-world datasets. Our results show that there is no single solution fitting all needs, and that generic search methods can outperform highly specialized algorithms.},
keywords={data privacy;medical information systems;δ-presence;I-diversity;biomedical data;data anonymization;data quality;domain-specific algorithms;full-domain generalization;generic search methods;global recoding;globally-optimal anonymization methods;k-anonymity;privacy criteria;t-closeness;Benchmark testing;Biomedical measurement;Data privacy;Lattices;Prediction algorithms;Privacy;Tagging;&amp;#948;-presence;benchmark;de-identification;k-anonymity;l-diversity;privacy;statistical disclosure control;t-closeness},
doi={10.1109/CBMS.2014.85},
ISSN={1063-7125},
month={May},}
@INPROCEEDINGS{6860669,
author={A. Tag and R. Weigel and A. Hagelauer and B. Bader and C. Huck and M. Pitschi and K. Wagner and D. Karolewski and C. Schäffel},
booktitle={2014 IEEE International Reliability Physics Symposium},
title={Influence of temperature distribution on behavior, modeling, and reliability of BAW resonators},
year={2014},
pages={5C.5.1-5C.5.7},
abstract={The influence of spatial inhomogeneous temperature distribution caused by dissipated power on BAW resonator's behavior has been investigated for the first time. A novel modeling approach with an acceptable calculation time taking the temperature distribution into account has been developed allowing an improved modeling of BAW resonator behavior at high power levels. By applying the modeling approach it could be shown that the common way to use a measured TCF to determine the device temperature at high power levels is not generally valid and can lead to inaccuracies in life time predictions. The modeling approach has been verified by infrared temperature measurements of BAW resonators under high power loads.},
keywords={acoustic resonators;bulk acoustic wave devices;reliability;temperature distribution;BAW resonator behavior;BAW resonator modelling;BAW resonator reliability;TCF;high power levels;infrared temperature measurements;life time predictions;spatial inhomogeneous temperature distribution;Electrodes;Frequency measurement;Heat sinks;Heating;Resonant frequency;Temperature distribution;Temperature measurement},
doi={10.1109/IRPS.2014.6860669},
ISSN={1541-7026},
month={June},}
@INPROCEEDINGS{6823535,
author={S. Subha},
booktitle={2013 International Conference on Green Computing, Communication and Conservation of Energy (ICGCE)},
title={An energy saving cache algorithm},
year={2013},
pages={757-760},
abstract={Set associative caches have fixed ways. All cache ways of accessed set are enabled during cache access. This paper proposes an algorithm to map address to fixed cache way of mapped set. The tag of the address is XOR'd with (w-1) in w-way set associative cache. The result is divided into blocks of size of number of cache ways w. The average of maximum and minimum of the frequency of the result of division is the mapped way. The mapped way is accessed. All other cache ways are in non-operational mode during cache access. Only one cache way is operational in this mapping. The proposed model is simulated with SPEC2K benchmarks. An average improvement in AMAT of 58% is observed with 49% improvement in energy consumption.},
keywords={benchmark testing;cache storage;power aware computing;AMAT;SPEC2K benchmarks;XOR;cache access;energy consumption;energy saving cache algorithm;fixed cache way;w-way set associative cache;Algorithm design and analysis;Analytical models;Arrays;Benchmark testing;Frequency conversion;Mathematical model;Prediction algorithms;Average memory access;energy savings;set associative cache},
doi={10.1109/ICGCE.2013.6823535},
month={Dec},}
@INPROCEEDINGS{6816332,
author={W. Huang and Man Li and Weisong Hu and Guojie Song and K. Xie},
booktitle={2013 10th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)},
title={Automated urban location annotation on mobile records},
year={2013},
pages={951-956},
abstract={Location information is becoming much more important than ever before, especially in mobile services. Being widespread, less cost of energy and almost free for collecting data make mobile phone a perfect location sensor probe. Meaningful location name rather than digital coordinates could provide much more valuable information. In this paper, we develop a location semantic predicting method referred to Location Annotation(LA) which can automatically annotate meaningful base stations of phone users with semantic tags such as “home”, “work place” and “club”. We extract several explicit features from phone records and spatial-temporal patterns of mobile phone users to build an annotation model based on Maximum Entropy Model. Then a machine learning method is presented to estimate the best configuration of parameters in the model. Finally, comprehensive experiments demonstrate good performance of our method. Overall accuracy is about 90% which outperforms simple and traditional classification methods by 10+%. Semantic location names are valuable to urban planning and optimization, transportation management and land use planning.},
keywords={learning (artificial intelligence);maximum entropy methods;mobile computing;mobile handsets;pattern classification;annotation model;automated urban location annotation;classification methods;location semantic prediction method;machine learning method;maximum entropy model;mobile phone users spatial-temporal patterns;mobile records;phone user base stations;semantic location names;semantic tags;Base stations;Feature extraction;Mobile communication;Mobile handsets;Predictive models;Sociology;Statistics},
doi={10.1109/FSKD.2013.6816332},
month={July},}
@INPROCEEDINGS{6810718,
author={M. J. Hakeem and K. Raahemifar and G. N. Khan},
booktitle={2014 IEEE International Conference on RFID (IEEE RFID)},
title={Novel modulo based Aloha anti-collision algorithm for RFID systems},
year={2014},
pages={97-102},
abstract={RFID (Radio frequency Identification) has become an efficient way to identify, track and/or trace objects and people. Its importance has motivated scientists and researchers to examine the challenges that are slowing its expeditious deployment in various applications. RFID collision is a major challenge imposed by the wireless links shared among a reader and the many tags in the interrogation zone. In most proposed anti-collision algorithms, tags reply randomly to time slots chosen by the reader. Since two or more tags may choose the same slot, this Random Access (RA) causes garbled data at the reader side; therefore, the identification process fails. In this paper, we propose a new anti-collision algorithm that adopts a novel method for eliminating the theory of RA to enhance system efficiency and to reduce both the number of rounds between reader and tag and the number of collided/empty slots over existing algorithms. In this algorithm, tags use modulo function to choose tag owned time slot. Another advantage of this method is that the reader estimates the next frame size and compares it with the previously selected frame sizes that are saved in the reader to ensure there is no redundancy. The performance of the algorithm is simulated and compared with existent ALOHA family algorithms.},
keywords={access protocols;object detection;object tracking;radio links;radiofrequency identification;random processes;telecommunication congestion control;RFID reader;RFID system;RFID tags;garbled data;interrogation zone;modulo based ALOHA anticollision algorithm;next frame size estimation;object identification;object tracking;people identification;people tracking;radiofrequency identification;random access;time slots;wireless links;Algorithm design and analysis;Classification algorithms;Conferences;Estimation;Heuristic algorithms;Prediction algorithms;Radiofrequency identification;ALOHA;Anti-Collision Algorithm;Multiple tags Identification;RFID System},
doi={10.1109/RFID.2014.6810718},
ISSN={2374-0221},
month={April},}
@INPROCEEDINGS{6778459,
author={Y. Ida and T. Nakamura and T. Matsumoto},
booktitle={2013 2nd IAPR Asian Conference on Pattern Recognition},
title={Label-Related/Unrelated Topic Switching Model: A Partially Labeled Topic Model Handling Infinite Label-Unrelated Topics},
year={2013},
pages={892-896},
abstract={We propose a Label-Related/Unrelated Topic Switching Model (LRU-TSM) based on Latent Dirichlet Allocation (LDA) for modeling a labeled corpus. In this model, each word is allocated to a label-related topic or a label-unrelated topic. Label-related topics utilize label information, and label-unrelated topics utilize the framework of Bayesian Nonparametrics, which can estimate the number of topics in posterior distributions. Our model handles label-related and -unrelated topics explicitly, in contrast to the earlier model, and improves the performances of applications to which is applied. Using real-world datasets, we show that our model outperforms the earlier model in terms of perplexity and efficiency for label prediction tasks that involve predicting labels for documents or pictures without labels.},
keywords={Bayes methods;document handling;nonparametric statistics;statistical distributions;Bayesian nonparametrics;LDA;LRU-TSM;application performance improvement;document label prediction;label prediction task efficiency;label prediction task perplexity;label-related topic;label-related topic switching model;label-unrelated topic;label-unrelated topic switching model;labeled corpus modeling;latent Dirichlet allocation;partially-labeled topic model handling infinite label-unrelated topics;picture label prediction;posterior distributions;real-world datasets;word allocation;Biological system modeling;Data models;Predictive models;Resource management;Switches;Vectors;Vocabulary;Bayesian methods;Tagging;Topic model},
doi={10.1109/ACPR.2013.163},
ISSN={0730-6512},
month={Nov},}
@INPROCEEDINGS{6779440,
author={D. Anand},
booktitle={2014 IEEE International Advance Computing Conference (IACC)},
title={Evaluating folksonomy information sources for genre prediction},
year={2014},
pages={887-892},
abstract={Automatic genre identification is a task which plays a crucial role in many domains such as automatic storytellers, recommender systems and web page topic detectors. Genre classification is especially interesting in the domain of narrative content which is characterized by a large number of ambiguous and overlapping categories. The rise in popularity of social tagging systems forms a rich source of input information which could be harnessed for this task. In this paper we investigate two different information folksonomy sources for the movie domain namely: keywords and tags, the first of which is user annotated and expert monitored whereas the latter is non-monitored. A comparison is performed to assess the efficacy of both sources in solving this multi-label classification problem and it is found that the in spite of being expert monitored and better structured, keywords are worse predictors of the genres of movies than tags in most cases.},
keywords={classification;information resources;Web page topic detectors;automatic genre identification;automatic storytellers;folksonomy information sources;genre classification;genre prediction;information folksonomy sources;movie domain;multilabel classification problem;narrative content;overlapping categories;recommender systems;social tagging systems;Conferences;Decision support systems;Handheld computers;Size measurement;Categorization;Folksonomy;information retrieval;keywords;multi-label classification;tags},
doi={10.1109/IAdCC.2014.6779440},
month={Feb},}
